---
title: "Introduction"
description: "Description of your new file."
---

Depot in DataOS is a Resource used to connect different data sources to DataOS by abstracting the complexities associated with the underlying source system (including protocols, credentials, and connection schemas). It enables users to establish connections and retrieve data from various data sources, such as file systems (e.g., AWS S3, Google GCS, Azure Blob Storage), data lake systems, database systems (e.g., Redshift, SnowflakeDB, Bigquery, Postgres), and event systems (e.g., Kafka, Pulsar) without moving the data.

Within DataOS, the hierarchical structure of a data source is represented as follows:

![Hierarchical Structure of a Data Source within DataOS](/resources/depot/udl.png "Hierarchical Structure of a Data Source within DataOS")

The Depot serves as the registration of data locations to be made accessible to DataOS. Through the Depot Service, each source system is assigned a unique address, referred to as a **Uniform Data Link (UDL)**. The UDL grants convenient access and manipulation of data within the source system, eliminating the need for repetitive credential entry. The UDL follows this format:

**`                                                             dataos://[depot]:[collection]/[dataset]`**

<Info>
  Depot Service is a DataOS Service that manages the Depot Resource. It facilitates in-depth introspection of Depots and their associated storage engines. Once a Depot is created, users can obtain comprehensive information about the datasets contained within, including details such as constraints, partition, indexing, etc.
</Info>

Leveraging the UDL enables access to datasets and seamless execution of various operations, including data transformation using various Clusters and [Policy](https://dataos.info/resources/policy/) assignments.

Once this mapping is established, Depot Service automatically generates the Uniform Data Link (UDL) that can be used throughout DataOS to access the data. As a reminder, the UDL has the format: `dataos://[depot]:[collection]/[dataset]`.

For a simple file storage system, "Collection" can be analogous to "Folder," and "Dataset" can be equated to "File." The Depot's strength lies in its capacity to establish uniformity, eliminating concerns about varying source system terminologies.

Once a Depot is created, all members of an organization gain secure access to datasets within the associated source system. The Depot not only facilitates data access but also assigns **default** [Access Policies](https://dataos.info/resources/policy/) to ensure data security. Moreover, users have the flexibility to define and utilize custom [Access Policies](https://dataos.info/resources/policy/) for the Depot and [Data Policies](https://dataos.info/resources/policy/) for specific datasets within the Depot.

<Info>
  Depot provides 'access' to data, meaning that data remains within the source system and is neither moved nor duplicated. However, DataOS offers multiple Stacks such as Flare, Benthos, etc. to perform ingestion, querying, syndication, and copying if the need arises.
</Info>

## How to create a Depot?

To create a Depot in DataOS, simply compose a manifest configuration file for a Depot and apply it using the DataOS [Command Line Interface (CLI)](https://dataos.info/interfaces/cli/).

### **Structure of a Depot manifest**

![](/resources/depot/depot_yaml.png)

To know more about the attributes of Depot manifest Configuration, refer to the link: [Attributes of Depot manifest](https://dataos.info/resources/depot/configurations/).

### **Prerequisites**

Before proceeding with Depot creation, it is essential to ensure that you possess the required authorization. To confirm your eligibility, execute the following commands in the CLI:

```bash
dataos-ctl user get
# Expected Output
INFO[0000] 😃 user get...                                
INFO[0000] 😃 user get...complete                        

      NAME     │     ID      │  TYPE  │        EMAIL         │              TAGS    
───────────────┼─────────────┼────────┼──────────────────────┼────────────────────
  IamGroot     │ iamgroot    │ person │ iamgroot@tmdc.io     │ roles:id:data-dev,  
               │             │        │                      │ roles:id:operator,  
               │             │        │                      │ roles:id:system-dev, 
               │             │        │                      │ roles:id:user,    
               │             │        │                      │ users:id:iamgroot
```

To create Depots, ensure that you possess the following tags.

* `roles:id:user`

* `roles:id:data-dev`

* `roles:id:system-dev`

<Warning>
  &#x20;If you do not possess these tags, contact the DataOS Operator or Administrator within your organization to assign you the necessary tag or the use case for the creation of the Depot.
</Warning>

### **Create a Depot manifest file**

<AccordionGroup>
  <Accordion title="ABFSS" defaultOpen={false}>
    ### Pre-requisites specific to the ABFSS Depot

    To create an ABFSS Depot you must have the following details:

    * **azurestorageaccountname**: This is the name of the Azure Storage account where your data resides. You can find this in the Azure portal under the "Storage accounts" section. It serves as a unique identifier for your storage service.

    * **azurestorageaccountkey**: This key is used to authenticate and access your Azure Storage account. You can retrieve it from the Azure portal by navigating to "Storage accounts" > "Access keys." It is critical to store this key securely as it allows access to your data.

    * **account**: The name of the Azure Storage account. It is the same as the `azurestorageaccountname` and can be found in the Azure portal under the "Storage accounts" section. It identifies the specific storage account where your data is stored.

    * **container**: The name of the container within the Azure Storage account. A container acts like a folder or directory that holds the data. You can create and find containers in the Azure portal under your specific storage account > "Containers."

    * **relativePath**: This is the relative path to the data within the container. It specifies the exact location of the data or folder, such as "data/customer" or "files/raw\_data." You will need to specify this path based on where your data is stored inside the container.

    * **format**: This defines the format of the data stored in the Azure Storage container. Common formats include "CSV," "JSON," or "Parquet." Specify the format according to the type of data you are working with.

    ### Create a ABFSS Depot

    Azure Blob File System Secure (ABFSS) is an object storage system. Object stores are distributed storage systems designed to store and manage large amounts of unstructured data.

    DataOS enables the creation of a Depot of type 'ABFSS' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing ABFSS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a ABFSS Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your ABFSS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Google Bigquery" defaultOpen={false}>
    ### Pre-requisites specific to the Bigquery Depot

    To create a Bigquery Depot you must have the following details:

    * **projectid**: The unique identifier of the Google Cloud project where your GCS bucket resides. You can find this in the Google Cloud Console under the "Project Info" section of your project's dashboard. It is essential for associating operations with the correct project.

    * **email**: The email address associated with the Google Cloud service account. You can retrieve this by navigating to the Google Cloud Console under "IAM & Admin" > "Service Accounts" and selecting the relevant service account. This email is used for authenticating requests to GCS.

    * **json\_keyfile**: The path to the JSON key file associated with the Google Cloud service account. This file is used for authentication and can be downloaded from the Google Cloud Console under "IAM & Admin" > "Service Accounts." Select the service account, click on "Keys," and download the key by selecting "Add Key" > "JSON." Ensure the file is securely stored and the path is accessible.

    * **project**: (Optional) The name of the Google Cloud project. While not always required, it can be specified to provide additional context. You can find it in the Google Cloud Console under the "Project Info" section.

    ### Create a Bigquery Depot

    Azure Blob File System Secure (ABFSS) is an object storage system. Object stores are distributed storage systems designed to store and manage large amounts of unstructured data.

    DataOS enables the creation of a Depot of type 'Bigquery' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Bigquery credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Bigquery Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Bigquery Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Eventhub" defaultOpen={false}>
    ### Pre-requisites specific to the Eventhub Depot

    To create a Eventhub Depot you must have the following details:

    * **Eventhub Shared Access Key Name**: The name of the shared access key for the Azure Eventhub. You can retrieve this from the Azure Portal by navigating to your Eventhub namespace, selecting “Shared access policies,” and locating the key name under the policy details.

    * **Eventhub Shared Access Key**: The shared access key associated with the Eventhub shared access key name. You can retrieve this from the Azure Portal by navigating to your Eventhub namespace, selecting “Shared access policies,” and copying the key value under the selected policy.

    ### Create a Eventhub Depot

    Eventhub is a streaming service platform. Streaming refers to the continuous and real-time transmission of data from a source to a destination. 

    DataOS provides the capability to connect to Eventhub data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of Eventhub, in the type field you will have to specify type 'EVENTHUB' and follow the below steps:

    <Warning>
      Please note that the credentials are directly specified in the Depot manifest using the `connectionSecret`, whereas credentials are referred via [Instance Secret](https://dataos.info/resources/instance_secret/) as `secrets` or `dataosSecrets`.
    </Warning>

    ### Step 1: Create an Instance Secret for securing Eventhub credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Eventhub Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Eventhub Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{"sanityeventhub01"}}
      version: v1
      type: depot
      tags:
        - ${{Eventhub}}
        - ${{Sanity}}
      layer: user
      depot:
        type: "EVENTHUB"
        compute: ${{runnable-default}}
        spec:
          endpoint: ${{"sb://event-hubns.servicebus.windows.net/"}}
        external: ${{true}}
        connectionSecret:
          - acl: r
            type: key-value-properties
            data:
              eh_shared_access_key_name: ${{$EH_SHARED_ACCESS_KEY_NAME}}
              eh_shared_access_key: ${{$EH_SHARED_ACCESS_KEY}}
          - acl: rw
            type: key-value-properties
            data:
              eh_shared_access_key_name: ${{$EH_SHARED_ACCESS_KEY_NAME}}
              eh_shared_access_key: ${{$EH_SHARED_ACCESS_KEY}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{"sanityeventhub01"}}
      version: v2alpha
      type: depot
      tags:
        - ${{Eventhub}}
        - ${{Sanity}}
      layer: user
      depot:
        type: "EVENTHUB"
        compute: ${{runnable-default}}
        eventhub:
          endpoint: ${{"sb://event-hubns.servicebus.windows.net/"}}
        external: ${{true}}
        secrets:
          - name: ${{eh-instance-secret-name}}-r
            allkeys: true

          - name: ${{eh-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="GCS" defaultOpen={false}>
    ### Pre-requisites specific to the GCS Depot

    To create a GCS Depot you must have the following details:

    * **projectid**: The unique identifier of the Google Cloud project where the GCS bucket resides. This can be obtained from the Google Cloud Console by navigating to the "Project Info" section on your project's dashboard. It is used to associate operations with the correct Google Cloud project.

    * **email**: The email address of the Google Cloud service account used for accessing GCS. This can be found in the Google Cloud Console under "IAM & Admin" > "Service Accounts" by selecting the relevant service account. This email is used for authenticating API requests.

    * **gcskey\_json**: The file path to the JSON key associated with the Google Cloud service account. This key is essential for authentication. You can download it from the Google Cloud Console by going to "IAM & Admin" > "Service Accounts," selecting the service account, and clicking on "Keys." Choose "Add Key" > "JSON" to generate and download the key securely.

    ### Create a GCS Depot

    DataOS provides the capability to connect to Google Cloud Storage data using Depot. To create a Depot of Google Cloud Storage follow the below steps:

    ### Step 1: Create an Instance Secret for securing GCS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a GCS Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your GCS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{"sanitygcs01"}}
      version: v1
      type: depot
      tags:
        - ${{GCS}}
        - ${{Sanity}}
      layer: user
      depot:
        type: GCS
        description: ${{"GCS depot for sanity"}}
        compute: ${{runnable-default}}
        spec:
          bucket: ${{"airbyte-minio-testing"}}
          relativePath: ${{"/sanity"}}
        external: ${{true}}
        connectionSecret:
          - acl: ${{rw}}
            type: key-value-properties
            data:
              projectid: ${{$GCS_PROJECT_ID}}
              email: ${{$GCS_ACCOUNT_EMAIL}}
            files:
              gcskey_json: ${{$GCS_KEY_JSON}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{"sanitygcs01"}}
      version: v2alpha
      type: depot
      tags:
        - ${{GCS}}
        - ${{Sanity}}
      layer: user
      depot:
        type: GCS
        description: ${{"GCS depot for sanity"}}
        compute: ${{runnable-default}}
        gcs:
          bucket: ${{"airbyte-minio-testing"}}
          relativePath: ${{"/sanity"}}
        external: ${{true}}
        secrets:
          - name: ${{gcs-instance-secret-name}}-r
            allkeys: true

          - name: ${{gcs-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="JDBC" defaultOpen={false}>
    ### Pre-requisites specific to the JDBC Depot

    To create a JDBC Depot you must have the following details:

    * **username**: The JDBC username used to authenticate and connect to the database. This username is assigned during database setup or can be obtained from the database administrator.

    * **password**: The password associated with the JDBC username for authentication. This is set during account creation or provided securely by the database administrator.

    * **subprotocol**: Specifies the subprotocol used by the JDBC connection, such as `mysql`, `postgresql`, or `oracle`. This identifies the type of database to which the connection is being established. Refer to your database configuration or documentation for the correct value.

    * **host**: The hostname or IP address of the server hosting the database. You can obtain this from the database administrator or your organization’s network team.

    * **port**: The port number on which the database is running. Common examples include `3306` for MySQL or `5432` for PostgreSQL. This information is typically available from the database administrator.

    * **database**: The name of the specific database to which the connection is made. You can find this in your database management system or ask the database administrator for the database name.

    ### Create a JDBC Depot

    DataOS provides the capability to establish a connection to a database using the JDBC driver in order to read data from tables using a Depot. The Depot facilitates access to all schemas visible to the specified user within the configured database. To create a Depot of type ‘JDBC‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing JDBC credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a JDBC Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your JDBC Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: JDBC                                      
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                              
          - acl: rw
            type: key-value-properties
            data:
              username: ${{jdbc-username}}
              password: ${{jdbc-password}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{jdbc-username}}
              password: ${{jdbc-password}}
        spec:                                           
          subprotocol: ${{subprotocol}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params:
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: JDBC                                      
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        jdbc:                                           
          subprotocol: ${{subprotocol}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params:
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="MongoDB" defaultOpen={false}>
    There are two ways to set up the MongoDB connection: Through \`username\` and \`password\`, and certificate authentication. Follow the sections below to create the Depot using both methods.

    ### **Prerequisites**

    The following are the prerequisites for creating a MongoDB Depot.

    **For username and password authentication:**

    1\. Obtain the username and password for MongoDB from your organization's database administrator.

    2\. Also, obtain the MongoDB node's details from the database administrator.

    **For certificate authentication:**

    1\. Obtain the username and password for MongoDB from your organization's database administrator.

    2\. Obtain the MongoDB node's details from the database administrator.

    3\. Obtain the \`.crt\` certificate file for the MongoDB connection from the database administrator for creating the Depot through certificate authentication.

    4\. Java 17 must be installed in your system.

    5\. Create a Keystore and Truststore file from a \`.crt\` file. Follow the below steps to create the keystore and trust store files from the \`.crt\` file.

    * &#x20;Initialize the Keystore, remember the password you create while initializing the Keystore.

    * &#x20;Run the following command, to import a certificate (\`demo-ssl-public-cert.cert\`) into the Java KeyStore with the alias (\`moderncert03\`). It will create a keystore and trust store files in the JAVA folder.

    ```bash
    sudo  keytool -importcert -alias moderncert03 -keystore $JAVA_HOME/lib/security/cacerts -storepass 123456 -file /Users/iamgroot/office/poc-squad/mongodb-poc/demo-ssl-public-cert.cert
    ```

    * &#x20;\`keystore\`: Points to the KeyStore file to update.

    * \`importcert\`: Specifies the operation of importing a certificate.

    * &#x20;\`alias moderncert03\`: Assigns the alias \`moderncert03\` to the imported certificate.

    * &#x20;\`storepass 123456\`: Provide the password for the keystone you created while initializing the keystore.

    * &#x20;\`file\`: Specifies the path to the certificate file to import.

    ### **Steps to create MongoDB Depot through username and password authentication**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret.

    1\. Create a manifest file for Depot containing the following code and update the details.

    ```yaml
    version: ${{v1}} # depot version
    name: ${{"mongodb03"}}
    type: ${{depot}}
    tags:
      - ${{MongoDb}}
      - ${{Sanity}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["mongodb-tmdc.dataos.info:27017"]}}
        params: 
          tls: ${{false}}
      external: ${{true}}
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{root}}
            password: ${{f5ce9b0d972fd9555560}}
    ```

    &#x20;  &#x20;

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    &#x20;  &#x20;

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    &#x20;  &#x20;

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb03         # MongoDB depot name
    ```

    &#x20;  &#x20;

    5\. Apply the Scanner Workflow by executing the below command.

    ```yaml
    dataos-ctl apply -f /home/office/workflow/depot_scanner.yaml
    ```

    ### &#x20;**Steps to create MongoDB Depot through certificate authentication**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret.

    1\. Create a manifest file for Depot containing the following code and update the details.

    &#x20;  &#x20;

    ```yaml
    version: ${{v1}} # depot version
    name: ${{"mongodb"}}
    type: ${{depot}}
    tags:
      - ${{MongoDb}}
      - ${{Sanity}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["SG-demo-66793.servers.mongodirector.com:27071"]}}
        params: 
          tls: ${{true}}
      external: ${{true}}
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{admin}}
            password: ${{Kl6swyCRLPteqljkdyrf}}
            keyStorePassword: ${{123456}}
            trustStorePassword: ${{123456}}
          files:
            ca_file: ${{mongodb-poc/demo-ssl-public-cert.cert}}
            key_store_file: ${{/Library/Java/JavaVirtualMachines/jdk-22.jdk/Contents/Home/lib/security/cacerts}}
            trust_store_file: ${{/Library/Java/JavaVirtualMachines/jdk-22.jdk/Contents/Home/lib/security/cacerts}}
    ```

    &#x20;  &#x20;

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    &#x20;  &#x20;

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    &#x20;

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    &#x20;  &#x20;

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb         # MongoDB depot name
    ```

    &#x20;  &#x20;

    5\. Apply the Scanner Workflow by executing the below command.

    ```bash
    dataos-ctl apply -f /home/office/workflow/udscoremongodb.yaml
    ```

    ### &#x20;**Steps to create MongoDB Depot through VPCE**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret. VPCE-based Depot is similar to certificate authentication Depot just additional parameters have been added that allow connection with VPCs.

    1\. Create a manifest file for Depot containing the following code and update the details.

    ```yaml
    version: ${{v1}}
    name: ${{"ldmmongodb"}}
    type: ${{depot}}
    tags:
      - ${{Mongodb}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["SG-demo-664533.servers.mongodirector.com:27071"]}}
        params: 
          tls: ${{true}}
          tlsAllowInvalidHostnames: ${{true}}
          directConnection: ${{true}}
      external: ${{true}}
      secrets:
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{admin}}
            password: ${{Kl6lyCRLPteqljkdyrf}}
            keyStorePassword: ${{changeit}}
            trustStorePassword: ${{changeit}}
          files:
            ca_file: ${{/Users/iamgroot/Downloads/ca-chain-cn-qa.crt}}
            key_store_file: ${{$JAVA_HOME/lib/security/cacerts}}
            trust_store_file: ${{$JAVA_HOME/lib/security/cacerts}}
    ```

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    &#x20;  &#x20;

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb         # MongoDB depot name
    ```

    5\. Apply the Scanner Workflow by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/workflow/udscoremongodb.yaml
    ```
  </Accordion>

  <Accordion title="Microsoft SQL Server" defaultOpen={false}>
    ### Pre-requisites specific to the MSSQL Depot

    To create a MSSQL Depot you must have the following details:

    * **Username**: The MSSQL username used to authenticate and access your MSSQL database. You can retrieve this from the MSSQL database admin by asking them to provide the username for your account.

    * **Password**: The password associated with the MSSQL username for authentication. You can obtain this from the MSSQL database admin, or if the password is already set, you will need to securely retrieve it.

    ### Create a MSSQL Depot

    DataOS allows you to connect to a Microsoft SQL Server database and read data from tables using Depots. A Depot provides access to all tables within the specified schema of the configured database. You can create multiple Depots to connect to different SQL servers or databases. To create a Depot of type ‘SQLSERVER‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing MSSQL credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a MSSQL Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your MSSQL Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="MySQL" defaultOpen={false}>
    ### Pre-requisites specific to the MySQL Depot

    To create a MySQL Depot you must have the following details:

    * **Username**: The MySQL username used to authenticate and access your MySQL database. This can be obtained from the database admin who manages user access.

    * **Password**: The password associated with the MySQL username for authentication. This is typically set when the user account is created and must be provided by the MySQL admin or securely retrieved if forgotten.

    ### Create a MySQL Depot

    DataOS allows you to connect to a MySQL database and read data from tables using Depots. A Depot provides access to all tables within the specified schema of the configured database. You can create multiple Depots to connect to different MySQL servers or databases. To create a Depot of type ‘MYSQL‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing MySQL credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a MySQL Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your MySQL Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Oracle" defaultOpen={false}>
    ### Pre-requisites specific to the Oracle Depot

    To create a Oracle Depot you must have the following details:

    * **Username**: The Oracle username used to authenticate and access your Oracle database. This can be obtained from the database administrator who manages user access.

    * **Password**: The password associated with the Oracle username for authentication. This can be obtained from the database administrator, or if the password is already set, you will need to securely retrieve it.

    ### Create a Oracle Depot

    DataOS allows you to connect to an Oracle database and access data from tables using Depots. A Depot provides access to all schemas within the specified service in the configured database. You can create multiple Depots to connect to different Oracle servers or databases. To create a Depot of type ‘ORACLE‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Oracle credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Oracle Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Oracle Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{dropzone}}
        - ${{oracle}}
      layer: user
      depot:
        type: ORACLE                                    
        description: ${{"Oracle Sample data"}}
        spec:
          subprotocol: ${{subprotocol}} # for example "oracle:thin"                                     
          host: ${{host}}
          port: ${{port}}
          service: ${{service}}
        external: ${{true}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}  
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{dropzone}}
        - ${{oracle}}
      layer: user
      depot:
        type: ORACLE                                    
        description: ${{"Oracle Sample data"}}
        oracle:
          subprotocol: ${{subprotocol}} # for example "oracle:thin"                                     
          host: ${{host}}
          port: ${{port}}
          service: ${{service}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="PostgreSQL" defaultOpen={false}>
    ### Pre-requisites specific to the Postgres Depot

    To create a Postgres Depot you must have the following details:

    * **Database name**: The name of the specific PostgreSQL database you want to connect to. This can be provided by the database administrator or found in the database management system where the database was created.

    * **Hostname/URL of the server**: The hostname or URL of the server where the PostgreSQL database is hosted. This is typically an IP address or a fully qualified domain name (FQDN). You can obtain this from your database administrator or the team managing the PostgreSQL server.

    * **Parameters**: Additional optional parameters required for the connection, such as SSL settings or timeout configurations. These can be provided by the database administrator or found in your PostgreSQL server's documentation, depending on the connection requirements.

    * **Username**: The username used for authentication to access the PostgreSQL database. This is created when the user account is set up and can be provided by the database administrator.

    * **Password**: The password associated with the PostgreSQL username for authentication. This is set during user account creation and must be securely obtained from the database administrator if forgotten.

    ### Create a Postgres Depot

    DataOS allows you to connect to a PostgreSQL database and read data from tables using Depots. A Depot provides access to all schemas visible to the specified user in the configured database. To create a Depot of type ‘POSTGRESQL‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Postgres credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Postgres Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Postgres Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <Warning>
      Use the below templates, if self-signed certificate is enabled.
    </Warning>

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{postgresdb}}
      version: v1
      type: depot
      layer: user
      depot:
        type: JDBC                  
        description: ${{To write data to postgresql database}}
        external: ${{true}}
        connectionSecret:           
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}  
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
        spec:                        
          subprotocol: "postgresql"
          host: ${{host}}
          port: ${{port}}
          database: ${{postgres}}
          params: #Required 
            sslmode: ${{disable}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{postgresdb}}
      version: v2alpha
      type: depot
      layer: user
      depot:
        type: JDBC                  
        description: ${{To write data to postgresql database}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        postgresql:                        
          subprotocol: "postgresql"
          host: ${{host}}
          port: ${{port}}
          database: ${{postgres}}
          params: #Required 
            sslmode: ${{disable}}
      ```
    </CodeGroup>

    <Warning>
      Use the below templates, if self-signed certificate is not enabled.
    </Warning>

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: Depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      Depot:
        type: POSTGRESQL
        description: ${{description}}
        external: true
        connectionSecret:                               
          - acl: rw
            type: key-value-properties
            data:
              username: ${{postgresql-username}}
              password: ${{posgtresql-password}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{postgresql-username}}
              password: ${{postgresql-password}}
        spec:                                          
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params: # Optional
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```

      ```yaml Instance Secret Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: Depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      Depot:
        type: POSTGRESQL
        description: ${{description}}
        external: true
        secrets:
          - name: ${{instance-secret-name}}-r
            allkeys: true

          - name: ${{instance-secret-name}}-rw
            allkeys: true
        postgresql:                                          
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params: # Optional
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Amazon Redshift" defaultOpen={false}>
    ### Pre-requisites specific to the Redshift Depot

    To create a Redshift Depot you must have the following details:

    * **Hostname**: The hostname or endpoint of the Redshift cluster, which specifies the server address to connect to. You can obtain this from the AWS Management Console under the Redshift cluster details or ask the administrator managing the cluster.

    * **Port**: The port number on which the Redshift cluster is listening. The default port for Redshift is `5439`, but it may differ based on your setup. Check the cluster details in the AWS Management Console or consult your database administrator.

    * **Database name**: The name of the specific database within the Redshift cluster you want to connect to. This can be found in the AWS Management Console or provided by the database administrator.

    * **Username and password**: The credentials used to authenticate and access the Redshift database. These are set up when the user account is created and must be securely obtained from the Redshift administrator.

    When accessing the Redshift database in workflows or other DataOS Resources, additional details are required:

    * **Bucket name**: The name of the S3 bucket where the data resides. You can find this in the AWS S3 Console or consult the administrator managing the data storage.

    * **Relative path**: The path within the S3 bucket pointing to the data you want to access. This path is typically structured based on your data organization and can be obtained from the team managing the data or the S3 Console.

    * **AWS access key**: The Access Key ID used to authenticate and authorize API requests to AWS. You can obtain this from the AWS IAM (Identity and Access Management) Console under your user’s security credentials or request it from your AWS administrator.

    * **AWS secret key**: The Secret Access Key associated with your AWS Access Key ID. This key is also available in the AWS IAM Console under security credentials and must be securely stored. If you do not have access, request it from your AWS administrator.

    ### Create a Redshift Depot

    DataOS provides the capability to establish a connection with the Amazon Redshift database. We have provided the template for the manifest file to establish this connection. To create a Depot of type ‘REDSHIFT‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Redshift credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Redshift Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Redshift Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{redshift-depot-name}}
      version: v1
      type: depot
      tags:
        - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
        type: REDSHIFT
        spec:
          host: ${{hostname}}
          subprotocol: ${{subprotocol}}
          port: ${{5439}}
          database: ${{sample-database}}
          bucket: ${{tmdc-dataos}}
          relativePath: ${{development/redshift/data_02/}}
        external: ${{true}}
        connectionSecret:
          - acl: ${{rw}}
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
              awsaccesskeyid: ${{access key}}
              awssecretaccesskey: ${{secret key}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{redshift-depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
        type: REDSHIFT
        redshift:
          host: ${{hostname}}
          subprotocol: ${{subprotocol}}
          port: ${{5439}}
          database: ${{sample-database}}
          bucket: ${{tmdc-dataos}}
          relativePath: ${{development/redshift/data_02/}}
        external: ${{true}}
        secrets:
          - name: ${{redshift-instance-secret-name}}-r
            allkeys: true

          - name: ${{redshift-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Amazon S3" defaultOpen={false}>
    ### Pre-requisites specific to the S3 Depot

    To create a S3 Depot you must have the following details:

    * **AWS access key ID**: The Access Key ID used to authenticate and authorize API requests to your AWS account. This can be obtained from the AWS IAM (Identity and Access Management) Console under your user’s security credentials or requested from your AWS administrator.

    * **AWS bucket name**: The name of the Amazon S3 bucket where the data resides. You can find this in the AWS S3 Console under the list of buckets or request it from the administrator managing the storage.

    * **Secret access key**: The Secret Access Key associated with your AWS Access Key ID, required for secure API requests. This is available in the AWS IAM Console under your user’s security credentials. Ensure that it is securely stored and shared only with authorized personnel.

    * **Scheme**: The scheme specifies the protocol to be used for the connection, such as `s3` or `https`. This information depends on your system’s configuration and can be confirmed with the team managing the connection setup or workflow.

    * **Relative Path**: The path within the S3 bucket that points to the specific data or folder you want to access. This path is typically structured according to how your data is organized and can be obtained from the team managing the data or the AWS S3 Console.

    * **Format**: The file format of the data stored in the S3 bucket, such as `CSV`, `Parquet`, or `JSON`. This information is determined by the structure of your data and can be confirmed with the team managing the data storage.

    ### Create a S3 Depot

    Azure Blob File System Secure (ABFSS) is an object storage system. Object stores are distributed storage systems designed to store and manage large amounts of unstructured data.

    DataOS enables the creation of a Depot of type 'Bigquery' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing S3 credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a S3 Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your S3 Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      description: ${{description}}
      depot:
        type: S3                                          
        external: ${{true}}
        spec:                                            
          scheme: ${{s3a}}
          bucket: ${{project-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
        connectionSecret:                                
          - acl: rw
            type: key-value-properties
            data:
              accesskeyid: ${{AWS_ACCESS_KEY_ID}}
              secretkey: ${{AWS_SECRET_ACCESS_KEY}}
              awsaccesskeyid: ${{AWS_ACCESS_KEY_ID}}
              awssecretaccesskey: ${{AWS_SECRET_ACCESS_KEY}}
          - acl: r
            type: key-value-properties
            data:
              accesskeyid: ${{AWS_ACCESS_KEY_ID}}
              secretkey: ${{AWS_SECRET_ACCESS_KEY}}
              awsaccesskeyid: ${{AWS_ACCESS_KEY_ID}}
              awssecretaccesskey: ${{AWS_SECRET_ACCESS_KEY}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      description: ${{description}}
      depot:
        type: S3                                          
        external: ${{true}}
        s3:                                            
          scheme: ${{s3a}}
          bucket: ${{project-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
        secrets:
          - name: ${{s3-instance-secret-name}}-r
            allkeys: true

          - name: ${{s3-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Snowflake" defaultOpen={false}>
    ### Pre-requisites specific to the Snowflake Depot

    To create a Snowflake Depot you must have the following details:

    * **Snowflake Account URL**: The unique URL used to access your Snowflake account, typically in the format `https://<account_name>.snowflakecomputing.com`. You can retrieve this from your Snowflake admin or find it in your Snowflake login credentials email.

    * **Snowflake Username**: The username used to log in to your Snowflake account. This is usually provided by the Snowflake admin when your account is created.

    * **Snowflake User Password**: The password associated with your Snowflake username for authentication. This password is set during account creation or upon first login. If forgotten, you may need to reset it via the Snowflake login page or contact your Snowflake admin.

    * **Snowflake Database Name**: The name of the database in Snowflake that you need to connect to. You can find this in the Snowflake console under the Databases section or by consulting the team managing the Snowflake environment.

    * **Database Schema**: The specific schema within the Snowflake database where your required table resides. This can also be found in the Snowflake console under the relevant database or provided by the team managing the database structure.

    ### Create a Snowflake Depot

    DataOS provides integration with Snowflake, allowing you to seamlessly read data from Snowflake tables using Depots. Snowflake is a cloud-based data storage and analytics data warehouse offered as a Software-as-a-Service (SaaS) solution. It utilizes a new SQL database engine designed specifically for cloud infrastructure, enabling efficient access to Snowflake databases. To create a Depot of type 'SNOWFLAKE', follow the below steps:

    ### Step 1: Create an Instance Secret for securing Snowflake credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Snowflake Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Snowflake Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{snowflake-depot}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        spec:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        connectionSecret:
          - acl: rw
            type: key-value-properties
            data:
              username: ${{snowflake-username}}
              password: ${{snowflake-password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{snowflake-depot}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        snowflake:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        secrets:
          - name: ${{redshift-instance-secret-name}}-r
            allkeys: true

          - name: ${{redshift-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Azure WASBS" defaultOpen={false}>
    ### Pre-requisites specific to the WASBS Depot

    To create a WASBS Depot you must have the following details:

    * **Storage Account Name**: The name of your Azure Storage account, which identifies your storage resource in Azure. You can retrieve this from the Azure portal by navigating to your Storage Account under the Storage Accounts service.

    * **Storage Account Key**: The access key used to authenticate and access your Azure Storage account. This can be obtained from the Azure portal by selecting your Storage Account, navigating to the "Access Keys" section, and copying the required key. Ensure it is securely stored.

    * **Container**: The name of the container within your Azure Storage account that holds the required data. You can find this in the Azure portal under your Storage Account by navigating to the "Containers" section.

    * **Relative Path**: The specific path to the file or directory within the container. This is typically provided by the team managing the data or can be derived from the structure of your container in the Azure portal.

    * **Format**: The data format of the files stored within the container, such as CSV, JSON, or Parquet. This information is usually known based on the type of data being stored and consumed. If unsure, consult the team responsible for the data.

    ### Create a WASBS Depot

    DataOS enables the creation of a Depot of type 'WASBS' to facilitate the reading of data stored in Azure Data Lake Storage. This Depot enables access to the storage account, which can contain multiple containers. A container serves as a grouping of multiple blobs. It is recommended to define a separate Depot for each container.To create a Depot of type ‘WASBS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing WASBS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a WASBS Depot manifest file

    Create a manifest file to hold the configuration details for your WASBS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: WASBS                                      
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                          
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: WASBS                                      
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{wasbs-instance-secret-name}}-r
            allkeys: true

          - name: ${{wasbs-instance-secret-name}}-rw
            allkeys: true
        wasbs:                                          
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Apache Pulsar" defaultOpen={false}>
    ### Pre-requisites specific to the Pulsar Depot

    To create a Pulsar Depot you must have the following details:

    * **Admin URL**: The administrative endpoint URL of your Apache Pulsar cluster. This URL is used to perform administrative operations on the cluster. You can obtain this from the Pulsar admin or from the configuration details of the Pulsar cluster, typically provided during setup or accessible in the deployment documentation.

    * **Service URL**: The service endpoint URL of your Apache Pulsar cluster. This URL is used by producers and consumers to connect to the cluster and exchange messages. You can retrieve this from the Pulsar admin or by referring to the connection details in the Pulsar cluster configuration.

    ### Create a Pulsar Depot

    DataOS provides the capability to create a Depot of type 'PULSAR' for reading topics and messages stored in Pulsar. This Depot facilitates the consumption of published topics and processing of incoming streams of messages. To create a Depot of type 'PULSAR', follow the below steps:

    ### Step 1: Create a Pulsar Depot manifest file

    Create a manifest file to hold the configuration details for your Pulsar Depot.&#x20;

    <CodeGroup>
      ```yaml Pulsar Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: PULSAR       
        description: ${{description}}
        external: ${{true}}
        spec:              
          adminUrl: ${{admin-url}}
          serviceUrl: ${{service-url}}
          tenant: ${{system}}
      # Ensure to obtain the correct tenant name and other specifications from your organization.
      ```
    </CodeGroup>

    ### Step 2: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Kafka" defaultOpen={false}>
    ### Pre-requisites specific to the Kafka Depot

    To create a Kafka Depot you must have the following details:

    * **KAFKA Broker List**: A comma-separated list of broker addresses in the Kafka cluster (e.g., `broker1:9092,broker2:9092`). The broker list allows the connection to discover all topics and partitions in the cluster. This information can be obtained from the Kafka admin or the cluster configuration file, usually found in the `server.properties` file.

    * **Schema Registry URL**: The URL of the Schema Registry associated with your Kafka cluster, which stores the schemas for the data being produced and consumed. This is required if your Kafka topics use Avro or other schema-based serialization formats. You can obtain this URL from the Kafka admin or the configuration details of the Schema Registry setup.

    ### Create a Kafka Depot

    DataOS allows you to create a Depot of type 'KAFKA' to read live topic data. This Depot enables you to access and consume real-time streaming data from Kafka. To create a Depot of type 'KAFKA', follow the below steps:

    ### Step 1: Create a Kafka Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Kafka Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 2: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Elasticsearch" defaultOpen={false}>
    ### Pre-requisites specific to the Elasticsearch Depot

    To create a Elasticsearch Depot you must have the following details:

    * **Username**: The username used to authenticate and access the Elasticsearch cluster. This is typically created and provided by the Elasticsearch admin during user setup or role assignment.

    * **Password**: The password associated with the Elasticsearch username for authentication. This is set during account creation and can be obtained securely from the Elasticsearch admin if forgotten.

    * **Nodes (Hostname/URL of the server and ports)**: The addresses of one or more Elasticsearch nodes, including the hostname or URL and the port (e.g., `node1.example.com:9200`). These nodes form part of the cluster and are used to establish the connection. This information can be retrieved from the Elasticsearch admin or by checking the cluster configuration details.

    ### Create a Elasticsearch Depot

    DataOS provides the capability to connect to Elasticsearch data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of type ‘ELASTICSEARCH‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Elasticsearch credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Elasticsearch Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Elasticsearch Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ELASTICSEARCH              
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                
          - acl: rw
            values:
              username: ${{username}}
              password: ${{password}}
          - acl: r
            values:
              username: ${{username}}
              password: ${{password}}
        spec:                           
          nodes: ${{["localhost:9092", "localhost:9093"]}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ELASTICSEARCH              
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        elasticsearch:                           
          nodes: ${{["localhost:9092", "localhost:9093"]}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>

  <Accordion title="Opensearch" defaultOpen={false}>
    ### Pre-requisites specific to the Opensearch Depot

    To create a Opensearch Depot you must have the following details:

    * **Username**: The username used to authenticate and access the OpenSearch cluster. This is typically set up by the OpenSearch admin and assigned to users with specific access permissions.

    * **Password**: The password associated with the OpenSearch username for authentication. This is created during user setup and should be securely retrieved from the OpenSearch admin if not known.

    * **Nodes (Hostname/URL of the server and ports)**: The addresses of one or more OpenSearch nodes, including the hostname or URL and the port (e.g., `node1.opensearch.example.com:9200`). These nodes are part of the cluster and are necessary to establish the connection. This information can be obtained from the OpenSearch admin or by referring to the cluster's configuration details.

    ### Create a Opensearch Depot

    DataOS provides the capability to connect to Opensearch data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of Opensearch, in the type field you will have to specify type ‘ELASTICSEARCH‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Opensearch credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Opensearch Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Opensearch Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: OPENSEARCH             
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                
          - acl: rw
            values:
              username: ${{username}}
              password: ${{password}}
          - acl: r
            values:
              username: ${{opensearch-username}}
              password: ${{opensearch-password}}
        spec:                           
          nodes:
            - ${{nodes}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: OPENSEARCH           
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        elasticesearch:                           
          nodes:
            - ${{nodes}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>
  </Accordion>
</AccordionGroup>

The manifest configuration file for a Depot can be divided into four main sections: [Resource section](https://dataos.info/resources/depot/#configure-resource-section), [Depot-specific section](https://dataos.info/resources/depot/#configure-depot-specific-section), [Connection Secrets section](https://dataos.info/resources/depot/#configure-connection-secrets-section), and [Specifications section](https://dataos.info/resources/depot/#configure-spec-section%C2%B6). Each section serves a distinct purpose and contains specific attributes.

**Configure Resource section**

The Resource section of the manifest configuration file consists of attributes that are common across all resource types. For more details regarding attributes in the Resource section, refer to the link: [Attributes of Resource section.](https://dataos.info/resources/manifest_attributes/)

**Configure Depot-specific section**

The Depot-specific section of the configuration file includes key-value properties specific to the Depot-type being created. Each Depot type represents a Depot created for a particular data source. Multiple Depots can be established for the same data source, and they will be considered as a single Depot type.&#x20;

**Configure connection Secrets section**

The configuration of connection secrets is specific to each Depot type and depends on the underlying data source. The details for these connection secrets, such as credentials and authentication information, should be obtained from your enterprise or data source provider. For commonly used data sources, we have compiled the connection secrets [here.](https://dataos.info/resources/depot/depot_config_templates/) Please refer to these templates for guidance on how to configure the connection secrets for your specific data source.

🗣 The credentials you use here need to have access to the schemas in the configured database.

**Examples**

Here are examples demonstrating how the key-value properties can be defined for different Depot-types:

**Alternative approach: using Instance Secret**

[Instance Secret](https://dataos.info/resources/instance_secret/) is also a [Resource](https://dataos.info/resources/) in DataOS that allows users to securely store sensitive piece of information such as username, password, etc. Using Secrets in conjunction with [Depots](https://dataos.info/resources/depot/), [Stacks](https://dataos.info/resources/stacks/) allows for decoupling of sensitive information from Depot and Stack YAMLs. For more clarity, let’s take the example of MySQL data source to understand how you can use Instance Secret Resource for Depot creation:

* Create an Instance Secret file with the details on the connection secret:

[](https://dataos.info/resources/depot/#__codelineno-7-1)`name: $${{mysql-secret}}`
[](https://dataos.info/resources/depot/#__codelineno-7-2)`version: v1      `
[](https://dataos.info/resources/depot/#__codelineno-7-3)`type: instance-secret`
[](https://dataos.info/resources/depot/#__codelineno-7-4)`instance-secret:`
[](https://dataos.info/resources/depot/#__codelineno-7-5)`  type: key-value-properties`
[](https://dataos.info/resources/depot/#__codelineno-7-6)`  acl: rw`
[](https://dataos.info/resources/depot/#__codelineno-7-7)`  data:`
[](https://dataos.info/resources/depot/#__codelineno-7-8)`    connection-user: $${{user}}`
[](https://dataos.info/resources/depot/#__codelineno-7-9)`    connection-password: $${{password}}`

* Apply this YAML file on DataOS CLI

[](https://dataos.info/resources/depot/#__codelineno-8-1)`dataos-ctl apply -f $${{path/instance_secret.yaml}}`

For example, if a user wishes to create a MySQL Depot, they can define a Depot configuration file as follows:

YAML Configuration File

[](https://dataos.info/resources/depot/#__codelineno-9-1)[](https://dataos.info/resources/depot/#__codelineno-9-2)[](https://dataos.info/resources/depot/#__codelineno-9-3)[](https://dataos.info/resources/depot/#__codelineno-9-4)[](https://dataos.info/resources/depot/#__codelineno-9-5)[](https://dataos.info/resources/depot/#__codelineno-9-6)[](https://dataos.info/resources/depot/#__codelineno-9-7)[](https://dataos.info/resources/depot/#__codelineno-9-8)[](https://dataos.info/resources/depot/#__codelineno-9-9)[](https://dataos.info/resources/depot/#__codelineno-9-10)[](https://dataos.info/resources/depot/#__codelineno-9-11)[](https://dataos.info/resources/depot/#__codelineno-9-12)[](https://dataos.info/resources/depot/#__codelineno-9-13)[](https://dataos.info/resources/depot/#__codelineno-9-14)[](https://dataos.info/resources/depot/#__codelineno-9-15)[](https://dataos.info/resources/depot/#__codelineno-9-16)[](https://dataos.info/resources/depot/#__codelineno-9-17)[](https://dataos.info/resources/depot/#__codelineno-9-18)[](https://dataos.info/resources/depot/#__codelineno-9-19)[](https://dataos.info/resources/depot/#__codelineno-9-20)[](https://dataos.info/resources/depot/#__codelineno-9-21)[](https://dataos.info/resources/depot/#__codelineno-9-22)

To learn more about Instance Secrets as a Resource and their usage, refer to the documentation [here](https://dataos.info/resources/instance_secret/).

**Configure spec section**

The `spec` section in the manifest configuration file plays a crucial role in directing the Depot to the precise location of your data and providing it with the hierarchical structure of the data source. By defining the specification parameters, you establish a mapping between the data and the hierarchy followed within DataOS.

Let's understand this hierarchy through real-world examples:

[BigQuery](https://dataos.info/resources/depot/#bigquery_1)[Amazon S3](https://dataos.info/resources/depot/#amazon-s3)[Kafka](https://dataos.info/resources/depot/#kafka)

In the case of BigQuery, the data is structured as "Projects" containing "Datasets" that, in turn, contain "Tables". In DataOS terminology, the "Project" corresponds to the "Depot", the "Dataset" corresponds to the "Collection", and the "Table" corresponds to the "Dataset".

Consider the following structure in [BigQuery](https://dataos.info/resources/depot/depot_config_templates/google_bigquery/):

* Project name: `bigquery-public-data` (Depot)

* Dataset name: `covid19_usa` (Collection)

* Table name: `datafile_01` (Dataset)

The UDL for accessing this data would be `dataos://bigquery-public-data:covid19_usa/datafile_01`.

In the YAML example below, the necessary values are filled in to create a [BigQuery](https://dataos.info/resources/depot/depot_config_templates/google_bigquery/) Depot:

Bigquery Depot manifest Configuration

[](https://dataos.info/resources/depot/#__codelineno-10-1)[](https://dataos.info/resources/depot/#__codelineno-10-2)[](https://dataos.info/resources/depot/#__codelineno-10-3)[](https://dataos.info/resources/depot/#__codelineno-10-4)[](https://dataos.info/resources/depot/#__codelineno-10-5)[](https://dataos.info/resources/depot/#__codelineno-10-6)[](https://dataos.info/resources/depot/#__codelineno-10-7)[](https://dataos.info/resources/depot/#__codelineno-10-8)[](https://dataos.info/resources/depot/#__codelineno-10-9)[](https://dataos.info/resources/depot/#__codelineno-10-10)[](https://dataos.info/resources/depot/#__codelineno-10-11)[](https://dataos.info/resources/depot/#__codelineno-10-12)

In this example, the Depot is named "covidbq" and references the project "bigquery-public-data" within Google Cloud. As a result, all the datasets and tables within this project can be accessed using the UDL `dataos://covidbq:<collection name>/<dataset name>`.

By appropriately configuring the specifications, you ensure that the Depot is accurately linked to the data source's structure, enabling seamless access and manipulation of datasets within DataOS.

### **Apply Depot YAML**[¶](https://dataos.info/resources/depot/#apply-depot-yaml "Permanent link")

Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI, using the command given below:

[](https://dataos.info/resources/depot/#__codelineno-14-1)`dataos-ctl apply -f ${{yamlfilepath}}`

## **How to manage a Depot?**[¶](https://dataos.info/resources/depot/#how-to-manage-a-depot "Permanent link")

### **Verify Depot creation**[¶](https://dataos.info/resources/depot/#verify-depot-creation "Permanent link")

To ensure that your Depot has been successfully created, you can verify it in two ways:

* Check the name of the newly created Depot in the list of Depots where you are named as the owner:

[](https://dataos.info/resources/depot/#__codelineno-15-1)`dataos-ctl get -t `depot

* Alternatively, retrieve the list of all Depots created in your organization:

[](https://dataos.info/resources/depot/#__codelineno-16-1)`dataos-ctl get -t depot `-a

You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

### **Delete Depot**[¶](https://dataos.info/resources/depot/#delete-depot "Permanent link")

📖 **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.

If you need to delete a Depot, use the following command in the DataOS CLI:

[](https://dataos.info/resources/depot/#__codelineno-17-1)`dataos-ctl delete -t depot -n ${{name of Depot}}`

By executing the above command, the specified Depot will be deleted from your DataOS environment.

## **How to utilize Depots?**[¶](https://dataos.info/resources/depot/#how-to-utilize-depots "Permanent link")

Once a Depot is created, you can leverage its Uniform Data Links (UDLs) to access data without physically moving it. The UDLs play a crucial role in various scenarios within DataOS.

### **Work with Stacks**[¶](https://dataos.info/resources/depot/#work-with-stacks "Permanent link")

Depots are compatible with different Stacks in DataOS. [Stacks](https://dataos.info/resources/stacks/) provide distinct approaches to interact with the system and enable various programming paradigms in DataOS. Several Stacks are available that can be utilized with Depots, including [Scanner](https://dataos.info/resources/stacks/scanner/) for introspecting Depots, [Flare](https://dataos.info/resources/stacks/flare/) for data ingestion, transformation, syndication, etc., [Benthos](https://dataos.info/resources/stacks/benthos/) for stream processing and [Data Toolbox](https://dataos.info/resources/stacks/data_toolbox/) for managing [Icebase](https://dataos.info/resources/depot/icebase/) DDL and DML.

[Flare](https://dataos.info/resources/stacks/flare/) and [Scanner](https://dataos.info/resources/stacks/scanner/) Stacks are supported by all Depots, while [Benthos](https://dataos.info/resources/stacks/benthos/), the stream-processing Stack, is compatible with read/write operations from streaming Depots like [Fastbase](https://dataos.info/resources/depot/fastbase/) and Kafka Depots.

The UDL references are used as addresses for your input and output datasets within the manifest configuration file.

### **Limit data source's file format**[¶](https://dataos.info/resources/depot/#limit-data-sources-file-format "Permanent link")

Another important function that a Depot can play is to limit the file type which you can read from and write to a particular data source. In the `spec` section of manifest config file, simply mention the `format` of the files you want to allow access for.

[](https://dataos.info/resources/depot/#__codelineno-18-1)`depot:`
[](https://dataos.info/resources/depot/#__codelineno-18-2)`  type: S3`
[](https://dataos.info/resources/depot/#__codelineno-18-3)`  description: $${{description}}`
[](https://dataos.info/resources/depot/#__codelineno-18-4)`  external: true`
[](https://dataos.info/resources/depot/#__codelineno-18-5)`  spec:`
[](https://dataos.info/resources/depot/#__codelineno-18-6)`    scheme: $${{s3a}}`
[](https://dataos.info/resources/depot/#__codelineno-18-7)`    bucket: $${{bucket-name}}`
[](https://dataos.info/resources/depot/#__codelineno-18-8)`     relativePath: "raw"  `
[](https://dataos.info/resources/depot/#__codelineno-18-9)`    format: $${{format}}  # mention the file format, such as JSON`

For file based systems, if you define the format as ‘Iceberg’, you can choose the meta-store catalog between Hadoop and Hive. This is how you do it:

[](https://dataos.info/resources/depot/#__codelineno-19-1)`depot:`
[](https://dataos.info/resources/depot/#__codelineno-19-2)`  type: ABFSS`
[](https://dataos.info/resources/depot/#__codelineno-19-3)`  description: "ABFSS Iceberg Depot for sanity"`
[](https://dataos.info/resources/depot/#__codelineno-19-4)`  compute: runnable-default`
[](https://dataos.info/resources/depot/#__codelineno-19-5)`  spec:`
[](https://dataos.info/resources/depot/#__codelineno-19-6)`     account:  `
[](https://dataos.info/resources/depot/#__codelineno-19-7)`     container:  `
[](https://dataos.info/resources/depot/#__codelineno-19-8)`    relativePath:`
[](https://dataos.info/resources/depot/#__codelineno-19-9)`    format: ICEBERG`
[](https://dataos.info/resources/depot/#__codelineno-19-10)`    endpointSuffix:`
[](https://dataos.info/resources/depot/#__codelineno-19-11)`    icebergCatalogType: Hive`

If you do not mention the catalog name as Hive, it will use Hadoop as the default catalog for Iceberg format.

![Flow when Hive is chosen as the catalog type](https://dataos.info/resources/depot/depot_catalog.png)

*Flow when Hive is chosen as the catalog type*

Hive, automatically keeps the pointer updated to the latest metadata version. If you use Hadoop, you have to manually do this by running the set metadata command as described on this page: [Set Metadata](https://dataos.info/resources/depot/icebase/).

### **Scan and catalog metadata**[¶](https://dataos.info/resources/depot/#scan-and-catalog-metadata "Permanent link")

By running the [Scanner](https://dataos.info/resources/stacks/scanner/), you can scan the metadata from a source system via the Depot interface. Once the metadata is scanned, you can utilize [Metis](https://dataos.info/interfaces/metis/) to catalog and explore the metadata in a structured manner. This allows for efficient management and organization of data resources.

### **Add Depot to Cluster sources to query the data**[¶](https://dataos.info/resources/depot/#add-depot-to-cluster-sources-to-query-the-data "Permanent link")

To enable the [Minerva](https://dataos.info/resources/cluster/#minerva) Query Engine to access a specific source system, you can add the Depot to the list of sources in the [Cluster](https://dataos.info/resources/cluster/). This allows you to query the data using the DataOS [Workbench](https://dataos.info/interfaces/workbench/).

### **Create Policies upon Depots to govern the data**[¶](https://dataos.info/resources/depot/#create-policies-upon-depots-to-govern-the-data "Permanent link")

[Access](https://dataos.info/resources/#access-policy) and [Data Policies](https://dataos.info/resources/#data-policy) can be created upon Depots to govern the data. This helps in reducing data breach risks and simplifying compliance with regulatory requirements. Access Policies can restrict access to specific Depots, collections, or datasets, while Data Policies allow you to control the visibility and usage of data.

### **Building data models**[¶](https://dataos.info/resources/depot/#building-data-models "Permanent link")

You can use Lens to create Data Models on top of Depots and explore them using the [Lens App UI](https://dataos.info/interfaces/lens/).

## **Supported storage architectures in DataOS**[¶](https://dataos.info/resources/depot/#supported-storage-architectures-in-dataos "Permanent link")

DataOS Depots facilitate seamless connectivity with diverse storage systems while eliminating the need for data relocation. This resolves challenges pertaining to accessibility across heterogeneous data sources. However, the escalating intricacy of pipelines and the exponential growth of data pose potential issues, resulting in cumbersome, expensive, and unattainable storage solutions. In order to address this critical concern, DataOS introduces support for two distinct and specialized storage architectures - [Icebase](https://dataos.info/resources/depot/icebase/) Depot, the Unified Lakehouse designed for OLAP data, and [Fastbase](https://dataos.info/resources/depot/fastbase/) Depot, the Unified Streaming solution tailored for handling streaming data.

### **Icebase**[¶](https://dataos.info/resources/depot/#icebase "Permanent link")

Icebase-type Depots are designed to store data suitable for OLAP processes. It offers built-in functionalities such as [schema evolution](https://dataos.info/resources/depot/icebase/#schema-evolution), [upsert commands](https://dataos.info/resources/depot/icebase/#creating-and-getting-datasets), and [time-travel capabilities](https://dataos.info/resources/depot/icebase/#maintenance-snapshot-modelling-and-metadata-listing) for datasets. With Icebase, you can conveniently perform these actions directly through the DataOS CLI, eliminating the need for additional Stacks like [Flare](https://dataos.info/resources/stacks/flare/). Moreover, queries executed on data stored in Icebase exhibit enhanced performance. For detailed information, refer to the Icebase [page](https://dataos.info/resources/depot/icebase/).

### **Fastbase**[¶](https://dataos.info/resources/depot/#fastbase "Permanent link")

Fastbase type Depots are optimized for handling streaming data workloads. It provides features such as [creating](https://dataos.info/resources/depot/fastbase/#create-a-dataset) and [listing topics](https://dataos.info/resources/depot/fastbase/#list-topics), which can be executed effortlessly using the DataOS CLI. To explore Fastbase further, consult the [link](https://dataos.info/resources/depot/fastbase/).

## **Data integration - Supported connectors in DataOS**[¶](https://dataos.info/resources/depot/#data-integration-supported-connectors-in-dataos "Permanent link")

The catalogue of data sources accessible by one or more components within DataOS is provided on the following page: [Supported Connectors in DataOS](https://dataos.info/resources/depot/list_of_connectors/).

## **Templates of Depot for different source systems**[¶](https://dataos.info/resources/depot/#templates-of-depot-for-different-source-systems "Permanent link")

To facilitate the creation of Depots accessing commonly used data sources, we have compiled a collection of pre-defined manifest templates. These templates serve as a starting point, allowing you to quickly set up Depots for popular data sources.

To make the process of creating a Depot configuration easier, we provide a set of predefined templates for various data sources. These templates serve as a starting point for configuring your Depot based on the specific data source you are working with. Simply choose the template that corresponds to your organization's data source and follow the instructions provided to fill in the required information.

🗣️ When using these templates, you will need to populate the key-value properties in the manifest file with the appropriate values for your data source. This requires a basic understanding of your organization's data infrastructure and the necessary credentials or connection details.

You can access these templates by visiting the following tabs:

[Data
Warehouse](https://dataos.info/resources/depot/#data-warehouse)[Lakehouse or
Data Lake](https://dataos.info/resources/depot/#lakehouse-or-data-lake)[Streaming
Source](https://dataos.info/resources/depot/#streaming-source)[NoSQL
Database](https://dataos.info/resources/depot/#nosql-database)[Relational
Database](https://dataos.info/resources/depot/#relational-database)

[Amazon Redshift](https://dataos.info/resources/depot/#amazon-redshift)[Google BigQuery](https://dataos.info/resources/depot/#google-bigquery)[Snowflake](https://dataos.info/resources/depot/#snowflake)

DataOS provides the capability to establish a connection with the Amazon Redshift database. We have provided the template for the manifest file to establish this connection. To create a Depot of type ‘REDSHIFT‘, utilize the following template:

🗣️ Please note that the credentials are directly specified in the Depot manifest using the `connectionSecret`, whereas credentials are referred via [Instance Secret](https://dataos.info/resources/instance_secret/) as `secrets` or `dataosSecrets`.

[Inline Credentials](https://dataos.info/resources/depot/#inline-credentials)[Instance Secret Reference](https://dataos.info/resources/depot/#instance-secret-reference)

**redshift\_v1.yaml**

[](https://dataos.info/resources/depot/#__codelineno-20-1)`name: ${{redshift-depot-name}}`
[](https://dataos.info/resources/depot/#__codelineno-20-2)`version: v1`
[](https://dataos.info/resources/depot/#__codelineno-20-3)`type: depot`
[](https://dataos.info/resources/depot/#__codelineno-20-4)`tags:`
[](https://dataos.info/resources/depot/#__codelineno-20-5)`  - ${{redshift}}`
[](https://dataos.info/resources/depot/#__codelineno-20-6)`layer: user`
[](https://dataos.info/resources/depot/#__codelineno-20-7)`description: ${{Redshift Sample data}}`
[](https://dataos.info/resources/depot/#__codelineno-20-8)`depot:`
[](https://dataos.info/resources/depot/#__codelineno-20-9)`  type: REDSHIFT`
[](https://dataos.info/resources/depot/#__codelineno-20-10)`  spec:`
[](https://dataos.info/resources/depot/#__codelineno-20-11)`    host: ${{hostname}}`
[](https://dataos.info/resources/depot/#__codelineno-20-12)`    subprotocol: ${{subprotocol}}`
[](https://dataos.info/resources/depot/#__codelineno-20-13)`    port: ${{5439}}`
[](https://dataos.info/resources/depot/#__codelineno-20-14)`    database: ${{sample-database}}`
[](https://dataos.info/resources/depot/#__codelineno-20-15)`    bucket: ${{tmdc-dataos}}`
[](https://dataos.info/resources/depot/#__codelineno-20-16)`    relativePath: ${{development/redshift/data_02/}}`
[](https://dataos.info/resources/depot/#__codelineno-20-17)`  external: ${{true}}`
[](https://dataos.info/resources/depot/#__codelineno-20-18)`  connectionSecret:`
[](https://dataos.info/resources/depot/#__codelineno-20-19)`    - acl: ${{rw}}`
[](https://dataos.info/resources/depot/#__codelineno-20-20)`      type: key-value-properties`
[](https://dataos.info/resources/depot/#__codelineno-20-21)`      data:`
[](https://dataos.info/resources/depot/#__codelineno-20-22)`        username: ${{username}}`
[](https://dataos.info/resources/depot/#__codelineno-20-23)`        password: ${{password}}`
[](https://dataos.info/resources/depot/#__codelineno-20-24)`        awsaccesskeyid: ${{access key}}`
[](https://dataos.info/resources/depot/#__codelineno-20-25)`        awssecretaccesskey: ${{secret key}}`

Follow these steps to create the Depot:

* **Step 1**: Create a manifest file.

* **Step 2**: Copy the template from above and paste it in a code.

* **Step 3**: Fill the values for the atttributes/fields declared in the YAML-based manifest file.

* **Step 4**: Apply the file through DataOS CLI.

**Requirements** To establish a connection with Redshift, the following information is required:

* Hostname

* Port

* Database name

* User name and password

Additionally, when accessing the Redshift Database in Workflows or other DataOS Resources, the following details are also necessary:

* Bucket name where the data resides

* Relative path

* AWS access key

* AWS secret key