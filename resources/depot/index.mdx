---
title: "Introduction"
description: "Depot in DataOS is a Resource used to connect different data sources to DataOS by abstracting the complexities associated with the underlying source system (including protocols, credentials, and connection schemas). It enables users to establish connections and retrieve data from various data sources, such as file systems (e.g., AWS S3, Google GCS, Azure Blob Storage), data lake systems, database systems (e.g., Redshift, SnowflakeDB, Bigquery, Postgres), and event systems (e.g., Kafka, Pulsar) without moving the data."
sidebarTitle: "Overview"
---

<Tip>
  ### **Depot in the Data Product lifecycle**

  In the Data Product Lifecycle, a Depot in DataOS serves as a foundational Resource for integrating, accessing, and managing data from various sources without physically moving it. It abstracts the complexities of different storage and database systems, ensuring seamless and secure data access for building and operating Data Products.

  Key roles of a Depot in the lifecycle include:

  * **Data Integration and accessibility:** A Depot enables seamless access to diverse data sources such as file systems (AWS S3, Google GCS), databases (PostgreSQL, Snowflake), and event streams (Kafka, Pulsar) through Uniform Data Links (UDLs). This allows Data Products to retrieve and utilize data efficiently without duplication.

  * **Standardization and abstraction:** By registering data locations within DataOS, a Depot standardizes access mechanisms, eliminating inconsistencies in connection protocols, schemas, and credentials across different systems. This simplifies Data Product development and ensures interoperability.

  * **Security and governance:** Depots enforce security policies, ensuring only authorized users and services can access specific datasets. Role-based access control (RBAC) and data policies (such as column masking) help maintain compliance with organizational and regulatory requirements.

  * **Data transformation and querying:** Depots facilitate operations like data ingestion, transformation, and query acceleration using DataOS Stacks (Flare, Benthos, etc.). This enhances data readiness for consumption in analytical and operational use cases.

  * **Metadata management and observability:** A Depot provides deep introspection into registered data sources, enabling visibility into constraints, partitions, and indexing. It also supports monitoring and tracking changes via Metis UI for enhanced observability.

  * **Foundation for Data Products:** Depots are essential for creating Data Products, as they enable teams to query and transform source data.&#x20;

  By leveraging Depots, organizations can build scalable, secure, and efficient data ecosystems within DataOS, ensuring Data roducts have reliable access to well-governed and high-quality data.
</Tip>

Within DataOS, the hierarchical structure of a data source is represented as follows:

![Hierarchical Structure of a Data Source within DataOS](/resources/depot/udl.png "Hierarchical Structure of a Data Source within DataOS")

The Depot serves as the registration of data locations to be made accessible to DataOS. Through the Depot Service, each source system is assigned a unique address, referred to as a **Uniform Data Link (UDL)**. The UDL grants convenient access and manipulation of data within the source system, eliminating the need for repetitive credential entry. The UDL follows this format:

**`                                                             dataos://[depot]:[collection]/[dataset]`**

<Info>
  Depot Service is a DataOS Service that manages the Depot Resource. It facilitates in-depth introspection of Depots and their associated storage engines. Once a Depot is created, users can obtain comprehensive information about the datasets contained within, including details such as constraints, partition, indexing, etc.
</Info>

Leveraging the UDL enables access to datasets and seamless execution of various operations, including data transformation using various Clusters and [Policy](https://dataos.info/resources/policy/) assignments.

Once this mapping is established, Depot Service automatically generates the Uniform Data Link (UDL) that can be used throughout DataOS to access the data. As a reminder, the UDL has the format: `dataos://[depot]:[collection]/[dataset]`.

For a simple file storage system, "Collection" can be analogous to "Folder," and "Dataset" can be equated to "File." The Depot's strength lies in its capacity to establish uniformity, eliminating concerns about varying source system terminologies.

Once a Depot is created, all members of an organization gain secure access to datasets within the associated source system. The Depot not only facilitates data access but also assigns **default** [Access Policies](https://dataos.info/resources/policy/) to ensure data security. Moreover, users have the flexibility to define and utilize custom [Access Policies](https://dataos.info/resources/policy/) for the Depot and [Data Policies](https://dataos.info/resources/policy/) for specific datasets within the Depot.

<Info>
  Depot provides 'access' to data, meaning that data remains within the source system and is neither moved nor duplicated. However, DataOS offers multiple Stacks such as Flare, Benthos, etc. to perform ingestion, querying, syndication, and copying if the need arises.
</Info>

## **How to create a Depot?**

To create a Depot in DataOS, simply compose a manifest configuration file for a Depot and apply it using the DataOS [Command Line Interface (CLI)](https://dataos.info/interfaces/cli/). A Data Product developer is generally responsible for creating a Depot.

### **Structure of a Depot manifest**

![](/resources/depot/depot_yaml.png)

To know more about the attributes of Depot manifest Configuration, refer to the link: [Attributes of Depot manifest](https://dataos.info/resources/depot/configurations/).

### **Steps to create a Depot**

This section involves steps for creating a Depot for different data sources supported by DataOS.

<AccordionGroup>
  <Accordion title="ABFSS" defaultOpen={false}>
    ### Pre-requisites specific to the ABFSS Depot

    To create an ABFSS Depot you must have the following details:


    **Pre-requisites specific to Depot creation**


    * **Tags:** A developer must possess the following tags, which can be obtained from an DataOS operator.

    ```bash
            NAME     │     ID      │  TYPE  │        EMAIL         │              TAGS               
      ───────────────┼─────────────┼────────┼──────────────────────┼─────────────────────────────────
        Iamgroot     │   iamgroot  │ person │   iamgroot@tmdc.io   │ roles:id:data-dev,                            
                     │             │        │                      │ roles:id:user,                  
                     │             │        │                      │ users:id:iamgroot  
    ```

    * **Use cases:** Alternatively, instead of assigning tags, a developer can create a Depot if an operator grants them the "Manage All Instance-level Resources of DataOS in the user layer" use case through Bifrost Governance.

    <Tabs>
      <Tab title="use case ">
        ![](/usecase2.png)
      </Tab>
    </Tabs>




    **Pre-requisites specific to the source system**

    * **Storage Account Name**: The name of the Azure storage account used to store your data. This can be obtained from the Azure portal under your storage account settings or from the administrator managing your Azure resources.

    * **Storage Account Key**: The key used to authenticate and access the Azure storage account. It is generated when the storage account is created and can be retrieved from the Azure portal under the "Access keys" section of your storage account.

    * **Container**: The name of the container within the Azure storage account that holds your data. You can find this information in the Azure portal under the "Containers" section of your storage account, or it can be provided by your Azure administrator.

    * **Relative Path**: The relative path to the specific data within the container. This path is relative to the root of the container, and can be provided by the person managing the data stored in the container or found by navigating through the container in Azure Blob Storage.

    * **Data Format Stored in the Container**: The format of the data stored in the container (e.g., Parquet, CSV, JSON). This should be specified during data storage and can be verified by reviewing the data files or by asking the administrator managing the data.

    ### Create a ABFSS Depot

    Azure Blob File System Secure (ABFSS) is an object storage system. Object stores are distributed storage systems designed to store and manage large amounts of unstructured data.

    DataOS enables the creation of a Depot of type 'ABFSS' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing ABFSS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a ABFSS Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your ABFSS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.

    ### Limit the data source's file format

    Another important function that a Depot can play is to limit the file type that can read from and write to a particular data source. In the `spec` section of the config file, simply mention the `format` of the files you want to allow access to.

    ```yaml
    depot:
      type: S3
      description: $${{description}}
      external: true
      spec:
        scheme: $${{s3a}}
        bucket: $${{bucket-name}}
        relativePath: "raw" 
        format: $${{format}}  # mention the file format, such as JSON
    ```

    For file-based systems, if you define the format as ‘Iceberg’, you can choose the meta-store catalog between Hadoop and Hive. This is how you do it:

    ```yaml
    depot:
      type: ABFSS
      description: "ABFSS Iceberg Depot for sanity"
      compute: runnable-default
      spec:
        account: 
        container: 
        relativePath:
        format: ICEBERG
        endpointSuffix:
        icebergCatalogType: Hive
    ```

    If you do not mention the catalog name as Hive, it will use Hadoop as the default catalog for Iceberg format. Hive automatically keeps the pointer updated to the latest metadata version. If you use Hadoop, you have to manually do this by running the set metadata command as described on this page: [Set Metadata](https://dataos.info/resources/depot/icebase/).
  </Accordion>

  <Accordion title="Amazon Redshift" defaultOpen={false}>
    ### Pre-requisites specific to the Redshift Depot

    To create a Redshift Depot you must have the following details:

    * **Hostname**: The hostname or endpoint of the Redshift cluster, which specifies the server address to connect to. You can obtain this from the AWS Management Console under the Redshift cluster details or ask the administrator managing the cluster.

    * **Port**: The port number on which the Redshift cluster is listening. The default port for Redshift is `5439`, but it may differ based on your setup. Check the cluster details in the AWS Management Console or consult your database administrator.

    * **Database name**: The name of the specific database within the Redshift cluster you want to connect to. This can be found in the AWS Management Console or provided by the database administrator.

    * **Username and password**: The credentials used to authenticate and access the Redshift database. These are set up when the user account is created and must be securely obtained from the Redshift administrator.

    When accessing the Redshift database in workflows or other DataOS Resources, additional details are required:

    * **Bucket name**: The name of the S3 bucket where the data resides. You can find this in the AWS S3 Console or consult the administrator managing the data storage.

    * **Relative path**: The path within the S3 bucket pointing to the data you want to access. This path is typically structured based on your data organization and can be obtained from the team managing the data or the S3 Console.

    * **AWS access key**: The Access Key ID used to authenticate and authorize API requests to AWS. You can obtain this from the AWS IAM (Identity and Access Management) Console under your user’s security credentials or request it from your AWS administrator.

    * **AWS secret key**: The Secret Access Key associated with your AWS Access Key ID. This key is also available in the AWS IAM Console under security credentials and must be securely stored. If you do not have access, request it from your AWS administrator.

    ### Create a Redshift Depot

    DataOS provides the capability to establish a connection with the Amazon Redshift database. We have provided the template for the manifest file to establish this connection. To create a Depot of type ‘REDSHIFT‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Redshift credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Redshift Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Redshift Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{redshift-depot-name}}
      version: v1
      type: depot
      tags:
        - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
        type: REDSHIFT
        spec:
          host: ${{hostname}}
          subprotocol: ${{subprotocol}}
          port: ${{5439}}
          database: ${{sample-database}}
          bucket: ${{tmdc-dataos}}
          relativePath: ${{development/redshift/data_02/}}
        external: ${{true}}
        connectionSecret:
          - acl: ${{rw}}
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
              awsaccesskeyid: ${{access key}}
              awssecretaccesskey: ${{secret key}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{redshift-depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
        type: REDSHIFT
        redshift:
          host: ${{hostname}}
          subprotocol: ${{subprotocol}}
          port: ${{5439}}
          database: ${{sample-database}}
          bucket: ${{tmdc-dataos}}
          relativePath: ${{development/redshift/data_02/}}
        external: ${{true}}
        secrets:
          - name: ${{redshift-instance-secret-name}}-r
            allkeys: true

          - name: ${{redshift-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Amazon S3" defaultOpen={false}>
    ### Pre-requisites specific to the S3 Depot

    To create a S3 Depot you must have the following details:

    * **AWS access key ID**: The Access Key ID used to authenticate and authorize API requests to your AWS account. This can be obtained from the AWS IAM (Identity and Access Management) Console under your user’s security credentials or requested from your AWS administrator.

    * **AWS bucket name**: The name of the Amazon S3 bucket where the data resides. You can find this in the AWS S3 Console under the list of buckets or request it from the administrator managing the storage.

    * **Secret access key**: The Secret Access Key associated with your AWS Access Key ID, required for secure API requests. This is available in the AWS IAM Console under your user’s security credentials. Ensure that it is securely stored and shared only with authorized personnel.

    * **Scheme**: The scheme specifies the protocol to be used for the connection, such as `s3` or `https`. This information depends on your system’s configuration and can be confirmed with the team managing the connection setup or workflow.

    * **Relative Path**: The path within the S3 bucket that points to the specific data or folder you want to access. This path is typically structured according to how your data is organized and can be obtained from the team managing the data or the AWS S3 Console.

    * **Format**: The file format of the data stored in the S3 bucket, such as `CSV`, `Parquet`, or `JSON`. This information is determined by the structure of your data and can be confirmed with the team managing the data storage.

    ### Create a S3 Depot

    Azure Blob File System Secure (ABFSS) is an object storage system. Object stores are distributed storage systems designed to store and manage large amounts of unstructured data.

    DataOS enables the creation of a Depot of type 'Bigquery' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing S3 credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a S3 Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your S3 Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      description: ${{description}}
      depot:
        type: S3                                          
        external: ${{true}}
        spec:                                            
          scheme: ${{s3a}}
          bucket: ${{project-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
        connectionSecret:                                
          - acl: rw
            type: key-value-properties
            data:
              accesskeyid: ${{AWS_ACCESS_KEY_ID}}
              secretkey: ${{AWS_SECRET_ACCESS_KEY}}
              awsaccesskeyid: ${{AWS_ACCESS_KEY_ID}}
              awssecretaccesskey: ${{AWS_SECRET_ACCESS_KEY}}
          - acl: r
            type: key-value-properties
            data:
              accesskeyid: ${{AWS_ACCESS_KEY_ID}}
              secretkey: ${{AWS_SECRET_ACCESS_KEY}}
              awsaccesskeyid: ${{AWS_ACCESS_KEY_ID}}
              awssecretaccesskey: ${{AWS_SECRET_ACCESS_KEY}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      description: ${{description}}
      depot:
        type: S3                                          
        external: ${{true}}
        s3:                                            
          scheme: ${{s3a}}
          bucket: ${{project-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
        secrets:
          - name: ${{s3-instance-secret-name}}-r
            allkeys: true

          - name: ${{s3-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.

    ### Limit the data source's file format

    Another important function that a Depot can play is to limit the file type that can read from and write to a particular data source. In the `spec` section of the config file, simply mention the `format` of the files you want to allow access to.

    ```yaml
    depot:
      type: S3
      description: $${{description}}
      external: true
      spec:
        scheme: $${{s3a}}
        bucket: $${{bucket-name}}
        relativePath: "raw" 
        format: $${{format}}  # mention the file format, such as JSON
    ```

    For file-based systems, if you define the format as ‘Iceberg’, you can choose the meta-store catalog between Hadoop and Hive. This is how you do it:

    ```yaml
    depot:
      type: ABFSS
      description: "ABFSS Iceberg Depot for sanity"
      compute: runnable-default
      spec:
        account: 
        container: 
        relativePath:
        format: ICEBERG
        endpointSuffix:
        icebergCatalogType: Hive
    ```

    If you do not mention the catalog name as Hive, it will use Hadoop as the default catalog for Iceberg format.

    ![Flow when Hive is chosen as the catalog type](https://dataos.info/resources/depot/depot_catalog.png)

    Hive automatically keeps the pointer updated to the latest metadata version. If you use Hadoop, you have to manually do this by running the set metadata command as described on this page: [Set Metadata](https://dataos.info/resources/depot/icebase/).
  </Accordion>

  <Accordion title="Apache Pulsar" defaultOpen={false}>
    ### Pre-requisites specific to the Pulsar Depot

    To create a Pulsar Depot you must have the following details:

    * **Admin URL**: The administrative endpoint URL of your Apache Pulsar cluster. This URL is used to perform administrative operations on the cluster. You can obtain this from the Pulsar admin or from the configuration details of the Pulsar cluster, typically provided during setup or accessible in the deployment documentation.

    * **Service URL**: The service endpoint URL of your Apache Pulsar cluster. This URL is used by producers and consumers to connect to the cluster and exchange messages. You can retrieve this from the Pulsar admin or by referring to the connection details in the Pulsar cluster configuration.

    ### Create a Pulsar Depot

    DataOS provides the capability to create a Depot of type 'PULSAR' for reading topics and messages stored in Pulsar. This Depot facilitates the consumption of published topics and processing of incoming streams of messages. To create a Depot of type 'PULSAR', follow the below steps:

    ### Step 1: Create a Pulsar Depot manifest file

    Create a manifest file to hold the configuration details for your Pulsar Depot.&#x20;

    <CodeGroup>
      ```yaml Pulsar Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: PULSAR       
        description: ${{description}}
        external: ${{true}}
        spec:              
          adminUrl: ${{admin-url}}
          serviceUrl: ${{service-url}}
          tenant: ${{system}}
      # Ensure to obtain the correct tenant name and other specifications from your organization.
      ```
    </CodeGroup>

    ### Step 2: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Azure WASBS" defaultOpen={false}>
    ### Pre-requisites specific to the WASBS Depot

    To create a WASBS Depot you must have the following details:

    * **Storage Account Name**: The name of your Azure Storage account, which identifies your storage resource in Azure. You can retrieve this from the Azure portal by navigating to your Storage Account under the Storage Accounts service.

    * **Storage Account Key**: The access key used to authenticate and access your Azure Storage account. This can be obtained from the Azure portal by selecting your Storage Account, navigating to the "Access Keys" section, and copying the required key. Ensure it is securely stored.

    * **Container**: The name of the container within your Azure Storage account that holds the required data. You can find this in the Azure portal under your Storage Account by navigating to the "Containers" section.

    * **Relative Path**: The specific path to the file or directory within the container. This is typically provided by the team managing the data or can be derived from the structure of your container in the Azure portal.

    * **Format**: The data format of the files stored within the container, such as CSV, JSON, or Parquet. This information is usually known based on the type of data being stored and consumed. If unsure, consult the team responsible for the data.

    ### Create a WASBS Depot

    DataOS enables the creation of a Depot of type 'WASBS' to facilitate the reading of data stored in Azure Data Lake Storage. This Depot enables access to the storage account, which can contain multiple containers. A container serves as a grouping of multiple blobs. It is recommended to define a separate Depot for each container.To create a Depot of type ‘WASBS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing WASBS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a WASBS Depot manifest file

    Create a manifest file to hold the configuration details for your WASBS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: WASBS                                      
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                          
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: WASBS                                      
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{wasbs-instance-secret-name}}-r
            allkeys: true

          - name: ${{wasbs-instance-secret-name}}-rw
            allkeys: true
        wasbs:                                          
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Elasticsearch" defaultOpen={false}>
    ### Pre-requisites specific to the Elasticsearch Depot

    To create a Elasticsearch Depot you must have the following details:

    * **Username**: The username used to authenticate and access the Elasticsearch cluster. This is typically created and provided by the Elasticsearch admin during user setup or role assignment.

    * **Password**: The password associated with the Elasticsearch username for authentication. This is set during account creation and can be obtained securely from the Elasticsearch admin if forgotten.

    * **Nodes (Hostname/URL of the server and ports)**: The addresses of one or more Elasticsearch nodes, including the hostname or URL and the port (e.g., `node1.example.com:9200`). These nodes form part of the cluster and are used to establish the connection. This information can be retrieved from the Elasticsearch admin or by checking the cluster configuration details.

    ### Create a Elasticsearch Depot

    DataOS provides the capability to connect to Elasticsearch data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of type ‘ELASTICSEARCH‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Elasticsearch credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Elasticsearch Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Elasticsearch Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ELASTICSEARCH              
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                
          - acl: rw
            values:
              username: ${{username}}
              password: ${{password}}
          - acl: r
            values:
              username: ${{username}}
              password: ${{password}}
        spec:                           
          nodes: ${{["localhost:9092", "localhost:9093"]}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ELASTICSEARCH              
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        elasticsearch:                           
          nodes: ${{["localhost:9092", "localhost:9093"]}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Eventhub" defaultOpen={false}>
    ### Pre-requisites specific to the Eventhub Depot

    To create a Eventhub Depot you must have the following details:

    * **Endpoint**: The URL endpoint used to connect to your EventHub instance. This can be found in the Azure portal under your EventHub namespace settings. It is typically in the format `your-namespace-name.servicebus.windows.net`.

    * **Eventhub Shared Access Key Name**: The name of the shared access key used to authenticate access to the EventHub. You can retrieve this from the Azure portal under your EventHub namespace’s "Shared Access Policies" section, where you can create or view existing access keys.

    * **Eventhub Shared Access Key**: The actual shared access key used to authenticate with EventHub. This key is found alongside the "Shared Access Key Name" in the "Shared Access Policies" section of your EventHub namespace settings in the Azure portal. Ensure that this key is securely stored and handled.

    ### Create a Eventhub Depot

    Eventhub is a streaming service platform. Streaming refers to the continuous and real-time transmission of data from a source to a destination. 

    DataOS provides the capability to connect to Eventhub data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of Eventhub, in the type field you will have to specify type 'EVENTHUB' and follow the below steps:

    <Warning>
      Please note that the credentials are directly specified in the Depot manifest using the `connectionSecret`, whereas credentials are referred via [Instance Secret](https://dataos.info/resources/instance_secret/) as `secrets` or `dataosSecrets`.
    </Warning>

    ### Step 1: Create an Instance Secret for securing Eventhub credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Eventhub Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Eventhub Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{"sanityeventhub01"}}
      version: v1
      type: depot
      tags:
        - ${{Eventhub}}
        - ${{Sanity}}
      layer: user
      depot:
        type: "EVENTHUB"
        compute: ${{runnable-default}}
        spec:
          endpoint: ${{"sb://event-hubns.servicebus.windows.net/"}}
        external: ${{true}}
        connectionSecret:
          - acl: r
            type: key-value-properties
            data:
              eh_shared_access_key_name: ${{$EH_SHARED_ACCESS_KEY_NAME}}
              eh_shared_access_key: ${{$EH_SHARED_ACCESS_KEY}}
          - acl: rw
            type: key-value-properties
            data:
              eh_shared_access_key_name: ${{$EH_SHARED_ACCESS_KEY_NAME}}
              eh_shared_access_key: ${{$EH_SHARED_ACCESS_KEY}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{"sanityeventhub01"}}
      version: v2alpha
      type: depot
      tags:
        - ${{Eventhub}}
        - ${{Sanity}}
      layer: user
      depot:
        type: "EVENTHUB"
        compute: ${{runnable-default}}
        eventhub:
          endpoint: ${{"sb://event-hubns.servicebus.windows.net/"}}
        external: ${{true}}
        secrets:
          - name: ${{eh-instance-secret-name}}-r
            allkeys: true

          - name: ${{eh-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Google Bigquery" defaultOpen={false}>
    ### Pre-requisites specific to the Bigquery Depot

    To create a Bigquery Depot you must have the following details:

    * **Project ID**: The unique identifier for your Google Cloud project used by BigQuery. You can find this in the Google Cloud Console under "Project Info" on your project dashboard.

    * **Email ID**: The email address associated with the Google Cloud service account used to access BigQuery. This can be found in the IAM & Admin section of the Google Cloud Console under the "Service Accounts" tab, where the email is listed for each service account.

    * **Credential Properties in JSON File**: A JSON file containing the necessary credential properties for authentication. This file is generated when you create a service account in Google Cloud. You can download it from the IAM & Admin > Service Accounts section in the Google Cloud Console by selecting a service account and clicking "Add Key" > "JSON".

    * **Additional Parameters**: Any extra configuration parameters required to establish the connection, such as project settings or specific access scopes. These may be provided by the administrator or defined in the BigQuery API documentation based on the integration you are working with.

    ### Create a Bigquery Depot

    DataOS enables the creation of a Depot of type 'Bigquery' to facilitate the reading of data stored in an Azure Blob Storage account. This Depot provides access to the storage account, which can consist of multiple containers. A container serves as a grouping mechanism for multiple blobs. It is recommended to define a separate Depot for each container. To create a Depot of type ‘ABFSS‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Bigquery credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Bigquery Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Bigquery Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="GCS" defaultOpen={false}>
    ### Pre-requisites specific to the GCS Depot

    To create a GCS Depot you must have the following details:

    * **GCS Bucket**: The name of the Google Cloud Storage bucket you want to access. You can find the bucket name in the Google Cloud Console under the "Storage" section, where all your buckets are listed.

    * **Relative Path**: The path to the specific file or directory within the GCS bucket. This path can be obtained by navigating to the file or directory in the Google Cloud Console and copying the relative path from the bucket.

    * **GCS Project ID**: The unique identifier of the Google Cloud project associated with the GCS bucket. This can be found in the Google Cloud Console under "Project Info" on your project dashboard.

    * **GCS Account Email**: The email address associated with the Google Cloud service account that has access to GCS. This can be found in the IAM & Admin section of the Google Cloud Console under the "Service Accounts" tab.

    * **GCS Key**: The JSON key file associated with the Google Cloud service account used for authentication. You can download this key by going to the IAM & Admin > Service Accounts section in the Google Cloud Console, selecting the service account, and clicking "Add Key" > "JSON".

    ### Create a GCS Depot

    DataOS provides the capability to connect to Google Cloud Storage data using Depot. To create a Depot of Google Cloud Storage follow the below steps:

    ### Step 1: Create an Instance Secret for securing GCS credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a GCS Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your GCS Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{"sanitygcs01"}}
      version: v1
      type: depot
      tags:
        - ${{GCS}}
        - ${{Sanity}}
      layer: user
      depot:
        type: GCS
        description: ${{"GCS depot for sanity"}}
        compute: ${{runnable-default}}
        spec:
          bucket: ${{"airbyte-minio-testing"}}
          relativePath: ${{"/sanity"}}
        external: ${{true}}
        connectionSecret:
          - acl: ${{rw}}
            type: key-value-properties
            data:
              projectid: ${{$GCS_PROJECT_ID}}
              email: ${{$GCS_ACCOUNT_EMAIL}}
            files:
              gcskey_json: ${{$GCS_KEY_JSON}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{"sanitygcs01"}}
      version: v2alpha
      type: depot
      tags:
        - ${{GCS}}
        - ${{Sanity}}
      layer: user
      depot:
        type: GCS
        description: ${{"GCS depot for sanity"}}
        compute: ${{runnable-default}}
        gcs:
          bucket: ${{"airbyte-minio-testing"}}
          relativePath: ${{"/sanity"}}
        external: ${{true}}
        secrets:
          - name: ${{gcs-instance-secret-name}}-r
            allkeys: true

          - name: ${{gcs-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="JDBC" defaultOpen={false}>
    ### Pre-requisites specific to the JDBC Depot

    To create a JDBC Depot you must have the following details:

    * **Database Name**: The name of the database you want to connect to. This can be provided by the database administrator or found in the database management interface, where your desired database is listed.

    * **Subprotocol Name**: The subprotocol specifies the type of database you are connecting to, such as `mysql`, `postgresql`, etc. This should match the JDBC driver you're using to establish the connection, and can typically be found in the documentation of the specific database or JDBC driver.

    * **Hostname/URL of the Server**: The hostname or URL of the server where the database is hosted. This is typically provided by the database administrator or hosting service, and points to the location of your database.

    * **Port**: The port number through which the database connection is made (e.g., `3306` for MySQL, `5432` for PostgreSQL). This information can also be obtained from the database administrator or found in the configuration of the database server.

    * **Parameters**: Any additional connection parameters needed for the JDBC connection, such as SSL settings, timeouts, or specific options for the database connection. These can be found in the JDBC driver documentation or obtained from the database administrator if custom configurations are required.

    * **Username**: The username used to authenticate the JDBC connection. This can be provided by the database administrator or set when the user account is created in the database management system.

    * **Password**: The password associated with the provided username. This is typically set when the user account is created and must be securely stored or retrieved from a safe location if forgotten.

    ### Create a JDBC Depot

    DataOS provides the capability to establish a connection to a database using the JDBC driver in order to read data from tables using a Depot. The Depot facilitates access to all schemas visible to the specified user within the configured database. To create a Depot of type ‘JDBC‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing JDBC credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a JDBC Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your JDBC Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: JDBC                                      
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                              
          - acl: rw
            type: key-value-properties
            data:
              username: ${{jdbc-username}}
              password: ${{jdbc-password}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{jdbc-username}}
              password: ${{jdbc-password}}
        spec:                                           
          subprotocol: ${{subprotocol}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params:
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: JDBC                                      
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        jdbc:                                           
          subprotocol: ${{subprotocol}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params:
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Kafka" defaultOpen={false}>
    ### Pre-requisites specific to the Kafka Depot

    To create a Kafka Depot you must have the following details:

    * **KAFKA Broker List**: A comma-separated list of broker addresses in the Kafka cluster (e.g., `broker1:9092,broker2:9092`). The broker list allows the connection to discover all topics and partitions in the cluster. This information can be obtained from the Kafka admin or the cluster configuration file, usually found in the `server.properties` file.

    * **Schema Registry URL**: The URL of the Schema Registry associated with your Kafka cluster, which stores the schemas for the data being produced and consumed. This is required if your Kafka topics use Avro or other schema-based serialization formats. You can obtain this URL from the Kafka admin or the configuration details of the Schema Registry setup.

    ### Create a Kafka Depot

    DataOS allows you to create a Depot of type 'KAFKA' to read live topic data. This Depot enables you to access and consume real-time streaming data from Kafka. To create a Depot of type 'KAFKA', follow the below steps:

    ### Step 1: Create a Kafka Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Kafka Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    ### Step 2: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="MongoDB" defaultOpen={false}>
    There are two ways to set up the MongoDB connection: Through \`username\` and \`password\`, and certificate authentication. Follow the sections below to create the Depot using both methods.

    ### **Prerequisites**

    The following are the prerequisites for creating a MongoDB Depot.

    **For username and password authentication:**

    1\. Obtain the username and password for MongoDB from your organization's database administrator.

    2\. Also, obtain the MongoDB node's details from the database administrator.

    **For certificate authentication:**

    1\. Obtain the username and password for MongoDB from your organization's database administrator.

    2\. Obtain the MongoDB node's details from the database administrator.

    3\. Obtain the \`.crt\` certificate file for the MongoDB connection from the database administrator for creating the Depot through certificate authentication.

    4\. Java 17 must be installed in your system.

    5\. Create a Keystore and Truststore file from a \`.crt\` file. Follow the below steps to create the keystore and trust store files from the \`.crt\` file.

    * &#x20;Initialize the Keystore, remember the password you create while initializing the Keystore.

    * &#x20;Run the following command, to import a certificate (\`demo-ssl-public-cert.cert\`) into the Java KeyStore with the alias (\`moderncert03\`). It will create a keystore and trust store files in the JAVA folder.

    ```bash
    sudo  keytool -importcert -alias moderncert03 -keystore $JAVA_HOME/lib/security/cacerts -storepass 123456 -file /Users/iamgroot/office/poc-squad/mongodb-poc/demo-ssl-public-cert.cert
    ```

    * &#x20;\`keystore\`: Points to the KeyStore file to update.

    * \`importcert\`: Specifies the operation of importing a certificate.

    * &#x20;\`alias moderncert03\`: Assigns the alias \`moderncert03\` to the imported certificate.

    * &#x20;\`storepass 123456\`: Provide the password for the keystone you created while initializing the keystore.

    * &#x20;\`file\`: Specifies the path to the certificate file to import.

    ### **Steps to create MongoDB Depot through username and password authentication**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret.

    1\. Create a manifest file for Depot containing the following code and update the details.

    ```yaml
    version: ${{v1}} # depot version
    name: ${{"mongodb03"}}
    type: ${{depot}}
    tags:
      - ${{MongoDb}}
      - ${{Sanity}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["mongodb-tmdc.dataos.info:27017"]}}
        params: 
          tls: ${{false}}
      external: ${{true}}
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{root}}
            password: ${{f5ce9b0d972fd9555560}}
    ```

    &#x20;  &#x20;

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    &#x20;  &#x20;

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    &#x20;  &#x20;

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb03         # MongoDB depot name
    ```

    &#x20;  &#x20;

    5\. Apply the Scanner Workflow by executing the below command.

    ```yaml
    dataos-ctl apply -f /home/office/workflow/depot_scanner.yaml
    ```

    ### &#x20;**Steps to create MongoDB Depot through certificate authentication**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret.

    1\. Create a manifest file for Depot containing the following code and update the details.

    &#x20;  &#x20;

    ```yaml
    version: ${{v1}} # depot version
    name: ${{"mongodb"}}
    type: ${{depot}}
    tags:
      - ${{MongoDb}}
      - ${{Sanity}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["SG-demo-66793.servers.mongodirector.com:27071"]}}
        params: 
          tls: ${{true}}
      external: ${{true}}
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{admin}}
            password: ${{Kl6swyCRLPteqljkdyrf}}
            keyStorePassword: ${{123456}}
            trustStorePassword: ${{123456}}
          files:
            ca_file: ${{mongodb-poc/demo-ssl-public-cert.cert}}
            key_store_file: ${{/Library/Java/JavaVirtualMachines/jdk-22.jdk/Contents/Home/lib/security/cacerts}}
            trust_store_file: ${{/Library/Java/JavaVirtualMachines/jdk-22.jdk/Contents/Home/lib/security/cacerts}}
    ```

    &#x20;  &#x20;

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    &#x20;  &#x20;

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    &#x20;

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    &#x20;  &#x20;

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb         # MongoDB depot name
    ```

    &#x20;  &#x20;

    5\. Apply the Scanner Workflow by executing the below command.

    ```bash
    dataos-ctl apply -f /home/office/workflow/udscoremongodb.yaml
    ```

    ### &#x20;**Steps to create MongoDB Depot through VPCE**

    This section involves the alternative steps to create a MongoDB  Depot without an Instance Secret. VPCE-based Depot is similar to certificate authentication Depot just additional parameters have been added that allow connection with VPCs.

    1\. Create a manifest file for Depot containing the following code and update the details.

    ```yaml
    version: ${{v1}}
    name: ${{"ldmmongodb"}}
    type: ${{depot}}
    tags:
      - ${{Mongodb}}
    layer: ${{user}}
    depot:
      type: ${{mongodb}}
      description: ${{"MongoDb depot for sanity"}}
      compute: ${{query-default}}
      spec:
        subprotocol: ${{"mongodb"}}
        nodes: ${{["SG-demo-664533.servers.mongodirector.com:27071"]}}
        params: 
          tls: ${{true}}
          tlsAllowInvalidHostnames: ${{true}}
          directConnection: ${{true}}
      external: ${{true}}
      secrets:
      connectionSecret:
        - acl: ${{rw}}
          type: ${{key-value-properties}}
          data:
            username: ${{admin}}
            password: ${{Kl6lyCRLPteqljkdyrf}}
            keyStorePassword: ${{changeit}}
            trustStorePassword: ${{changeit}}
          files:
            ca_file: ${{/Users/iamgroot/Downloads/ca-chain-cn-qa.crt}}
            key_store_file: ${{$JAVA_HOME/lib/security/cacerts}}
            trust_store_file: ${{$JAVA_HOME/lib/security/cacerts}}
    ```

    2\. Apply the Depot manifest file by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/depots/mongo_depot.yaml
    ```

    &#x20;  &#x20;

    3\. Verify the Depot creation by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl get -t depot -n mongodb03 -w public
    ```

    4\. Scan the metadata. By creating and applying the Scanner Workflow by referring to the MongoDB Depot name, you can extract the metadata that can be accessed on the Metis App.

    ```yaml
    version: v1
    name: depotscanner
    type: workflow
    tags:
      - postgres
      - scanner
    description: The job scans schema tables and register metadata
    workflow:
      dag:
        - name: depotjob
          description: The job scans schema from postgres depot tables and register metadata to metis
          spec:
            stack: scanner:2.0
            compute: runnable-default
            stackSpec:
              depot: dataos://mongodb         # MongoDB depot name
    ```

    5\. Apply the Scanner Workflow by executing the below command.

    &#x20;  &#x20;

    ```bash
    dataos-ctl apply -f /home/office/workflow/udscoremongodb.yaml
    ```
  </Accordion>

  <Accordion title="Microsoft SQL Server" defaultOpen={false}>
    ### Pre-requisites specific to the MSSQL Depot

    To create a MSSQL Depot you must have the following details:

    * **Host URL and Parameters**: The URL or hostname of the SQL Server where the database is hosted. This is typically provided by the database administrator. Additional connection parameters (such as encryption settings or timeout values) might be needed depending on the specific requirements of the SQL Server and the connection method.

    * **Database Schema**: The schema in the Microsoft SQL Server where your tables and other database objects are located. This can be provided by the database administrator, and it helps define the structure and organization of the data within the database.

    * **Port**: The port number used to connect to the SQL Server. The default port for Microsoft SQL Server is `1433`, but it might vary depending on the configuration. This information can be obtained from the database administrator.

    * **Username**: The username used for authentication to the Microsoft SQL Server. This is typically provided by the database administrator or set up by the user account when created in the SQL Server.

    * **Password**: The password associated with the provided username for authentication. This password is securely stored and is required for establishing the connection to the SQL Server. It can be set when the user account is created and should be securely retrieved if forgotten.

    ### Create a MSSQL Depot

    DataOS allows you to connect to a Microsoft SQL Server database and read data from tables using Depots. A Depot provides access to all tables within the specified schema of the configured database. You can create multiple Depots to connect to different SQL servers or databases. To create a Depot of type ‘SQLSERVER‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing MSSQL credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a MSSQL Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your MSSQL Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    **Use the below template, if self-signed certificate is enabled.**

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        connectionSecret:                                 
          - acl: rw
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
          - acl: r
            type: key-value-properties
            data:
              azurestorageaccountname: ${{account-name}}
              azurestorageaccountkey: ${{account-key}}
        spec:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: ABFSS                                       
        description: ${{description}}
        external: ${{true}}
        compute: ${{runnable-default}}
        secrets:
          - name: ${{abfss-instance-secret-name}}-r
            allkeys: true

          - name: ${{abfss-instance-secret-name}}-rw
            allkeys: true
        abfss:                                             
          account: ${{account-name}}
          container: ${{container-name}}
          relativePath: ${{relative-path}}
          format: ${{format}}
      ```
    </CodeGroup>

    **Use the template, if self-signed certificate is not enabled.**

    <CodeGroup>
      ```yaml Inline credentials  Depot manifest file
      name: ${{mssql01}}
      version: v1
      type: depot
      tags:
        - ${{dropzone}}
        - ${{mssql}}
      layer: user
      depot:
        type: JDBC
        description: ${{MSSQL Sample data}}
        spec:
          subprotocol: ${{sqlserver}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database}}
          params: # Required
            encrypt: ${{false}}
        external: ${{true}}
        hiveSync: ${{false}}
        connectionSecret:
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}  
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{mssql01}}
      version: v2alpha
      type: depot
      tags:
        - ${{dropzone}}
        - ${{mssql}}
      layer: user
      depot:
        type: JDBC
        description: ${{MSSQL Sample data}}
        jdbc:
          subprotocol: ${{sqlserver}}
          host: ${{host}}
          port: ${{port}}
          database: ${{database}}
          params: # Required
            encrypt: ${{false}}
        external: ${{true}}
        hiveSync: ${{false}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="MySQL" defaultOpen={false}>
    ### Pre-requisites specific to the MySQL Depot

    To create a MySQL Depot you must have the following details:

    * **Host URL and Parameters**: The URL or hostname of the MySQL server where the database is hosted. This is typically provided by the database administrator. Additional parameters (such as character set or connection timeout) may be needed based on the specific configuration of the MySQL server.

    * **Port**: The port number used to connect to the MySQL server. By default, MySQL uses port `3306`, but this could vary if a custom port is configured. This information should be provided by the database administrator.

    * **Username**: The username used to authenticate the connection to the MySQL database. This username is typically created by the database administrator and must have the necessary privileges to access the desired database.

    * **Password**: The password associated with the provided username for authentication. The password is typically set when the username is created and must be kept secure. It is required to establish a successful connection to the database.

    ### Create a MySQL Depot

    DataOS allows you to connect to a MySQL database and read data from tables using Depots. A Depot provides access to all tables within the specified schema of the configured database. You can create multiple Depots to connect to different MySQL servers or databases. To create a Depot of type ‘MYSQL‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing MySQL credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a MySQL Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your MySQL Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    **Use this template, if self-signed certificate is enabled.**

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{mysql01}}
      version: v1
      type: depot
      tags:
        - ${{dropzone}}
        - ${{mysql}}
      layer: user
      depot:
        type: MYSQL
        description: ${{"MYSQL Sample Database"}}
        spec:
          subprotocol: "mysql"
          host: ${{host}}
          port: ${{port}}
          params: # Required
            tls: ${{skip-verify}}
        external: ${{true}}
        connectionSecret:
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}} 
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{mysql01}}
      version: v2alpha
      type: depot
      tags:
        - ${{dropzone}}
        - ${{mysql}}
      layer: user
      depot:
        type: MYSQL
        description: ${{"MYSQL Sample Database"}}
        mysql:
          subprotocol: "mysql"
          host: ${{host}}
          port: ${{port}}
          params: # Required
            tls: ${{skip-verify}}
        external: ${{true}}
        secrets:
          - name: ${{instance-secret-name}}-r
            keys: 
              - ${{instance-secret-name}}-r

          - name: ${{instance-secret-name}}-rw
            keys: 
              - ${{instance-secret-name}}-rw
      ```
    </CodeGroup>

    **Use this template, if self-signed certificate is not enabled.**

    <CodeGroup>
      ```javascript Inline credentials Depot manifest file
      ```

      ```
      console.log("Hello World");
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Opensearch" defaultOpen={false}>
    ### Pre-requisites specific to the Opensearch Depot

    To create a Opensearch Depot you must have the following details:

    * **Username**: The username used to authenticate and access the OpenSearch cluster. This is typically set up by the OpenSearch admin and assigned to users with specific access permissions.

    * **Password**: The password associated with the OpenSearch username for authentication. This is created during user setup and should be securely retrieved from the OpenSearch admin if not known.

    * **Nodes (Hostname/URL of the server and ports)**: The addresses of one or more OpenSearch nodes, including the hostname or URL and the port (e.g., `node1.opensearch.example.com:9200`). These nodes are part of the cluster and are necessary to establish the connection. This information can be obtained from the OpenSearch admin or by referring to the cluster's configuration details.

    ### Create a Opensearch Depot

    DataOS provides the capability to connect to Opensearch data using Depot. The Depot facilitates access to all documents that are visible to the specified user, allowing for text queries and analytics. To create a Depot of Opensearch, in the type field you will have to specify type ‘ELASTICSEARCH‘,  follow the below steps:

    ### Step 1: Create an Instance Secret for securing Opensearch credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Opensearch Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Opensearch Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: OPENSEARCH             
        description: ${{description}}
        external: ${{true}}
        connectionSecret:                
          - acl: rw
            values:
              username: ${{username}}
              password: ${{password}}
          - acl: r
            values:
              username: ${{opensearch-username}}
              password: ${{opensearch-password}}
        spec:                           
          nodes:
            - ${{nodes}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: OPENSEARCH           
        description: ${{description}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        elasticesearch:                           
          nodes:
            - ${{nodes}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Oracle" defaultOpen={false}>
    ### Pre-requisites specific to the Oracle Depot

    To create a Oracle Depot you must have the following details:

    * **URL of your Oracle account**: The URL or hostname of the Oracle database server where the database is hosted. This is typically provided by the database administrator and may include additional connection parameters.

    * **Username**: The login username used to authenticate the connection to the Oracle database. This username is created by the database administrator and must have the appropriate permissions to access the desired database.

    * **Password**: The password associated with the provided username for authentication. This is required to establish a secure connection and is typically set by the administrator or the user during account creation.

    * **Database Name**: The name of the Oracle database you want to connect to. This is used to specify the target database within the Oracle server and is provided by the database administrator.

    * **Database Schema**: The schema in the Oracle database where your desired tables are located. Schemas are logical containers for database objects like tables, views, and indexes, and this information helps you access the correct data.

    ### Create a Oracle Depot

    DataOS allows you to connect to an Oracle database and access data from tables using Depots. A Depot provides access to all schemas within the specified service in the configured database. You can create multiple Depots to connect to different Oracle servers or databases. To create a Depot of type ‘ORACLE‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Oracle credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Oracle Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Oracle Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: depot
      tags:
        - ${{dropzone}}
        - ${{oracle}}
      layer: user
      depot:
        type: ORACLE                                    
        description: ${{"Oracle Sample data"}}
        spec:
          subprotocol: ${{subprotocol}} # for example "oracle:thin"                                     
          host: ${{host}}
          port: ${{port}}
          service: ${{service}}
        external: ${{true}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}  
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{dropzone}}
        - ${{oracle}}
      layer: user
      depot:
        type: ORACLE                                    
        description: ${{"Oracle Sample data"}}
        oracle:
          subprotocol: ${{subprotocol}} # for example "oracle:thin"                                     
          host: ${{host}}
          port: ${{port}}
          service: ${{service}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="PostgreSQL" defaultOpen={false}>
    ### Pre-requisites specific to the Postgres Depot

    To create a Postgres Depot you must have the following details:

    * **Database name**: The name of the specific PostgreSQL database you want to connect to. This can be provided by the database administrator or found in the database management system where the database was created.

    * **Hostname/URL of the server**: The hostname or URL of the server where the PostgreSQL database is hosted. This is typically an IP address or a fully qualified domain name (FQDN). You can obtain this from your database administrator or the team managing the PostgreSQL server.

    * **Parameters**: Additional optional parameters required for the connection, such as SSL settings or timeout configurations. These can be provided by the database administrator or found in your PostgreSQL server's documentation, depending on the connection requirements.

    * **Username**: The username used for authentication to access the PostgreSQL database. This is created when the user account is set up and can be provided by the database administrator.

    * **Password**: The password associated with the PostgreSQL username for authentication. This is set during user account creation and must be securely obtained from the database administrator if forgotten.

    ### Create a Postgres Depot

    DataOS allows you to connect to a PostgreSQL database and read data from tables using Depots. A Depot provides access to all schemas visible to the specified user in the configured database. To create a Depot of type ‘POSTGRESQL‘, follow the below steps:

    ### Step 1: Create an Instance Secret for securing Postgres credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Postgres Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Postgres Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <Warning>
      Use the below templates, if self-signed certificate is enabled.
    </Warning>

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{postgresdb}}
      version: v1
      type: depot
      layer: user
      depot:
        type: JDBC                  
        description: ${{To write data to postgresql database}}
        external: ${{true}}
        connectionSecret:           
          - acl: r
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}  
          - acl: rw
            type: key-value-properties
            data:
              username: ${{username}}
              password: ${{password}}
        spec:                        
          subprotocol: "postgresql"
          host: ${{host}}
          port: ${{port}}
          database: ${{postgres}}
          params: #Required 
            sslmode: ${{disable}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{postgresdb}}
      version: v2alpha
      type: depot
      layer: user
      depot:
        type: JDBC                  
        description: ${{To write data to postgresql database}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        postgresql:                        
          subprotocol: "postgresql"
          host: ${{host}}
          port: ${{port}}
          database: ${{postgres}}
          params: #Required 
            sslmode: ${{disable}}
      ```
    </CodeGroup>

    <Warning>
      Use the below templates, if self-signed certificate is not enabled.
    </Warning>

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{depot-name}}
      version: v1
      type: Depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      Depot:
        type: POSTGRESQL
        description: ${{description}}
        external: true
        connectionSecret:                               
          - acl: rw
            type: key-value-properties
            data:
              username: ${{postgresql-username}}
              password: ${{posgtresql-password}}
          - acl: r
            type: key-value-properties
            data:
              username: ${{postgresql-username}}
              password: ${{postgresql-password}}
        spec:                                          
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params: # Optional
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```

      ```yaml Instance Secret Depot manifest file
      name: ${{depot-name}}
      version: v2alpha
      type: Depot
      tags:
        - ${{tag1}}
      owner: ${{owner-name}}
      layer: user
      Depot:
        type: POSTGRESQL
        description: ${{description}}
        external: true
        secrets:
          - name: ${{instance-secret-name}}-r
            allkeys: true

          - name: ${{instance-secret-name}}-rw
            allkeys: true
        postgresql:                                          
          host: ${{host}}
          port: ${{port}}
          database: ${{database-name}}
          params: # Optional
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>

  <Accordion title="Snowflake" defaultOpen={false}>
    ### Pre-requisites specific to the Snowflake Depot

    To create a Snowflake Depot you must have the following details:

    * **Snowflake Account URL**: The unique URL used to access your Snowflake account, typically in the format `https://<account_name>.snowflakecomputing.com`. You can retrieve this from your Snowflake admin or find it in your Snowflake login credentials email.

    * **Snowflake Username**: The username used to log in to your Snowflake account. This is usually provided by the Snowflake admin when your account is created.

    * **Snowflake User Password**: The password associated with your Snowflake username for authentication. This password is set during account creation or upon first login. If forgotten, you may need to reset it via the Snowflake login page or contact your Snowflake admin.

    * **Snowflake Database Name**: The name of the database in Snowflake that you need to connect to. You can find this in the Snowflake console under the Databases section or by consulting the team managing the Snowflake environment.

    * **Database Schema**: The specific schema within the Snowflake database where your required table resides. This can also be found in the Snowflake console under the relevant database or provided by the team managing the database structure.

    ### Create a Snowflake Depot

    DataOS provides integration with Snowflake, allowing you to seamlessly read data from Snowflake tables using Depots. Snowflake is a cloud-based data storage and analytics data warehouse offered as a Software-as-a-Service (SaaS) solution. It utilizes a new SQL database engine designed specifically for cloud infrastructure, enabling efficient access to Snowflake databases. To create a Depot of type 'SNOWFLAKE', follow the below steps:

    ### Step 1: Create an Instance Secret for securing Snowflake credentails

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    Begin by creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    ### Step 2: Create a Snowflake Depot manifest file

    Begin by creating a manifest file to hold the configuration details for your Snowflake Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.&#x20;

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{snowflake-depot}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        spec:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        connectionSecret:
          - acl: rw
            type: key-value-properties
            data:
              username: ${{snowflake-username}}
              password: ${{snowflake-password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{snowflake-depot}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        snowflake:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        secrets:
          - name: ${{redshift-instance-secret-name}}-r
            allkeys: true

          - name: ${{redshift-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    ### Step 3: Apply the Depot manifest file

    Once you have the manifest file ready in your code editor, simply copy the path of the manifest file and apply it through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    <CodeGroup>
      ```bash Command
      dataos-ctl resource apply -f ${{yamlfilepath}}
      ```

      ```bash Alternative command
      dataos-ctl apply -f ${{yamlfilepath}}
      ```
    </CodeGroup>

    ### **Verify the Depot creation**

    To ensure that your Depot has been successfully created, you can verify it in two ways:

    * Check the name of the newly created Depot in the list of Depots where you are named as the owner:&#x20;

    ```bash
    dataos-ctl get -t depot
    ```

    [](https://dataos.info/resources/depot/#__codelineno-15-1)

    * Additionally, retrieve the list of all Depots created in your organization:

    ```bash
    dataos-ctl get -t depot -a
    ```

    [](https://dataos.info/resources/depot/#__codelineno-16-1)

    You can also access the details of any created Depot through the DataOS GUI in the [Operations App](https://dataos.info/interfaces/operations/) and [Metis UI](https://dataos.info/interfaces/metis/).

    ### **Delete a Depot**

    <Tip>
      **Best Practice:** As part of best practices, it is recommended to regularly delete Resources that are no longer in use. This practice offers several benefits, including saving time and reducing costs.
    </Tip>

    If you need to delete a Depot, use the following command in the DataOS CLI:

    <CodeGroup>
      ```bash Command
      dataos-ctl delete -t depot -n ${{name of Depot}}
      ```

      ```bash Alternative command
      dataos-ctl delete -f ${{path of your manifest file}}
      ```
    </CodeGroup>

    [](https://dataos.info/resources/depot/#__codelineno-17-1)

    By executing the above command, the specified Depot will be deleted from your DataOS environment.
  </Accordion>
</AccordionGroup>

The manifest configuration file for a Depot can be divided into two main sections: the Resource meta section and the [Resource-specific section](/resources/depot/configurations). Each section serves a distinct purpose and contains specific attributes.

## **How to utilize Depots?**

Once a Depot is created, you can leverage its Uniform Data Links (UDLs) to access data without physically moving it. The UDLs play a crucial role in various scenarios within DataOS.

<AccordionGroup>
  <Accordion title="Data ingestion and transformation" defaultOpen={false}>
    A Depot can be used further for data ingestion and transformation through Flare Stack. To learn more about the Flare Stack, please refer to this link.&#x20;

    **For example:**

    The below manifest file defines a Flare 6.0-based workflow, `wf-vendor-insights`, for ingesting and transforming vendor insights data from poss3 Depot into Icebase (Lakehouse) Depot. It includes a DAG node, `dg-vendor-insights`, which processes `promotional_data` from a CSV dataset in poss3 and outputs `transformed_data` as an Iceberg table in Icebase. The Workflow specifies compute resources for execution, with a driver and executor configuration limiting cores and memory. The transformation logic applies an SQL query that extracts key vendor attributes, including organization details, contact information, certifications, and category identifiers, while also generating a row number using `row_number() OVER (ORDER BY id)`. The resulting dataset is saved in Iceberg format with Parquet storage and gzip compression, ensuring optimized analytics and efficient querying.

    <AccordionGroup>
      <Accordion title="Flare Stack manifest file" defaultOpen={false}>
        ```
        version: v1
        name: wf-vendor-insights
        type: workflow
        tags:
          - vendor.insights
          - Tier.Gold
        description: The job is to ingest vendor-insights from poss3 into Icebase.
        workflow:
          title: promotional
          dag:
            - name: dg-vendor-insights
              description: The job is to ingest vendor-insights from poss3 into Icebase.
              title: organization  data 
              spec:
                tags:
                  - vendor.insights
                stack: flare:6.0
                compute: runnable-default
                stackSpec:
                  driver:
                    coreLimit: 1200m
                    cores: 1
                    memory: 1024m
                  executor:
                    coreLimit: 1200m    
                    cores: 1
                    instances: 1
                    memory: 1024m
                  job:
                    explain: true
                    inputs:
                      - name: promotional_data
                        dataset: dataos://poss3:promo_effectiveness/target_count02.csv?acl=rw
                        format: csv
                        options: 
                          inferSchema: true

                    logLevel: INFO
                    outputs:
                      - name: transformed_data
                        dataset: dataos://icebase:promo_effectiveness/vendor_subscription_insight?acl=rw
                        format: Iceberg
                        description: The job is to ingest vendor data from S3 into Icebase.
                        tags:
                          - vendor.insights
                        options:
                          saveMode: overwrite
                          iceberg:
                            properties:
                              write.format.default: parquet
                              write.metadata.compression-codec: gzip
                        title: vendor data

                    steps:
                      - sequence:
                          - name: transformed_data
                            sql: >
                               select 
                                  messagekey,
                                  id,
                                  organization_id,
                                  unique_entity_id,
                                  state_of_incorporation,
                                  business_type,
                                  no_dbe_designations,
                                  emergency_disaster,
                                  emergency_health,
                                  emergency_other,
                                  created_at,
                                  updated_at,
                                  doing_business_as,
                                  doing_business_as_keyword,
                                  ein,
                                  duns,
                                  name,
                                  organization_name_keyword,
                                  website,
                                  address1,
                                  address2,
                                  city,
                                  country_code,
                                  state,
                                  zip_code,
                                  billing_address1,
                                  billing_address2,
                                  billing_city,
                                  billing_country_code,
                                  billing_state,
                                  billing_zip_code,
                                  description,
                                  phone_country,
                                  phone,
                                  phone_ext,
                                  fax_phone_country,
                                  fax_phone,
                                  fax_phone_ext,
                                  categories_ids,
                                  categories_codes,
                                  nigp_categories,
                                  naics_categories,
                                  unspsc_categories,
                                  users_emails,
                                  users_ids,
                                  subscribed_governments_ids,
                                  certification_ids,
                                  self_reported_certifications,
                                  verified_certifications,
                                  vendor_list_ids,
                                  private_vendor_lists,
                                  public_vendor_lists,
                                  ethnicities,
                                  row_number() over (order by id) as row_num 
                                from  promotional_data
        ```
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Building semantic models" defaultOpen={false}>
    A semantic model can be build using Depot as a source. To learn more about this please [refer to this link](https://dashboard.mintlify.com/dataosinfo/dataosinfo/editor/v4#data-ingestion-and-transformation) on creating a semantic model using  Snowflake Depot as a source. You can build semantic model on Redshift, Bigquery, Postgres, Snowflake, and Lakehouse type of Depots. To create a semantic model on other type of Depots one must ingest data to DataOS Lakehouse using Flare Stack.
  </Accordion>

  <Accordion title="Query the source" defaultOpen={false}>
    For better understanding of the data, a user can query the data directly on Workbench without moving the data by creating a Cluster Resource.

    **For example:**

    The below manifest file defines a Minerva-based Cluster Resource, `minervabq`, for querying data within DataOS. It specifies a `query-default` compute environment and configures Minerva-specific settings, including a single replica with resource allocations of 2 CPU cores (2000m) and 4Gi memory for both limits and requests. Debug settings define `INFO` log level for general logs and `ERROR` for Trino logs, ensuring controlled verbosity. The Cluster is linked to a Depot at `dataos://depotbq`, allowing seamless access to data stored in that location for querying and processing.

    ```yaml
    # Resource meta section
    name: minervabq
    version: v1
    type: cluster
    description: testing 
    tags:
      - cluster

    # Cluster-specific section
    cluster:
      compute: query-default
      type: minerva

      # Minerva-specific section
      minerva:
        replicas: 1
        resources:
          limits:
            cpu: 2000m
            memory: 4Gi
          requests:
            cpu: 2000m
            memory: 4Gi
        debug:
          logLevel: INFO
          trinoLogLevel: ERROR
        depots:
          - address: dataos://bqdepot
    ```
  </Accordion>

  <Accordion title="Accelerating queries" defaultOpen={false}>
    For enhancing the query performance, a user can cache the data using Flash Stack.

    **For example:**

    The below manifest file defines a DataOS service named `flashtest`, built on the `flash+python:2.0` Stack. The Service caches a dataset named `city`, stored in `dataos://icebase:retail/city`, and initializes a table `mycity` by selecting data from `retail.city`.&#x20;

    ```yaml
    name: flashtest
    version: v1
    type: service
    tags:
      - service
    description: Flash service
    workspace: public
    service:
      servicePort: 8080
      servicePorts:
        - name: backup
          servicePort: 5433  
      ingress:
        enabled: true
        stripPath: false
        path: /flash/public:flashtest
        noAuthentication: true
      replicas: 1
      logLevel: info
      compute: runnable-default
      envs:
        APP_BASE_PATH: 'dataos-basepath'
        FLASH_BASE_PATH: /flash/public:flashtest
      resources:
        requests:
          cpu: 1000m
          memory: 1024Mi
      stack: flash+python:2.0
      stackSpec:
    # Datasets
        datasets:
          - name: city
            address: dataos://icebase:retail/city

        init:
          - create table mycity as (select * from retail.city)

        # schedule:
        #   - expression: "*/2 * * * *"
        #     sql: INSERT INTO mycustomer BY NAME (select * from customer);
    ```
  </Accordion>

  <Accordion title="Building Data APIs" defaultOpen={false}>
    A user can create the Rest APIs using Depot as a source through Talos Stack. To learn more about the Talos, please refer to this link.
  </Accordion>

  <Accordion title="Building Data Products" defaultOpen={false}>
    A Depot plays a fundamental role in Data Product development, a Data Product can be created directly from the source Depot or from the Icebase (Lakehouse) Depot.

    **For example:**

    The below  manifest file defines the deployment of the `vendor-data-insights` Data Product, categorized under `Vendor Management` with a `Bronze` tier. It provides insights into vendor data, including incorporation details, DBE status, and contact attributes, supporting reporting, tracking, and vendor engagement analysis. The Data Product ingests information from various sources within `operationaldb`, including organizations, categories, vendors, certifications, and user data. The processed insights are stored in `operationlake:analytics:vendor_data_insights`. Additionally, it exposes service endpoints through `Lens` and `Talos` for querying vendor insights and API access.

    ```yaml
    name: vendor-data-insights
    version: v1beta
    entity: product
    type: data

    # High-level description and purpose
    purpose: To provide detailed insights into vendor data, including incorporation details, DBE status, and contact information. This enables reporting, tracking, and analysis of vendor engagement.

    # Product categorization tags
    tags:   
     - DPDomain.VendorManagement
     - DPTier.Bronze
     - DPUsecase.Vendor Insights

    description: Provides vendor details, including state of incorporation, DBE status, and contact attributes like email, phone, and organization information. This data product supports reporting, tracking, and vendor management workflows.

    # External references and documentation
    refs:
     - title: 'Vendor Data Management Info'
       href: https://proud-cobra.dataos.app/metis/assets/table/icebase.icebase.promo_effectiveness.vendor_subscription_insights

    # Main product configuration
    v1beta:
     data:
       # Metadata and tracking information
       meta:
         sourceCodeUrl: https://bitbucket.org/tmdc/vendor-data-insights/src/main/
         title: Vendor Subscription Insights
         trackerUrl: https://rubikai.atlassian.net/browse/DPRB-245?atlOrigin=eyJpIjoiOWMyMDg1ZGE4ZjQ2NDUwNTkzYjYyYTMxM2IwMDY5YWQiLCJwIjoiaiJ9

       # Team members and roles
       collaborators:
         - name: soumadipde
           description: data product owner
         - name: kanakgupta
           description: data product developer
         - name: apoorvverma
           description: consumer of vendor insights

       # Resource bundle configuration
       resource:
         description: 'Resources associated with Vendor Data Insights Data Product'
         purpose: 'Vendor Data Management'
         refType: dataos  
         ref:  bundle:v1beta:vendor-data-bundle

       # Input data sources
       inputs:
         # Vendor database sources
        #  - refType: dataos
        #    ref: dataset:icebase:promo_effectiveness:vendor_subscription_insights

         - refType: dataos
           ref: dataset:operationaldb:public:organization

         - refType: depot
           ref: dataos://operationaldb:public/categories

         - refType: depot
           ref: dataos://operationaldb:public/vendor

         - refType: depot
           ref: dataos://operationaldb:public/certification

         - refType: depot
           ref: dataos://operationaldb:public/pending_user

         - refType: depot
           ref: dataos://operationaldb:public/user

         - refType: depot
           ref: dataos://operationaldb:public/vendor_categories

         - refType: depot
           ref: dataos://operationaldb:public/vendor_lists

         - refType: depot
           ref: dataos://operationaldb:public/government_subscriptions

         - refType: depot
           ref: dataos://operationaldb:public/vendor_list_users

       # Output destinations
       outputs:
         - refType: dataos
           ref: dataset:operationlake:analytics:vendor_data_insights

       # Service endpoints
       ports:
          lens:
            ref: lens:v1alpha:vendor-insights360:public
            refType: dataos

          talos:
          - ref: service:v1:vendor-dataapi:public
            refType: dataos


    ```
  </Accordion>

  <Accordion title="Depot as Lakehouse" defaultOpen={false}>
    Whenever a user ingests the data, it gets stored in the DataOS Lakehouse Depot. To know more about the Lakehouse, please refer to this link.

    **For example:**
    The below manifest file defines the `alphaomega` Lakehouse, which operates on the `Iceberg` format and is hosted on `Azure`. The storage layer is managed through an `S3`-based Icebase Depot, utilizing the `dataos-lakehouse` bucket with a relative path of `/test`. Access control is configured via secrets for read (`alphaomega-r`) and read-write (`alphaomega-rw`) permissions. The Lakehouse employs an `Iceberg REST Catalog` as its metastore for data management and utilizes `Themis` as the query engine.

    ```yaml
    # Resource-meta section 
    name: alphaomega
    version: v1alpha
    type: lakehouse
    tags:
      - Iceberg
      - Azure
    description: Icebase depot of storage-type S3
    owner: iamgroot
    layer: user

    # Lakehouse-specific section 
    lakehouse:
      type: iceberg
      compute: runnable-default
      iceberg:

        # Storage section 
        storage:
          depotName: alphaomega
          type: s3
          s3:
            bucket: dataos-lakehouse   
            relativePath: /test
          secrets:
            - name: alphaomega-r
              keys:
                - alphaomega-r
              allkeys: true 
            - name: alphaomega-rw
              keys:
                - alphaomega-rw
              allkeys: true  

        # Metastore section 
        metastore:
          type: "iceberg-rest-catalog"

        # Query engine section 
        queryEngine:
          type: themis
    ```
  </Accordion>

  <Accordion title="Scanning metadata" defaultOpen={false}>
    After creating the Depot, a user can run a Scanner Workflow for Depot to extract the metadata which later can be accessed on Metis UI. To know more about Scanner, please refer to this.

    **For example:**

    The below manifest file defines the `wf-postgres-depot` Workflow, designed to scan schema tables from a `PostgreSQL`-based Depot and register the extracted data into `Metis2`. It operates under the `scanner:2.0` Stack with Compute Resources managed by `runnable-default` and runs as the `metis` user. The Workflow is tagged as `postgres-depot` and `scanner`, indicating its role in schema discovery. The configuration includes a `depot` specification (`postgresdepot`) and placeholders for source configurations that allow filtering of databases, schemas, and tables based on inclusion and exclusion patterns.

    ```yaml
    version: v1
    name: wf-postgres-depot
    type: workflow
    tags:
      - postgres-depot
    description: The workflow scans schema tables and register data
    workflow:
      dag:
        - name: postgres-depot
          description: The job scans schema from depot tables and register data to metis2
          spec:
            tags:
              - scanner
            stack: scanner:2.0
            compute: runnable-default
            runAsUser: metis
            stackSpec:
              depot: postgresdepot           
              # sourceConfig:           
              #   config:
              #     markDeletedTables: false
              #     includeTables: true
              #     includeViews: true
              #     databaseFilterPattern:
              #       includes:
              #         - <databasename> 
              #       excludes:
              #         - <databasename> 
              #     schemaFilterPattern:
              #       includes:
              #         - <schemaname>
              #       excludes:
              #         - <schemaname>
              #     tableFilterPattern:
              #       includes:
              #         - <schemaname>
              #       excludes:
              #         - <schemaname>
    ```

    After successfully executing the Scanner Workflow, a user can find the metadata on Metis UI as shown below.
  </Accordion>
</AccordionGroup>

## **Governance**

To learn about the governance of Depot, please [refer to this link.](/resources/depot/governance)

## **Observability**

Depots can be monitored using Monitor Resource, to know more about it, please [refer to this link.](/resources/depot/observability)

## **How to catalog a Depot?**

Once a Depot is successfully created, you can view its metadata in the Metis UI. Additionally, any updates made to a Depot can be tracked in the activity section of Metis.

## **Best Practices**&#x20;

This section involves do's and don'ts for managing a Depot.

* As an operator, always assign a use-case to a user instead of directly assigning a tag, as assigning a use-case gives only specific permissions.

* Give your Depot a meaningful name that represents its purpose. **Example:** This makes it easier to manage and identify Depots.

  * ✅ `sales-reports-s3` instead of ❌ `s3-1`

  * ✅ `customer-db-mysql` instead of ❌ `db1`

* Avoid creating a Depot with inline credentials, always create an Instance Secret.

* Do not delete a Depot if other Resources are dependent on it.

## **Data integration - Supported connectors in DataOS**

The catalog of data sources accessible by one or more components within DataOS is provided on the following page: [Supported Connectors in DataOS](https://dataos.info/resources/depot/list_of_connectors/).

## **FAQs**

<AccordionGroup>
  <Accordion title="1. How do I know which data sources are supported in DataOS?" defaultOpen={false}>
    The complete list of supported connectors is available in the **Data Integration - Supported Connectors** section.
  </Accordion>

  <Accordion title="2. What are Uniform Data Links (UDLs), and how do they work?" defaultOpen={false}>
    UDLs allow seamless access to data stored in a Depot without physically copying it. They simplify data sharing, transformation, and querying across different components within DataOS.
  </Accordion>

  <Accordion title="3. How can I use the data in a Depot without moving it?" defaultOpen={false}>
    You can use **Uniform Data Links (UDLs)** to access data stored in Depot without physically moving it. UDLs act as virtual connections, making data available for various operations within DataOS.
  </Accordion>

  <Accordion title="4. Can I restrict access to specific columns or rows in a Depot?" defaultOpen={false}>
    As a developer, you can apply the data masking Policy on a specific column that will replace the actual values with your defined masked characters.
  </Accordion>

  <Accordion title="5. Can you get metadata directly from source data?" defaultOpen={false}>
    Yes, metadata can be extracted directly from source data using the DataOS Scanner Stack. This enables developers to retrieve metadata from external systems such as Postgres, MSSQL, MYSQL, etc.

    **Metadata Extraction Workflows**
    **Depot Scan Workflow:** Uses Depots to scan datasets. Requires the Depot name or address to connect to the data source.

    **Extracted Metadata Includes:**

    * **Dataset/Table Information:** Names, owners, tags, schemas, column names, descriptions

    * **Data Quality & Profiling:** Query usage, user information

    * **Depot Resource:** Metadata about the deployed Depot Resource, its status and operations data.
  </Accordion>

  <Accordion title="6. Can users see everyone’s Depots or only their own?" defaultOpen={false}>
    By default, users with data-dev and operator roles can create and view all Depots. However, access can be restricted by:

    * Removing the “Manage All Depot” use-case.

    * Creating policies to prevent access to the Depot listing endpoint.

    * Exploring Depot details via the Operations App or Metis UI in DataOS.
  </Accordion>

  <Accordion title="7. Are workspaces distinctly different if multiple Depots are spun up?" defaultOpen={false}>
    No. Depots are global entities in DataOS and are not tied to specific workspaces.
  </Accordion>
</AccordionGroup>

##