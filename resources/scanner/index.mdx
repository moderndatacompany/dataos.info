---
title: "Scanner Stack "
---

The Scanner stack in DataOS is a Python-based framework designed for developers to extract metadata from external source systems (such as RDBMS, Data Warehouses, Messaging services, Dashboards, etc.) and the components/services within the DataOS environment to extract information about Data products and DataOS Resources.

With the DataOS Scanner stack, you can extract both general information about datasets/tables, such as their names, owners, and tags, as well as more detailed metadata like table schemas, column names, and descriptions. Additionally, this stack can help you retrieve metadata related to data quality and profiling, query usage, and user information associated with your data assets.

It can also connect with Dashboard and Messaging services to get the related metadata. For example, in the case of dashboards, it extracts information about the dashboard, dashboard Elements, and associated data sources.

Using the Scanner stack within DataOS, metadata can be extracted from DataOS Products and DataOS Resources. The extracted metadata offers detailed insights into the input, output & SLOs (Service Level Objectives) for every data product, along with all the data access permissions, infrastructure resources used for creating it and more. Users can track the entire life cycle of data product creation. The Scanner stack collects comprehensive metadata across 	 Resources such as Workflows, Services, Clusters, Depots, etc.including their historical runtime and operations data.

## How Does Scanner Stack Work?

In DataOS, metadata extraction is treated as a job, which is accomplished using a DataOS resource called Workflow. This stack provides the ability to write workflows that extract metadata from various sources and store it in a metadata store. The Scanner workflow typically includes a source, transformations, and a sink.

Similar to an ETL (Extract, Transform, Load) job, the Scanner workflow connects to the metadata source, extracts the metadata, and applies transformations to convert it into a standardized format. The transformed metadata is then pushed to a REST API server, which is backed by a centralized metadata store or database such as MySQL or Postgres. This process can be performed in either a batch or scheduled manner, depending on the requirements.

<Info>
  The default metadata store in DataOS is MetisDB which is a Postgres database.
</Info>

The stored metadata is used by various DataOS components for discoverability, governance, and observability. External apps running on top of DataOS can also fetch this metadata via Metis server APIs.

<Info>
  In DataOS, all the metadata entities are defined and consumed in JSON format.
</Info>

![](/scanner_framework.png "DataOS Scanner stack for metadata extraction")

Apart from the external applications, the Scanner stack can also extract metadata from various applications & services of DataOS. The scanner job reads related metadata and pushes it to the metadata store through the Metis REST API server. You can then explore this information through the Metis UI.

The Scanner job connects with the following DataOS components and stores the extracted metadata to Metis DB:

* **Collation Service**: To scan and publish metadata related to data pipelines, including workflow information, execution history, and execution states. It also collects metadata for historical data such as pods and logs, as well as data processing stacks like <Tooltip tip="Benthos is a robust and efficient stream processing stack within DataOS. It offers a user-friendly declarative YAML programming approach, enabling seamless execution of essential data engineering tasks like data transformation, mapping, schema validation, filtering, and enrichment.">Benthos</Tooltip> and <Tooltip tip="Flare stack is used for building end-to-end data pipelines within DataOS. It uses a YAML-based declarative programming paradigm built as an abstraction over Apache Spark. It offers an all-in-one solution for performing diverse data ingestion, transformation, enrichment, and syndication processes on batch and streaming data.">Flare</Tooltip>, capturing job information and source-destination relationships.&#x20;

* **Gateway Service**: To retrieve information from data profiles (descriptive statistics for datasets) and data quality tables (quality checks for your data along with their pass/fail status). It also scans data related to query usage, enabling insights into heavy datasets, popular datasets, and associations between datasets.

* <Tooltip tip="Heimdall is the authentication, authorization, and governance engine in DataOS, responsible for implementing a robust security strategy. It ensures that only authorized users have access to DataOS resources.!">Heimdall</Tooltip>: To scan and retrieve information about users in the DataOS environment, including their descriptions and profile images. This user information is accessible through the Metis UI.

* **Pulsar** **Service**: To keep listening to the messages being published on it by various other services and stacks within the system.

<Info>
  DataOS Scanner is a flexible and extensible framework; you can easily integrate it with new sources.
</Info>

## Creating and Scheduling Scanner Workflows

Within DataOS, different workflows can be deployed and scheduled, which will connect to the data sources to extract metadata.

* **Depot Scan Workflow**: With this type of Scanner workflow, depots are used to get connected to the metadata source to extract Entities’ metadata. It enables you to scan all the datasets referred by a depot. You need to provide the depot name or address, which will connect to the data source.

* **Non-Depot Scan Workflow**: With this type of scanner workflow, you must provide the connection details and credentials for the underlying metadata source in the YAML file. These connection details depend on the underlying source and may include details such as host URL, project ID, email, etc.

<Info>
  The non-Depot scan can help extract metadata from sources where depot creation is not supported or when you do not have an already created depot.
</Info>

You can write Scanner workflows in the form of a sequential YAML for a pull-based metadata extraction system built into DataOS for a wide variety of sources in your data stack. These workflows can be scheduled to run automatically at a specified frequency.

![](/scanner_yaml.png "Scanner YAML Components")

# Creating Scanner Workflows[¶](https://dataos.info/resources/stacks/scanner/creating_scanner_workflows/#creating-scanner-workflows "Permanent link")

## Prerequisites

* The connection details and credentials for the underlying metadata source. For Example, a Depot is created which enables users to establish connections and retrieve data from various data sources, such as file systems, data lake systems, database systems, etc. without moving the data.

<Accordion title="Depot Example (Snowflake)">
  ```
  name: ${{snowflake-depot}}
  version: v1
  type: depot
  tags:
    - ${{tag1}}
    - ${{tag2}}
  layer: user
  depot:
    type: snowflake
    description: ${{snowflake-depot-description}}
    spec:
      warehouse: ${{warehouse-name}}
      url: ${{snowflake-url}}
      database: ${{database-name}}
    external: true
    connectionSecret:
      - acl: rw
        type: key-value-properties
        data:
          username: ${{snowflake-username}}
          password: ${{snowflake-password}}
  ```
</Accordion>

* Permission to run the Scanner workflow: A user must have either Operator level access (`roles:id:operator` tag) or grant to the “**Run as Scanner User”** use case.

* Include the property `runAsUser: metis` under the `spec` section in the Scanner YAML.

<Note>
  To obtain the required use case, please contact the DataOS system administrator.
</Note>

## Step 1: Creating Scanner YAML Configuration

1. Define resource properties such as name, version, type, owner etc. These properties are common for all resources. To learn more, refer to [Configuring the Resource Section](https://dataos.info/resources/workflow/#configure-the-resource-section) page.

2. Scanner workflows are either single-time run or scheduled to run at a specific cadence. To schedule a workflow, you must add the `schedule` property, under which you define a `cron` To learn about these properties, refer to [Schedulable workflows](https://dataos.info/resources/workflow/#scheduled-workflow).

3. Define the Scanner job properties in the `dag`, such as job name, description.

4. Define the specification for stack and compute for the Scanner workflow. Also specify user ID of the use case assignee. The default value here is `metis`. but 'Run as a Scanner user' use case should be granted to run Scanner workflow.

5. Under the ‘**Scanner**’ section, provide the data source connection details specific to the underlying source to be scanned.
   **For Depot Scan:** Depot provides a reference to the source from which metadata is read/ingested.
   **`depot`**: Give the name or address of the depot. The Scanner job will scan all the datasets referred by a depot. Depot keeps connection details and secrets, so you do not need to give them explicitly in Scanner YAML.