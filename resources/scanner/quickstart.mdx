---
title: "Quick Guide for Scanner"
sidebarTitle: "QuickStart"
description: "This guide explains the steps to create a Scanner Workflow to connect to the metadata source and extract the metadata of various entities. "
---

The Scanner stack in DataOS is designed for developers to extract metadata from external source systems (such as RDBMS, Data Warehouses, Messaging services, Dashboards, etc.) and the components/services within the DataOS environment to extract information about Data products and DataOS Resources.

<Tip>
  Scanning metadata allows for a comprehensive understanding of the source data's structure, which helps in designing and optimizing ETL processes that meet desired use cases and ensure data quality. This guide explains the different ways to scan metadata and provides detailed instructions to perform the metadata scan.
</Tip>

## Quick Steps for Scanner Workflow

Follow the below steps:

![](/quickstart-scan-01.png)

[Depots](https://dataosinfo.mintlify.app/resources/depot/index) are used to connect to metadata sources to extract entities' metadata. Users must provide the Depot name or address to establish a connection and scan all datasets from the data source referenced by the Depot.

### **Source Data in Snowflake**

For illustration purposes, the Snowflake data source will be connected.

![](/quickstart-scan-02.png)

<Steps>
  <Step title="Step 1: Check Required Permissions">
    * To scan metadata from the Snowflake source system, the Snowflake user must have USAGE privileges on required schemas.

    ![](/quickstart-scan-03.png)

    * To run the Scanner workflow, a user must have Metis admin access or a grant for the “Run as Scanner User” use case.

    * Ensure the Depot for Snowflake is created and have Read access. If it does not exist, then

      [create a Depot](https://dataosinfo.mintlify.app/resources/depot/index#steps-to-create-a-depot)

      .
  </Step>

  <Step title="Step 2: Create a Depot to Connect with Snowflake">
    User need to connect with a data source by creating a Depot and then will use the Depot to scan the metadata for the entities.

    * First create an Instance Secret for securing Snowflake credentials, using following manifest file:

    ```yaml
    # Snowflake Read Instance-secret Manifest

    name: ${{snowflake-depot-name}}-r # Unique identifier for Resource, replace ${snowflake-depot-name} with depot name
    version: v1 # Manifest version
    type: instance-secret # Type of the Resource
    description: ${{description}} # Purpose of the Instance-secret
    layer: user # DataOS layer
    instance-secret: 
      type: key-value-properties # Secret type
      acl: r # Access control: 'r' for read-only
      data: 
        username: ${{username}} # replace with snowflake username
        password: ${{password}} # replace with snowflake password
    ```

    Get more information on creating an Instance Secret Resource by following the [Instance Secret document](/resources/instance_secret/index#abfss).&#x20;

    <Warning>
      This step can be skipped if you prefer to create a Depot without referencing an Instance Secret.
    </Warning>

    * Now create a manifest file to hold the configuration details for your Snowflake Depot. A Depot can be created in two ways: either by directly specifying the credentials inline within the same manifest file or by creating an Instance Secret containing those credentials and referencing the Instance Secret by name in the Depot manifest file.

    <CodeGroup>
      ```yaml Inline credentials Depot manifest file
      name: ${{snowflake-depot}}
      version: v1
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        spec:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        connectionSecret:
          - acl: rw
            type: key-value-properties
            data:
              username: ${{snowflake-username}}
              password: ${{snowflake-password}}
      ```

      ```yaml Instance Secret reference Depot manifest file
      name: ${{snowflake-depot}}
      version: v2alpha
      type: depot
      tags:
        - ${{tag1}}
        - ${{tag2}}
      layer: user
      depot:
        type: snowflake
        description: ${{snowflake-depot-description}}
        snowflake:
          warehouse: ${{warehouse-name}}
          url: ${{snowflake-url}}
          database: ${{database-name}}
        external: true
        secrets:
          - name: ${{snowflake-instance-secret-name}}-r
            allkeys: true

          - name: ${{snowflake-instance-secret-name}}-rw
            allkeys: true
      ```
    </CodeGroup>

    * Apply the Depot manifest file through the DataOS CLI by pasting the path in the placeholder, using the command given below:

    ```bash
    dataos-ctl resource apply -f ${{yamlfilepath}}
    ```

    [
    ](https://dataos.info/quick_guides/scan_metadata/depot/#__codelineno-0-1)
  </Step>

  <Step title="Step 3:  Write Scanner Workflow with Filter Patterns">
    Now build a Scanner workflow to scan the data source. The workflow includes the `depot name` and `filter patterns`. Filter patterns enables to control whether or not to include databases/schemas/tables as part of metadata ingestion.

    * Provide the workflow properties, such as version, name, description, tags, etc., in the YAML file.

    * Provide the Depot name or address(Universal Data Link) to connect to the data source.

    * Specify 

      `schemaFilterPattern`

       and 

      `tableFilterPattern`

       to filter schemas/tables which are of interest.

    * Use 

      `includes:`

       and 

      `excludes:`

       to specify schema/table names or a regex rule to include/exclude tables while scanning the schema.

    ```yaml
    version: v1
    name: snowflake-scanner-test                               
    type: workflow
    tags: 
      - snowflake-scanner-test
    description: The workflow scans the schema of tables and registers their metadata
    workflow: 
      dag: 
        - name: scanner2-snowflake
          description: The job scans schema from sanity snowflake Depot tables and registers their metadata on metis2
          spec: 
            stack: scanner:2.0                            
            compute: runnable-default
            stackSpec: 
              depot: snowflaketestsource 
              sourceConfig: 
                config: 
                  schemaFilterPattern: 
                    includes: 
                      - TPCH_SF1$
                  tableFilterPattern:  
                    includes: 
                      - region
                      - supplier
                      - Customer
                      - ORDERS
    ```

    <Info>
       In this case, database filters are not used because the Snowflake Depot is created for the specific database. To configure the Depot in our DataOS cataloging structure, User will need the Snowflake source URL and the database name. (SNOWFLAKE\_SAMPLE\_DATA)
    </Info>


  </Step>

  <Step title="Step 4:  Check Metadata Source Created on Metis">
    On Metis UI, go to Settings > Databases to access it.

    ![](/quickstart-scan-04.png)

    **Scanned Database**

    Click on the database.

    ![](/quickstart-scan-05.png)

    Scanned Tables on Metis using includes Filter Pattern:

    ![](/quickstart-scan-06.png)

    Schema of the Scanned Customer Table (validate with the source)

    ![](/quickstart-scan-07.png)

    To know more about how to specify filters in different scenarios, refer to [Filter Pattern Examples](https://dataosinfo.mintlify.app/resources/scanner/recipes#example-scenarios).





  </Step>
</Steps>