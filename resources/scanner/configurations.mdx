---
'og:image': "Configurations"
title: "How to configure a Scanner ?"
---

## Manifest file attributes in Scanner Workflows

The manifest file within a Scanner stack includes attributes designed to facilitate metadata extraction. These attributes specify source configurations, apply filtering criteria, and manage metadata control. Key functionalities include:

* **Source configuration**: Defines the connection details and parameters for the data source.

* **Filtering**: Enables metadata extraction to be limited to specific databases, schemas, tables, or topics.

* **Metadata Control**: Manages the extraction process by identifying and flagging deleted tables and topics.

## **Scanner Configuration Attributes details**

The Scanner Workflow attributes given below provide further details on their roles in metadata extraction:

### **`spec`**

**Description**: Specs of the Scanner Workflow

| **Data Type** | **Requirement** | **Default Value** | **Possible Value** |
| ------------- | --------------- | ----------------- | ------------------ |
| mapping       | Mandatory       |                   |                    |

**Example Usage:**

```yaml
spec:
  stack: scanner:2.0
```

### **`stack`**

**Description:** A Stack is a Resource that serves as a secondary extension point, enhancing the capabilities of a Workflow Resource by introducing additional programming paradigms.

| **Data Type** | **Requirement** | **Default Value** | **Possible Value**          |
| ------------- | --------------- | ----------------- | --------------------------- |
| string        | Mandatory       | None              | flare/toolbox/scanner/alpha |

**Additional Details:** You also need to specify specific versions of the stack. If no version is explicitly specified, the system will automatically select the latest version as the default option

**Example Usage:**

```yaml
stack: scanner:2.0
```

### **`compute`**

**Description:** A Compute resource provides processing power for the job.

| **Data Type** | **Requirement** | **Default Value** | **Possible Value**                                               |
| ------------- | --------------- | ----------------- | ---------------------------------------------------------------- |
| string        | Mandatory       | None              | runnable-default or any other custom compute created by the user |

**Example Usage:**

```yaml
compute: runnable-default
```

### **`runAsUser`**

**Description:** When the "runAsUser" field is configured with the UserID of the use-case assignee, it grants the authority to perform operations on behalf of that user.

| **Data Type** | **Requirement** | **Default Value** | **Possible Value**              |
| ------------- | --------------- | ----------------- | ------------------------------- |
| string        | Mandatory       | None              | UserID of the Use Case Assignee |

**Additional information**: The default value here is `metis`. but 'Run as a Scanner user' use case should be granted to run Scanner Workflow. **Example Usage:**

`runAsUser: metis`

### **`depot`**

**Description**: Name or address of the Depot. Depot provides a reference to the source from which metadata is read/ingested.

| **Data Type** | **Requirement** | **Default Value** | **Possible Value**                               |
| ------------- | --------------- | ----------------- | ------------------------------------------------ |
| string        | Mandatory       | None              | icebase, redshift\_depot, dataos://icebase, etc. |

**Additional information**: The Scanner job will scan all the datasets referred by a Depot. Scanner Workflow will automatically create a source (with the same name as the Depot name) where the scanned metadata is saved within Metastore.

**Example Usage**:

```yaml
stackSpec:   
  depot: dataos://icebase
```

### **`type`**

**Description**: Type of the dataset to be scanned. This depends on the underlying data source.

| **Data Type** | **Requirement** | **Default Value** | **Possible Value**                  |
| ------------- | --------------- | ----------------- | ----------------------------------- |
| string        | Mandatory       | None              | snowflake, bigquery, redshift, etc. |

**Example Usage**:

```yaml
stackSpec:
  type: snowflake
```

### **`sourceConnection`**

**Description**: Source connection configuration properties required to connect with the underlying data source to be scanned.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| mapping       | optional        | None              | None                |

### **`type`**

**Description**: Data source type in the sourceConnection section.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                 |
| ------------- | --------------- | ----------------- | ----------------------------------- |
| string        | optional        | None              | Redshift, Snowflake, Bigquery, etc. |

**Example Usage**:

```yaml
sourceConnection:
  config:
    type: Snowflake
```

### **`sourceConfig`**

**Description**: Source configuration properties required to control the metadata scan.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| mapping       | Mandatory       | None              | None                |

### **`type`**

**Description**: Specify source config type; This is for type of metadata to be scanned.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                 |
| ------------- | --------------- | ----------------- | ----------------------------------- |
| string        | optional        | None              | DatabaseMetadata, DashboardMetadata |

**Additional information**: There will be more properties under the 'sourceConfig' section to customize and control metadata scanning.

**Example Usage**:

```yaml
sourceConfig:
  config:
    type: DatabaseMetadata
```

### **`databaseFilterPattern`**

**Description**: To determine which databases to include/exclude during metadata ingestion.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| mapping       | Mandatory       | None              |                     |

**Additional information**: Applicable in case of databases/warehouses

### **`includes OR excludes`**

* `includes:` Add an array of regular expressions to this property in the YAML. The Scanner Workflow will include any databases whose names match one or more of the provided regular expressions. All other databases will be excluded.

* `excludes`: Add an array of regular expressions to this property in the YAML. The Scanner Workflow will exclude any databases whose names match one or more of the provided regular expressions. All other databases will be included.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                                                      |
| ------------- | --------------- | ----------------- | ------------------------------------------------------------------------ |
| string        | Optional        | None              | Exact values (e.g., 'employee'), regular expressions (e.g., '^sales.\*') |

**Example Usage**:

```yaml
sourceConfig:
  config:
    type: DatabaseMetadata
    databaseFilterPattern:
      includes:
        - TMDCSNOWFLAKEDB
```

### **`schemaFilterPattern`**

**Description**: To determine which schemas to include/exclude during metadata ingestion.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| mapping       | Mandatory       | None              |                     |

**Additional information**: Applicable in case of databases/warehouses

### **`includes OR excludes`**

* `includes:` Add an array of regular expressions to this property in the YAML. The Scanner Workflow will include any schemas whose names match one or more of the provided regular expressions. All other schemas will be excluded.

* `excludes`: Add an array of regular expressions to this property in the YAML. The Scanner Workflow will exclude any schemas whose names match one or more of the provided regular expressions. All other schemas will be included.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                                                      |
| ------------- | --------------- | ----------------- | ------------------------------------------------------------------------ |
| string        | Optional        | None              | Exact values (e.g., 'employee'), regular expressions (e.g., '^sales.\*') |

**Example Usage**:

```yaml
sourceConfig:
  config:
    schemaFilterPattern:
      excludes:
        - mysql.*
        - information_schema.*
        - ^sys.*
```

**Additional information**: Applicable in case of databases/warehouses

### **`tableFilterPattern`**

**Description**: To determine which tables to include/exclude during metadata ingestion.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                                                      |
| ------------- | --------------- | ----------------- | ------------------------------------------------------------------------ |
| mapping       | Mandatory       | None              | Exact values (e.g., 'employee'), regular expressions (e.g., '^sales.\*') |

**Additional information**: Applicable in case of databases/warehouses

### **`includes OR excludes`**

* `includes:` Add an array of regular expressions to this property in the YAML to include any tables whose names match one or more of the provided regular expressions. All other tables will be excluded.

* `excludes`: Add an array of regular expressions to this property in the YAML. The Scanner Workflow will exclude any tables whose names match one or more of the provided regular expressions. All other tables will be included.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                                                      |
| ------------- | --------------- | ----------------- | ------------------------------------------------------------------------ |
| string        | Optional        | None              | Exact values (e.g., 'employee'), regular expressions (e.g., '^sales.\*') |

**Example Usage**:

```yaml
sourceConfig:
  config:
    tableFilterPattern:
      includes:
        - ^cust.*
```

**Additional information**: Applicable in case of databases/warehouses.

### **`topicFilterPattern`**

**Description**: To determine which topics to include/exclude during metadata ingestion.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| mapping       | Mandatory       | none              |                     |

**Additional information**: Applicable in case of stream data.

### **`includes OR excludes`**

* `includes:` Add an array of regular expressions to this property in the YAML to include any topics whose names match one or more of the provided regular expressions. All other topics will be excluded.

* `excludes`: Add an array of regular expressions to this property in the YAML to exclude any topics whose names match one or more of the provided regular expressions. All other topics will be included.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values**                                                      |
| ------------- | --------------- | ----------------- | ------------------------------------------------------------------------ |
| string        | Optional        | None              | Exact values (e.g., 'employee'), regular expressions (e.g., '^sales.\*') |

**Example Usage**:

```yaml
sourceConfig:
  config:
    topicFilterPattern:
      includes:
        - ^topic00.*
```

<aside>
  💡

  Filter patterns support Regex in includes and excludes expressions.
</aside>

### **`markDeletedTables`**

**Description**: Set the Mark Deleted Tables property to true to flag tables as soft-deleted if they are not present anymore in the source system.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Additional information**: If a dataset is deleted from the source and hasn't been ingested in Metis during a previous Scanner run, there will be no visible change in the scanned metadata on the Metis UI. However, if the deleted dataset has already been ingested in MetisDB from previous Scanner runs, users can run a Scanner Workflow for the specific Depot they want to scan with the **`markDeletedTables: true`** option in the Workflow configuration. After a successful run, users can check the Metis UI to see the tables that have been marked as deleted.

**Example Usage**:

```yaml
sourceConfig:
  config:
    markDeletedTables: false
```

### **`markDeletedTablesfromFilterOnly`**

**Description**: Set the Mark Deleted Tables property to true to flag tables as soft-deleted if they are not present anymore in the source system.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Additional information**: Set this property to true to flag tables as soft-deleted if they are not present anymore within the filtered schema or database only. This flag is useful when you have more than one ingestion pipelines.

**Example Usage**:

```yaml
sourceConfig:
  config:
    markDeletedTablesfromFilterOnly: false
```

### **`ingestSampleData`**

**Description**: Set this property to true to ingest sample data from the topics.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Additional information**: Set this property to true to flag tables as soft-deleted if they are not present anymore within the filtered schema or database only. This flag is useful when you have more than one ingestion pipelines.

**Example Usage**:

```yaml
sourceConfig:
  config:
    ingestSampleData: false
```

### **`markDeletedTopics`**

**Description**: Set this property to true to flag topics as soft-deleted if they are not present anymore in the source system.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Additional information**: Set this property to true to flag tables as soft-deleted if they are not present anymore within the filtered schema or database only. This flag is useful when you have more than one ingestion pipelines.

**Example Usage**:

```yaml
sourceConfig:
  config:
    markDeletedTables: false
```

### **`includeViews`**

**Description**: Set this property to include views for metadata scanning.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Additional information**: Set this property to true to flag tables as soft-deleted if they are not present anymore within the filtered schema or database only. This flag is useful when you have more than one ingestion pipelines.

**Example Usage**:

```yaml
sourceConfig:
  config:
    includeViews: true
```

### **`enableDebugLog`**

**Description**: To set the default log level to debug.

| **Data Type** | **Requirement** | **Default Value** | **Possible Values** |
| ------------- | --------------- | ----------------- | ------------------- |
| boolean       | Optional        | false             | true, false         |

**Example Usage**:

```yaml
sourceConfig:
  config:
    enableDebugLog: true
```

## Example manifest configuration

```yaml
│
└─── TEST_SPORTS_RETAIL # DB Name
│   │
│   └─── PUBLIC # Schema Name
│   │   │
│   │   └─── CUSTOMER # Table Name
│   │   │
│   │   └─── CUSTOMER_ADDRESS # Table Name
│   │   │
│   │   └─── CUSTOMER_DEMOGRAPHICS # Table Name
│   │   │
│   │   └─── CALL_CENTER # Table Name
│   │
│   └─── INFORMATION # Schema Name
│       │
│       └─── ORDERS # Table Name
│       │
│       └─── REGION # Table Name
│       │
│       └─── CUSTOMER # Table Name
│
└─── RETAIL_DB # DB Name
    │
    └─── PUBLIC # Schema Name
        │
        └─── CUSTOMER_ONLINE # Table Name
        │
        └─── CUSTOMER_OFFLINE # Table Name
        │
        └─── LOYAL_CUSTOMER # Table Name
```

Consider Snowflake source contains data have the above given structure, so completer configuration file looks like :

```yaml
name: scanner2-snowflake-depot  # Name of the scanner workflow
version: v1  # Version of the workflow
type: workflow  # Specifies that this is a workflow
tags:
  - scanner  # Tagging as a scanner workflow
  - snowflake  # Tagging for Snowflake data source
description: The workflow scans Snowflake data source through depot scan  
# Description of the workflow

workflow:
  dag:
    - name: scanner2-snowflake-job  # Name of the scanner job
      description: The job scans schema datasets referred to by Snowflake Depot and registers in Metis2
      tags:
          - scanner2  # Tags related to the scanner job
      spec:
        stack: scanner:2.0  # Specifies the scanner stack version
        compute: runnable-default  # Defines the compute resource to be used for processing
        runAsUser: metis  # Specifies the user with execution privileges
        stackSpec:
          depot: snowflake03  # Name of the depot providing the source metadata
          sourceConfig:
            config:
              type: DatabaseMetadata  # Specifies that the source is a database metadata scan
              
              databaseFilterPattern:  # Filtering databases for metadata ingestion
                includes:
                  - ^TEST_SPORTS_RETAIL$  # Regex for including TEST_SPORTS_RETAIL database
                  - RETAIL_DB  
  # When User mention name in the filter pattern, Scanner workflow will automatically convert the name to the filter pattern considering it as a prefix so the created regex will be `^RETAIL_DB.*`
              
              schemaFilterPattern:  # Filtering schemas for metadata ingestion
                excludes:
                  - INFORMATION # Excluding INFORMATION schema 
                  
              tableFilterPattern:  # Filtering tables for metadata ingestion
                includes:
	                - .*CUSTOMER.*    # Including tables which contains CUSTOMER in their name
                
              markDeletedTables: false  # Do not mark deleted tables as soft-deleted
              includeTags: true  # Include metadata tags in the scan
              includeViews: true  # Include views in the metadata scan

```

* **To know more about Filter pattern click HERE**By combining all three filters, user can achieve a hierarchical filtering approach that successively narrows down the scope of the metadata scanning in the Scanner Workflow. This ensures that only the desired databases, schemas, and tables are included in the Workflow based on your specified criteria. If you do not explicitly specify any of these filters then all available entities are scanned.## **Configure Filters in Scanner YAML**It's important to note that filters exclusively support regular expressions. This document will guide you through the use of different filter types by providing suitable regex patterns based on the given situation. Metadata filters can be configured in Scanner YAML under the `sourceConfig` section.```yaml
  sourceConfig:
    config:
      databaseFilterPattern:
        includes:
          - database1
          - database2
        excludes:
          - database3
          - database4
      schemaFilterPattern:
        includes:
          - schema1
          - schema2
        excludes:
          - schema3
          - schema4
      tableFilterPattern:
        includes:
          - table1
          - table2
        excludes:
          - table3
          - table4
  ```<aside>
    💡

    Note that the filter supports regex as `includes` OR `excludes`. When a user specify a pattern in the `includes` section, the Scanner will evaluate which entities match the pattern and include them in the metadata scanning and entities that do not match the pattern will be automatically excluded. The same principle applies to the excludes section, where the Scanner excludes entities that match the specified pattern, while automatically including the rest.
  </aside>

<Warning>
  Note that the filter supports regex as \`includes\` OR \`excludes\`. When a user specify a pattern in the \`includes\` section, the Scanner will evaluate which entities match the pattern and include them in the metadata scanning and entities that do not match the pattern will be automatically excluded. The same principle applies to the excludes section, where the Scanner excludes entities that match the specified pattern, while automatically including the rest.
</Warning>

* ## **Example Scenarios**Let us consider a scenario where we aim to ingest metadata from a Snowflake instance that comprises multiple databases, as shown below. These databases contain various schemas and tables.```yaml
  │
  └─── SNOWFLAKE # DB Name
  │
  └─── SNOWFLAKE_SAMPLE_DATA # DB Name
  │
  └─── TEST_SNOWFLAKE_DB # DB Name
  │
  └─── TEST_HEALTHCARE # DB Name
  │
  └─── TEST_SPORTS_RETAIL # DB Name
  │
  └─── TEST_DUMMY_DB # DB Name
  │
  └─── RETAIL_DB # DB Name
  ```### **Database Filters**Use `databaseFilterPattern` to determine which databases to include/exclude during metadata ingestion.**Example 1**In this particular example, our objective is to ingest metadata of all databases that include the term "SNOWFLAKE" in their names. To achieve this, we would apply the filter pattern **`.*SNOWFLAKE.*`** in the `includes` property. Consequently, this filter pattern will ensure the ingestion of databases such as  `SNOWFLAKE`, `SNOWFLAKE_SAMPLE_DATA` and `TEST_SNOWFLAKE_DB`.```yaml
  sourceConfig:
    config:
      databaseFilterPattern:
        includes:
          - .*SNOWFLAKE.*
  ```**Example 2**If we want to scan only databases that start with `SNOWFLAKE`  then the filter pattern regex applied would be `^SNOWFLAKE.*` and they will include `SNOWFLAKE`, `SNOWFLAKE_SAMPLE_DATA`.```yaml
  sourceConfig:
        config:
          databaseFilterPattern:
            includes:
              - ^SNOWFLAKE.*
  ```**Example 3**In order to exclusively scan the only database with name `SNOWFLAKE`, the filter pattern would be `^SNOWFLAKE$````yaml
  sourceConfig:
        config:
          databaseFilterPattern:
            includes:
              - ^SNOWFLAKE$
  ```**Example 4**In this example, we want to ingest metadata of all databases for which the name starts with `TEST` OR ends with `DB` , then the filter pattern applied would be `^TEST` & `DB$` in the includes property. The scanning process will include databases such as  `TEST_SNOWFLAKEDB` & `DUMMY_DB`.```yaml
  sourceConfig:
    config:
      databaseFilterPattern:
        includes:
          - ^SNOWFLAKE.*
          - .*DB$
  ```### **Schema Filters**Schema filter patterns determine which schemas to include/exclude during metadata ingestion. These are just examples of schemas that could exist in a Snowflake instance, taken for demonstration purpose. The actual schemas and table names may vary based on your specific use case and requirements.```yaml
  │
  └─── SNOWFLAKE # DB Name
  │   │
  │   └─── PUBLIC # Schema Name
  │   │
  │   └─── TPCH_SF1 # Schema Name
  │
  │   └─── TPCH_SF2 # Schema Name
  │   │
  │   └─── INFORMATION_SCHEMA # Schema Name
  │
  └─── SNOWFLAKE_SAMPLE_DATA # DB Name
  │   │
  │   └─── PUBLIC # Schema Name
  │   │
  │   └─── INFORMATION_SCHEMA # Schema Name
  │   │
  │   └─── TPCH_SF1 # Schema Name
  │   │
  │   └─── TPCH_SF10 # Schema Name
  │   │
  │   └─── TPCH_SF100 # Schema Name
  ```**Example 1**Let's consider a scenario where we want to scan the metadata from the "public" schema present in all databases. The filter pattern would be `public`. This will include the schema `public` present in databases `SNOWFLAKE`  `SNOWFLAKE_SAMPLE_DATA`.```yaml
  sourceConfig:
    config:
      schemaFilterPattern:
        includes:
          - public
  ```<aside>
    💡

    When you mention name in the filter pattern, Scanner Workflow will automatically convert the name to the filter pattern considering it as a prefix so the created regex will be `^public.*`
  </aside>**Example 2**We wish to exclude the schema `TPCH_SF100` from metadata scanning. As this schema is present only in one database, you can use `excludes` property with the pattern `^TPCH_SF100$` .```yaml
  sourceConfig:
    config:
      schemaFilterPattern:
        excludes:
          - ^TPCH_SF100$
  ```**Example 3**Suppose we intend to include all schemas that begin with the prefix "TPCH" in all databases. In this case, the appropriate regular expression (regex) to achieve this would be **`^TPCH.*`**. This regex pattern will result in the metadata scan of schemas such as `TPCH_SF1` and `TPCH_SF2` from the `SNOWFLAKE` database, as well as schemas like `TPCH_SF1`, `TPCH_SF10`, and `TPCH_SF100` from the `SNOWFLAKE_SAMPLE_DATA` database.```yaml
  sourceConfig:
    config:
      ...
      schemaFilterPattern:
        includes:
          - ^TPCH*
  ```**Example 4**We want to include only the schema TPCH\_SF1 present in all the databases but not TPCH\_SF100 or TPCH\_SF1000.```yaml
  sourceConfig:
    config:
      ...
      schemaFilterPattern:
        includes:
          - ^TPCH_SF1$
  ```### **Table Filter pattern**Use `tableFilterPattern` to determine which tables to include/exclude during metadata ingestion.```yaml
  │
  └─── TEST_SPORTS_RETAIL # DB Name
  │   │
  │   └─── PUBLIC # Schema Name
  │   │   │
  │   │   └─── CUSTOMER # Table Name
  │   │   │
  │   │   └─── CUSTOMER_ADDRESS # Table Name
  │   │   │
  │   │   └─── CUSTOMER_DEMOGRAPHICS # Table Name
  │   │   │
  │   │   └─── CALL_CENTER # Table Name
  │   │
  │   └─── INFORMATION # Schema Name
  │       │
  │       └─── ORDERS # Table Name
  │       │
  │       └─── REGION # Table Name
  │       │
  │       └─── CUSTOMER # Table Name
  │
  └─── RETAIL_DB # DB Name
      │
      └─── PUBLIC # Schema Name
          │
          └─── CUSTOMER_ONLINE # Table Name
          │
          └─── CUSTOMER_OFFLINE # Table Name
          │
          └─── LOYAL_CUSTOMER # Table Name
  ```**Example 1**In this example, our objective is to scan only the table with the name `CUSTOMER` from all the schemas of `TEST_SPORTS_RETAIL` database. To achieve this, we can use the filter pattern **`^CUSTOMER$`**. By applying this pattern during the metadata scanning process, the tables named `CUSTOMER` from the `TEST_SPORTS_RETAIL.PUBLIC` and `TEST_SPORTS_RETAIL.INFORMATION` schemas will be included.```yaml
  sourceConfig:
    config:
      ...
      databaseFilterPattern:
        includes:
          - TEST_SPORTS_RETAIL 
      tableFilterPattern:
        includes:
          - ^CUSTOMER$
  ```**Example 2**In this example scenario, we want to scan all the tables having CUSTOMER in their name. We will use the filter pattern .*CUSTOMER.*. This will result in scanning of all the tables such as `CUSTOMER`, `CUSTOMER_ONLINE`, `CUSTOMER_OFFLINE`, `LOYAL_CUSTOMER`, etc.```yaml
  sourceConfig:
    config:
      ...
      tableFilterPattern:
        includes:
          - .*CUSTOMER.*
  ```