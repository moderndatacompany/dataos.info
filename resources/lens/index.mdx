---
title: "Introduction"
description: "Lens Resource in DataOS is a logical modeling layer for accessing tabular data in data warehouses or lakehouses. It operates on top of physical tables, allowing the extension of these tables into logical tables by adding logical columns (measures) and relationships. It empowers analytical engineers, the key architects of business intelligence, with a model-first approach.
"
---

<Tip>
  **Lens in the Data Product Lifecycle**

  Lens operates in the consumption layer of the Data Product Life Cycle within DataOS, By leveraging Lens, Data Products can be created to inform decision-making, ensuring data is well-organized and aligned with business objectives. To consume it, Lens exposes APIs such as Postgres, REST, and Graphql.
</Tip>

**Why Lens?**

The data modeling layer serves as an interface that overlays the underlying data, consistently presenting business users with familiar and well-defined terms like `product`, `customer`, or `revenue`. This abstraction enables users to access and consume data in a way that aligns with their understanding, facilitating self-service analytics and reducing dependence on data engineers for ad-hoc data requests.

As a resource within the DataOS ecosystem, Lens enhances Data Product consumption by delivering improvements in how Data Products are accessed and utilized. It streamlines the developer experience in consumption patterns, focusing specifically on refining the use and interaction with data products.

## Key features of Lens

Lens  is engineered to handle complex and large-scale data models with ease. Key features include:

* **Code modularity:** Lens supports modular code structures, simplifying the maintenance of extensive models, particularly when dealing with real-world entities (represented as tables), dimensions, and measures. This modularity enables efficient development and management, allowing teams to navigate large codebases with reduced complexity.

* **Segments:** [Segments](/resources/lens/working_with_segments/) are predefined filters that enable the definition of complex filtering logic in SQL. They allow you to create specific subsets of data, such as users from a particular city, which can be reused across different queries and reports. This feature helps streamline the data exploration process by simplifying the creation of reusable filters.

* **API Support:** Lens enhances interoperability by simplifying application development with support for [Postgres API](/resources/lens/exploration_of_deployed_lens_using_sql_apis/), [REST API](/resources/lens/exploration_of_deployed_lens_using_rest_apis/), and [Graphql](https://dashboard.mintlify.com/dataosinfo/dataosinfo/editor/v4#graphql). Additionally, learn how to [work with payloads](/resources/lens/working_with_payload/) for querying and interacting with the system in the API Documentation.

* **Governance and Access Control:** Lens ensures data governance through[ user group management and data policies](/resources/lens/working_with_user_groups_and_data_policies/), enabling precise control over who can access and interact with data models.

* **BI integration:** Lens improves interoperability through robust integration with PowerBI, Tableau and Superset. This ensures that data models can be easily utilized across various BI platforms, enhancing the overall analytics experience. For more details on BI integration, visit the [BI Integration Guide](/resources/lens/bi_integration/).

* **Performance optimization through Flash:** Designed to work with DataOS Lakehouse and Iceberg-format depots, [Flash](/resources/stacks/flash/) improves query performance by leveraging in-memory execution. This optimization ensures that data teams can efficiently handle large-scale queries with enhanced speed and performance.

## How to build a Semantic model?

The process begins with creating a new Lens project and generating a data model. Once the model is prepared, it will be tested within the development environment to ensure it is error-free before deployment.

### Single source

<AccordionGroup>
  <Accordion title="Redshift" defaultOpen={false}>
    ## Step 1: Create the AWS Redshift Depot

    If the Depot is inactive, you must create one using the provided template.

    ```yaml
    name: ${{redshift-depot-name}}
    version: v2alpha
    type: depot
    tags:
    - ${{redshift}}
    layer: user
    description: ${{Redshift Sample data}}
    depot:
    type: REDSHIFT
    redshift:
      host: ${{hostname}}
      subprotocol: ${{subprotocol}}
      port: ${{5439}}
      database: ${{sample-database}}
      bucket: ${{tmdc-dataos}}
      relativePath: ${{development/redshift/data_02/}}
    external: ${{true}}
    secrets:
      - name: ${{redshift-instance-secret-name}}-r
        allkeys: true

      - name: ${{redshift-instance-secret-name}}-rw
        allkeys: true

    ```

    ## Step 2: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](https://dataosinfo.mintlify.app/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    ## Step 1: Create the AWS Redshift Depot

    If the Depot is inactive, you must create one using the provided template.

    ```yaml
    name: ${{redshift-depot-name}}
    version: v2alpha
    type: depot
    tags:
    - ${{redshift}}
    layer: user
    description: ${{Redshift Sample data}}
    depot:
    type: REDSHIFT
    redshift:
      host: ${{hostname}}
      subprotocol: ${{subprotocol}}
      port: ${{5439}}
      database: ${{sample-database}}
      bucket: ${{tmdc-dataos}}
      relativePath: ${{development/redshift/data_02/}}
    external: ${{true}}
    secrets:
      - name: ${{redshift-instance-secret-name}}-r
        allkeys: true

      - name: ${{redshift-instance-secret-name}}-rw
        allkeys: true
    ```

    ## Step 2: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    Once the Lens with semantic model is prepared, create a `lens_deployment.yml` file parallel to the model folder to configure the deployment using the YAML template below.

    ```yaml
    version: v1alpha
    name: "redshiftlens"
    layer: user
    type: lens
    tags:
      - lens
    description: redshiftlens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: depot # source type is depot here
        name: redshiftdepot # name of the redshift depot
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/depot/redshift/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional    
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi
      worker: # optional
        replicas: 2 # optional
        logLevel: debug  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      router: # optional
        logLevel: info  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      metric:
        logLevel: info  # Logging level for the metric component
    ```

    Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

    * **Defining the Source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the AWS Redshift Depot created.

    * **Setting Up Compute and Secrets:**

      * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

      * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

    <Code>
      ```yaml Command
      dataos-ctl resource apply -f ${manifest-file-path}
      ```
    </Code>

    ## Docker compose manifest file

    <Accordion title="Click here to see the full docker compose manifest file" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-donkey.dataos.app

        # Overview
        LENS2_NAME: ${redshiftlens}
        LENS2_DESCRIPTION: "Ecommerce use case on sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "iamgroot"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: ${depot}  
        LENS2_SOURCE_NAME: ${redshiftdepot}
        LENS2_SOURCE_CATALOG_NAME: ${redshiftdepot}

        # Log
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        #LENS_DB_QUERY_TIMEOUT: 15m
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-02
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model

      ```
    </Accordion>

    ## Check query statistics for AWSRedshift

    <Note>
      Ensure the user has AWS console access before proceeding.
    </Note>

    <Steps>
      <Step title="Log in to AWS Console ">
        Login to the AWS Console and search for ‘Redshift’ in the AWS Console search bar to access the Redshift.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled1.png)
      </Step>

      <Step title="Select Redshift Cluster">
        Click on ‘Amazon Redshift’ from the search results. You will be directed to the Redshift dashboard. Select the appropriate region and choose the desired cluster name from the list.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled2.png)
      </Step>

      <Step title="Access Query Monitoring">
        Select the cluster you want to monitor. Navigate to the ‘Query monitoring’ tab to view query statistics.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled3.png)
      </Step>

      <Step title="View running and completed queries">
        In the ‘Query monitoring’ tab, you will see a list of running and completed queries.&#x20;

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled4.png)
      </Step>

      <Step title="Monitor specific query">
        Click on the query you want to monitor and view the query statistics, as shown in the example below.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled5.png)
      </Step>
    </Steps>
  </Accordion>

  <Accordion title="BigQuery" defaultOpen={false}>
    ## Step 1: Create the Bigquery Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: ${{bigquerydepot}}
    version: v2alpha
    type: depot
    tags:
      - ${{dropzone}}
      - ${{bigquery}}
    owner: ${{owner-name}}
    layer: user
    depot:
      type: BIGQUERY                 
          description: ${{description}} # optional
      external: ${{true}}
      secrets:
        - name: ${{bq-instance-secret-name}}-r
          allkeys: true

        - name: ${{bq-instance-secret-name}}-rw
          allkeys: true
      bigquery:  # optional                         
        project: ${{project-name}} # optional
        params: # optional
          ${{"key1": "value1"}}
          ${{"key2": "value2"}}

    ```

    ## Step 2: Prepare the Lens model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure, only the necessary columns are extracted, and the SQL dialect is specific to the bigquery. For instance,

        * &#x20;format table names as: `project_id.dataset.table`.

        * Use `STRING` for text data types instead of `VARCHAR`.

        * Replace generic functions with BigQuery’s `EXTRACT` function.

        For instance, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For instance, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For instance, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](/resources/lens/working_with_user_groups_and_data_policies/).

        ## Step 3: Deployment manifest file

        After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

        ```yaml
        # RESOURCE META SECTION
        version: v1alpha # Lens manifest version (mandatory)
        name: "bigquery-lens" # Lens Resource name (mandatory)
        layer: user # DataOS Layer (optional)
        type: lens # Type of Resource (mandatory)
        tags: # Tags (optional)
          - lens
        description: bigquery depot lens deployment on lens2 # Lens Resource description (optional)

        # LENS-SPECIFIC SECTION
        lens:
          compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
          secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
            - name: bitbucket-cred # Referred Instance Secret name (mandatory)
              allKeys: true # All keys within the secret are required or not (optional)

          source: # Data Source configuration
            type: depot # Source type is depot here
            name: bigquerydepot # Name of the bigquery depot

          repo: # Lens model code repository configuration (mandatory)
            url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
            lensBaseDir: sample/lens/source/depot/bigquery/model # Relative path of the Lens 'model' directory in the repository (mandatory)
            syncFlags: # Additional flags used during synchronization, such as specific branch.
              - --ref=lens # Repository Branch

          api: # API Instances configuration (optional)
            replicas: 1 # Number of API instance replicas (optional)
            logLevel: info  # Logging granularity (optional)
            resources: # CPU and memory configurations for API Instances (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 2000m
                memory: 2048Mi

          worker: # Worker configuration (optional)
            replicas: 2 # Number of Worker replicas (optional)
            logLevel: debug # Logging level (optional)
            resources: # CPU and memory configurations for Worker (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          router: # Router configuration (optional)
            logLevel: info  # Level of log detail (optional)
            resources: # CPU and memory resource specifications for the router (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          iris:
            logLevel: info # Level of log detail (optional)
            resources: # CPU and memory resource specifications for the iris board (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          metric:    #optional
            logLevel: info
        ```

        Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

        * **Defining the Source:**

          * **Source type:** The `type` attribute in the `source` section must be explicitly set to `depot`.

          * **Source name:** The `name` attribute in the `source` section should specify the name of the Bigquery Depot created .

        * **Setting Up Compute and Secrets:**

          * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

          * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

        * **Defining Repository:**

          * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

          * **`lensBaseDir`:** The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/bigquery/model`.

          * **`secretId`:** The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub) . It specifies the secret needed to securely authenticate and access the repository.

          * **`syncFlags`**: Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model rsides in the dev branch.

        * **Configuring API, Worker and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.
      </Step>
    </Steps>

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command. Replace the placeholder with the&#x20;

    ```
    dataos-ctl apply -f {manifest-file-path}}
    ```

    ## Docker compose manifest file

    Ensure that highlighted attributes are in the Docker Compose Manifest file for proper configuration during the connection setup process.

    <Accordion title="Click here to see the full docker-compose manifest" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-monkey.dataos.app
        # Overview
        LENS2_NAME: sales360
        LENS2_DESCRIPTION: "Ecommerce use case on sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "rakeshvishvakarma, iamgroot"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: ${depot} #source name - depot
        LENS2_SOURCE_NAME: ${bigquerydepot} #name of the bigquery depot
        DATAOS_RUN_AS_APIKEY: ${A1ZjMDliZTFhZWJhMQ==}
        # LogZjAtNDY4My05
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-05
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model
            # - ./scripts/commons.js:/app/scripts/commons.js
            # - ./scripts/bootstrap.js:/app/scripts/bootstrap.js
            # - ./scripts/config.js:/app/scripts/config.js

      ```
    </Accordion>
  </Accordion>

  <Accordion title="Postgres" defaultOpen={false}>
    ## Step 1: Create Postgres Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: ${{postgresdb}}
    version: v2alpha
    type: depot
    layer: user
    depot:
      type: JDBC                  
      description: ${{To write data to postgresql database}}
      external: ${{true}}
      secrets:
        - name: ${{sf-instance-secret-name}}-r
          allkeys: true

        - name: ${{sf-instance-secret-name}}-rw
          allkeys: true
      postgresql:                        
        subprotocol: "postgresql"
        host: ${{host}}
        port: ${{port}}
        database: ${{postgres}}
        params: #Required 
          sslmode: ${{disable}}
    ```

    While creating Lens on Postgres Depot the following aspects need to be considered:

    * The SQL dialect used in the `model/sql` folder to load data from the Postgres source should be of the Postgres dialect.

    * The table naming in the `model/table` should be of the format: `schema.table`.

    ## Step 2: Prepare the semantic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

    ```
    dataos-ctl  apply -f ${manifest-file-path}
    ```

    ## Docker compose manifest file

    <Accordion title="Click here to see the full docker compose manifest file" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-monkey.dataos.app
        # Overview
        LENS2_NAME: sales360
        LENS2_DESCRIPTION: "Ecommerce use case on Adventureworks sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "iamgroot, iamloki"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: depot
        LENS2_SOURCE_NAME: postgreslens2
        DATAOS_RUN_AS_APIKEY: bGVuc3NzLmUzMDA1ZjMzLTZiZjAtNDY4My05ZjhhLWNhODliZTFhZWJhMQ==
        LENS2_DB_SSL : "true"
        # Log
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-05
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model
      ```
    </Accordion>
  </Accordion>

  <Accordion title="Snowflake" defaultOpen={false}>
    ## Prerequisite

    CLI Version should be `dataos-cli 2.26.39-dev` or greater.

    ## Step 1: Create the Snowflake Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: snowflake-depot
    version: v2alpha
    type: depot
    tags:
      - Snowflake depot
      - user data
    layer: user
    depot:
      name: sftest
      type: snowflake
      description: Depot to fetch data from Snowflake datasource
      secrets:
        - name: sftest-r
          keys:
            - sftest-r
          allKeys: true
        - name: sftest-rw
          keys:
            - sftest-rw
          allKeys: true
      external: true
      snowflake:
        database: TMDC_V1
        url: ABCD23-XYZ8932.snowflakecomputing.com
        warehouse: COMPUTE_WH
        account: ABCD23-XYZ8932
      source: sftest
    ```

    ## Step 2: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure the SQL dialect matches snowflake syntax. Format table names as `schema.table`.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
         "sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        #### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        #### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

    [](https://dataos.info/resources/lens/data_sources/snowflake/#__codelineno-2-1)

    ```yaml
    # RESOURCE META SECTION
    version: v1alpha # Lens manifest version (mandatory)
    name: "snowflake-lens" # Lens Resource name (mandatory)
    layer: user # DataOS Layer (optional)
    type: lens # Type of Resource (mandatory)
    tags: # Tags (optional)
      - lens
    description: snowflake depot lens deployment on lens2 # Lens Resource description (optional)

    # LENS-SPECIFIC SECTION
    lens:
      compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
      secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
        - name: bitbucket-cred # Referred Instance Secret name (mandatory)
          allKeys: true # All keys within the secret are required or not (optional)

      source: # Data Source configuration
        type: depot # Source type is depot here
        name: snowflake-depot # Name of the snowflake depot

      repo: # Lens model code repository configuration (mandatory)
        url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
        lensBaseDir: sample/lens/source/depot/snowflake/model # Relative path of the Lens 'model' directory in the repository (mandatory)
        syncFlags: # Additional flags used during synchronization, such as specific branch.
          - --ref=lens # Repository Branch

      api: # API Instances configuration (optional)
        replicas: 1 # Number of API instance replicas (optional)
        logLevel: info  # Logging granularity (optional)
        resources: # CPU and memory configurations for API Instances (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi

      worker: # Worker configuration (optional)
        replicas: 2 # Number of Worker replicas (optional)
        logLevel: debug # Logging level (optional)
        resources: # CPU and memory configurations for Worker (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      router: # Router configuration (optional)
        logLevel: info  # Level of log detail (optional)
        resources: # CPU and memory resource specifications for the router (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      iris:
        logLevel: info # Level of log detail (optional)
        resources: # CPU and memory resource specifications for the iris board (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      metric:    #optional
        logLevel: info
    ```

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings, apply the manifest using the following command:

    ```
    dataos-ctl apply -f ${manifest-file-path}
    ```
  </Accordion>
</AccordionGroup>

### Multi source

<AccordionGroup>
  <Accordion title="Minerva" defaultOpen={false}>
    ## Prerequisite

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for access control
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h3">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "minervalens"
    layer: user
    type: lens
    tags:
      - lens
    description: minerva deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: minerva #minerva/themis/depot
        name: minervacluster  #name of minerva cluster
        catalog: icebase
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/minerva/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional 
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi

      worker: # optional
        replicas: 2 # optional
        logLevel: debug  # optional

        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      router: # optional
        logLevel: info  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `minerva`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Minerva Cluster. For example, if the name of your Minerva Cluster is miniature the Source name would be `miniature`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Minerva Cluster that you intend to use. For instance, if the catalog is named icebase, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    <Info>
      Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
    </Info>

    ## Docker compose manifest file

    <Accordion title="Click here to view the complete docker compose  manifest and it's procedure" defaultOpen={false}>
      ```
      Prerequisites
      The hostname for the Trino database server.
      The username for the DataOS User.
      The name of the database to use with the Minerva query engine database server.
      Docker compose configuration
      Add the following environment variables to your Lens (.env) file
      Environment variables attribute
      Example
      trino.zip

      ```
    </Accordion>

    ## Check Query Stats for Minerva

    To check the query statistics, please follow the steps below:

    <Steps>
      <Step title="Open the Operations app">
        By default, the **User Space** tab is displayed upon accessing the Operations app. In User Space click on the 'Minerva Queries' tab to view query execution details.

        Set the following filters:

        * Source: `lens2`

        * Dialect: `trino_sql`

        Optionally, refine your results by filtering based on **Cluster**, **Username**, or other available criteria as needed.

        ![](https://mintlify.s3-us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/minerva/Untitled1.png)
      </Step>

      <Step title="Select the query id">
        Identify and choose the **Query ID** of interest from the **Minerva Queries** tab. Once selected, the system will display detailed execution statistics, providing insights into performance, execution time, and resource utilization.
      </Step>
    </Steps>
  </Accordion>

  <Accordion title="Themis" defaultOpen={false}>
    ## Prerequisites

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for governance
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h3">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "themis-lens"
    layer: user
    type: lens
    tags:
      - lens
    description: themis lens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: themis #minerva/themis/depot
        name: lenstestingthemis
        catalog: icebase
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/themis/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=main #repo-name

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional    
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi
      worker: # optional
        replicas: 2 # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      router: # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the Source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `themis`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Themis Cluster. For example, if the name of your Themis Cluster is `clthemis` the Source name would be `clthemis`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Themis Cluster that you intend to use. For instance, if the catalog is named `lakehouse_retail`, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    The above manifest is intended for a cluster named `lenstestingthemis`, created on the themis source, with the depot or data catalog named `icebase`. To use this manifest, copy the file and update the source details accordingly.

    <Info>
      Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
    </Info>
  </Accordion>
</AccordionGroup>

## Configurations

Lens can be configured to connect to different sources using data source attributes and configurable attributes in the `docker-compose.yml` or `lens.yml` manifest files. Here is a comprehensive guide to APIs and configuring supported properties.

* [Configuration Fields of the Deployment Manifest File (YAML) for Lens Resource](/resources/lens/lens_manifest_attributes/)
  Understand the various configuration fields available in the deployment manifest file for Lens resources.

* [Configuration Fields of the Docker Compose File](/resources/lens/docker_compose_manifest_attributes/)
  Review the configuration fields and settings in the Docker Compose file for orchestrating multi-container applications.

<Info>
  🗣️ If working with Lens 1.0 interface, click [here](/interfaces/lens/).
</Info>

## How to consume the semantic model?

After creating a Lens data model, the next step is to consume it—this means interacting with the model by running queries. The following section explains the key concepts for querying Lens through various methods, though all queries follow the same general format. Multiple ways are available to explore or interact with the Lens model or its underlying data, allowing you to ask meaningful questions of the data and retrieve valuable insights. Exploration can be performed using the following methods:

### BI Tools

<AccordionGroup>
  <Accordion title="Power BI" defaultOpen={false}>
    <Tabs>
      <Tab title="Using CLI">
        ## Prerequisites

        Before proceeding with the integration of Lens with Power BI, ensure that you have the following prerequisites in place:

        * **CURL**: Ensure that CURL is installed on your system. Windows users may need to use `curl.exe`.

        * **Lens API Endpoint**: The API endpoint provided by Lens to synchronize the semantic model, enabling integration with Power BI.

        * **DataOS API key**: You will need your apikey  for authentication. The API key can be obtained using the following command in your terminal:

        ### Curl command

        Use the following CURL command to synchronize the semantic model with Power BI:

        <CodeGroup>
          ```bash Command
          curl --location --request POST 'https://<DATAOS_FQDN>/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' \
            --header 'apikey: ${APIKEY}' \
            --output ${FILE_NAME}.zip
          ```

          ```
          curl --location --request POST 'https://alpha-omega.dataos.app /lens2/sync/api/v1/power-bi/curriculum:sales360' \
            --header 'apikey: abcdefgHi==' \
            --output ${FILE_NAME}.zip  
          ```
        </CodeGroup>

        ## Parameters

        | Parameter      | Description                                                                                                               |
        | -------------- | ------------------------------------------------------------------------------------------------------------------------- |
        | URL            | The URL  for syncing Lens with Power BI. It includes:                                                                     |
        | apikey         | User's API key for the current context in Lens.                                                                           |
        | `filename.zip` | Here you give the name of the Power BI file with which name you wish to download it. For example, `product-analysis.zip`. |

        This is a  workspace.
      </Tab>

      <Tab title="Using DPH" />
    </Tabs>
  </Accordion>

  <Accordion title="Tableau" defaultOpen={false}>
    Content for Tableau goes here.
  </Accordion>

  <Accordion title="Superset" defaultOpen={false}>
    Content for Superset goes here.
  </Accordion>
</AccordionGroup>

## How to do data modelling in semantic model?

[Data modeling](/resources/lens/overview/) is the process of defining and structuring raw data into organized and meaningful business definitions. It involves creating logical schemas, relationships, and aggregations to represent how data is stored, processed, and accessed. Effective data modeling ensures optimal query performance and allows users to extract valuable insights without modifying the underlying data structure. Below are resources to guide you through essential aspects of data modeling to optimize performance and accuracy.

* [Data modelling concepts](/resources/lens/concepts/) and [Overview](/resources/lens/overview/):
  Understand the core principles and methodologies essential for designing effective data models.

* [Do's and don'ts:](/resources/lens/dos_and_donts/)
  Explore recommended guidelines and techniques to create efficient and scalable data models, along with a concise list of actions to follow and pitfalls to avoid.

* [Error reference:](/resources/lens/errors/)
  A quick reference for understanding and resolving common errors in data modeling.

## How to observe and monitor a semantic model?

* How to audit a semantic model?

* Access control

  * Data Source

  * Bifrost

  * Metis

  * User groups

## How to govern a semantic model?

## How are semantic models orchestrated?

## How to configure a semantic model?

## How to catalog a semantic model?

## How to fine-tune a semantic model?

The Lens semantic layer provides several optimization techniques that can significantly enhance the performance of data queries. The following page explores best practices and strategies for fine-tuning your Lens model to maximize efficiency.

* [Optimizing Lens model: Best practices for the Semantic Layer](/resources/lens/fine_tuning_a_lens_model/)