---
title: "Introduction"
description: "Lens Resource in DataOS enables the creation of a semantic modeling layer, providing structured access to tabular data in data warehouses and lakehouses. By operating on top of physical tables, extending it into logical models by defining measures, dimensions, and relationships without altering the underlying data. "
---

<Tip>
  **Lens in the Data Product Lifecycle**

  Lens operates in the consumption layer of the Data Product Life Cycle within DataOS, By leveraging Lens, semantic model can be created to inform decision-making, ensuring data is well-organized and aligned with business objectives. To consume it, Lens exposes APIs such as Postgres, REST, and Graphql.
</Tip>

**Why Lens?**

The semantic modeling layer of the Lens serves as an interface that overlays the underlying data, consistently presenting business users with familiar and well-defined terms such as `product`, `customer`, or `revenue`. This abstraction enables users to access and consume data in a way that aligns with their understanding, facilitating self-service analytics and reducing dependence on data engineers for ad-hoc data requests.

As a Resource within the DataOS ecosystem, Lens enhances Data Product consumption by delivering improvements in how Data Products are accessed and utilized. It streamlines the developer experience in consumption patterns, focusing specifically on refining the use and interaction with Data Products. It empowers analytical engineers, the key architects of business intelligence, with a model-first approach.&#x20;

## Key features of Lens

Lens is engineered to handle complex and large-scale semantic models with ease. Key features include:

* **Code modularity:**

  Lens supports modular code structures, simplifying the maintenance of extensive models, particularly when dealing with real-world entities (represented as tables), dimensions, and measures. This modularity enables efficient development and management, allowing teams to navigate large codebases with reduced complexity.

* **Segments:**

  [Segments](/resources/lens/working_with_segments/)

  are predefined filters that enable the definition of complex filtering logic in SQL. It allows to create specific subsets of data, such as users from a particular city, which can be reused across different queries and reports. This feature helps streamline the data exploration process by simplifying the creation of reusable filters.

* **API support:**

  Lens enhances interoperability by simplifying application development with support for

  [Postgres API](/resources/lens/explore_lens_using_sql_apis/)

  ,

  [REST API](/resources/lens/explore_lens_using_rest_apis/)

  , and

  [GraphQL API](/resources/lens/explore_lens_using_graphql/)

  .

* **Governance and access control:**

  Lens ensures data governance through

  [ user group management and data policies](/resources/lens/working_with_user_groups_and_data_policies/)

  , enabling precise control over who can access and interact with the semantic model.

* **BI integration:**

  Lens improves interoperability through robust integration with PowerBI, Tableau, and Superset. This ensures that semantic models can be easily utilized across various BI platforms, enhancing the overall analytics experience. For more details on BI integration, visit the

  [BI Integration guide](/resources/lens/bi_integration/)

  .

* **Performance optimization through Flash:**

  Designed to work with DataOS Lakehouse and Iceberg-format Depots,

  [Flash](/resources/stacks/flash/)

  improves query performance by leveraging in-memory execution. This optimization ensures that data teams can efficiently handle large-scale queries with enhanced speed and performance.

## How to build a semantic model using Lens?

The process begins with creating a new Lens project and generating a semantic model. Once the model is prepared, it will be tested within the development environment to ensure it is error-free before deployment.

### Single source

<AccordionGroup>
  <Accordion title="Redshift" defaultOpen={false}>
    The following document guides on building a semantic model on Redshift as source.

    ## Step 1: Create the AWS Redshift Depot

    If the Depot is inactive, create one using the provided template.

    ```yaml
    name: ${{redshift-depot-name}}
    version: v2alpha
    type: depot
    tags:
    - ${{redshift}}
    layer: user
    description: ${{Redshift Sample data}}
    depot:
    type: REDSHIFT
    redshift:
      host: ${{hostname}}
      subprotocol: ${{subprotocol}}
      port: ${{5439}}
      database: ${{sample-database}}
      bucket: ${{tmdc-dataos}}
      relativePath: ${{development/redshift/data_02/}}
    external: ${{true}}
    secrets:
      - name: ${{redshift-instance-secret-name}}-r
        allkeys: true

      - name: ${{redshift-instance-secret-name}}-rw
        allkeys: true
    ```

    ## Step 2: Prepare the sematic model folder

    In the model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each sub folder contains specific files related to the semantic model. Download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls/` folder, create `.sql` files for each table, where each file is responsible for loading or selecting the relevant data from the source. Ensure only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
          "onelakehouse"."retail".channel;
        ```

        Alternatively,  write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "onelakehouse"."retail".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create views" titleSize="h2">
        Create a `views/` folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        The user\_groups.yml is used manages access levels for the semantic model. It defines user groups that categorize users based on access privileges. Multiple groups can be created, with different users assigned to each, enabling controlled access to the model. By default, a 'default' user group is included in the YAML file, encompassing all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    Once the semantic model is prepared, create a `lens_deployment.yml` file parallel to the model folder.

    ```yaml
    version: v1alpha
    name: "redshiftlens"
    layer: user
    type: lens
    tags:
      - lens
    description: redshiftlens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: depot # source type is depot here
        name: redshiftdepot # name of the redshift depot
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/depot/redshift/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens
    ```

    Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

    * **Defining the Source:**

      * **`type`:**

        The

        `type`

        attribute in the

        `source`

        section must be explicitly set to

        `depot`

        .

      * **`name`:**

        The

        `name`

        attribute in the

        `source`

        section should specify the name of the AWS Redshift Depot created.

    * **Setting Up Compute and Secrets:**

      * Define the compute settings, such as which engine (e.g.,

        `runnable-default`

        ) will process the data.

      * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

    * **Defining Repository:**

      * **`url`**

        The

        `url`

        attribute in the repo section specifies the Git repository where the semantic model files are stored. For instance, if your repo name is lensTutorial then the repo

        `url`

        will be

        [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**

        The

        `lensBaseDir`

        attribute refers to the directory in the repository containing the semantic model. Example:

        `sample/lens/source/depot/awsredshift/model`

        .

      * **`secretId`:**

        The

        `secretId`

        attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**

        :  Specifies additional flags to control repository synchronization. Example:

        `--ref=dev`

        specifies that the semantic model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):**

      Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, Deploy the Lens using the following `apply` command:

    ```yaml
    dataos-ctl apply -f {manifest-file-path}
    ```
  </Accordion>

  <Accordion title="BigQuery" defaultOpen={false}>
    ## Step 1: Create the Bigquery Depot

    If the Depot is not active, create one using the provided template.

    ```yaml
    name: ${{bigquerydepot}}
    version: v2alpha
    type: depot
    tags:
      - ${{dropzone}}
      - ${{bigquery}}
    owner: ${{owner-name}}
    layer: user
    depot:
      type: BIGQUERY                 
          description: ${{description}} # optional
      external: ${{true}}
      secrets:
        - name: ${{bq-instance-secret-name}}-r
          allkeys: true

        - name: ${{bq-instance-secret-name}}-rw
          allkeys: true
      bigquery:  # optional                         
        project: ${{project-name}} # optional
        params: # optional
          ${{"key1": "value1"}}
          ${{"key2": "value2"}}

    ```

    ## Step 2: Prepare the semantic model folder

    In the model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the semantic model. Download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure, only the necessary columns are extracted, and the SQL dialect is specific to the bigquery. For instance,

        * &#x20;format table names as: `project_id.dataset.table`.

        * Use `STRING` for text data types instead of `VARCHAR`.

        * Replace generic functions with BigQuery’s `EXTRACT` function.

        For instance, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
          "onelakehouse"."retail".channel;
        ```

        Alternatively, one can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "onelakehouse"."retail".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For instance, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For instance, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. egments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create views" titleSize="h2">
        Create a `views/` folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        The user\_groups.yml is used manages access levels for the semantic model. It defines user groups that categorize users based on access privileges. Multiple groups can be created, with different users assigned to each, enabling controlled access to the model. By default, a 'default' user group is included in the YAML file, encompassing all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](/resources/lens/working_with_user_groups_and_data_policies/).

        ## Step 3:  Create Lens deployment manifest file

        After setting up the semantic model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

        ```yaml
        # RESOURCE META SECTION
        version: v1alpha # Lens manifest version (mandatory)
        name: "bigquery-lens" # Lens Resource name (mandatory)
        layer: user # DataOS Layer (optional)
        type: lens # Type of Resource (mandatory)
        tags: # Tags (optional)
          - lens
        description: bigquery depot lens deployment on lens2 # Lens Resource description (optional)

        # LENS-SPECIFIC SECTION
        lens:
          compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
          secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
            - name: bitbucket-cred # Referred Instance Secret name (mandatory)
              allKeys: true # All keys within the secret are required or not (optional)

          source: # Data Source configuration
            type: depot # Source type is depot here
            name: bigquerydepot # Name of the bigquery depot

          repo: # semantic model code repository configuration (mandatory)
            url: https://bitbucket.org/tmdc/sample # URL of repository containing the semantic model (mandatory)
            lensBaseDir: sample/lens/source/depot/bigquery/model # Relative path of the Lens 'model' directory in the repository (mandatory)
            syncFlags: # Additional flags used during synchronization, such as specific branch.
              - --ref=lens # Repository Branch
        ```

        Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

        * **Defining the Source:**

          * **Source type:** The `type` attribute in the `source` section must be explicitly set to `depot`.

          * **Source name:** The `name` attribute in the `source` section should specify the name of the Bigquery Depot created .

        * **Setting Up Compute and Secrets:**

          * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

          * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

        * **Defining Repository:**

          * **`url`** The `url` attribute in the repo section specifies the Git repository where the semantic model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

          * **`lensBaseDir`:** The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/bigquery/model`.

          * **`secretId`:** The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub) . It specifies the secret needed to securely authenticate and access the repository.

          * **`syncFlags`**: Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the semantic model rsides in the dev branch.

        * **Configuring API, Worker and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.
      </Step>
    </Steps>

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, deploy the Lens using the following `apply` command.

    ```
    dataos-ctl apply -f {manifest-file-path}}
    ```
  </Accordion>

  <Accordion title="Postgres" defaultOpen={false}>
    ## Step 1: Create Postgres Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: ${{postgresdb}}
    version: v2alpha
    type: depot
    layer: user
    depot:
      type: JDBC                  
      description: ${{To write data to postgresql database}}
      external: ${{true}}
      secrets:
        - name: ${{sf-instance-secret-name}}-r
          allkeys: true

        - name: ${{sf-instance-secret-name}}-rw
          allkeys: true
      postgresql:                        
        subprotocol: "postgresql"
        host: ${{host}}
        port: ${{port}}
        database: ${{postgres}}
        params: #Required 
          sslmode: ${{disable}}
    ```

    While creating Lens on Postgres Depot the following aspects need to be considered:

    * The SQL dialect used in the `model/sql` folder to load data from the Postgres source should be of the Postgres dialect.

    * The table naming in the `model/table` should be of the format: `schema.table`.

    ## Step 2: Prepare the semantic model folder

    In the model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
          "onelakehouse"."retail".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "onelakehouse"."retail".customer; #catalog_name
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create views" titleSize="h2">
        Create a `views/` folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        The user\_groups.yml is used manages access levels for the semantic model. It defines user groups that categorize users based on access privileges. Multiple groups can be created, with different users assigned to each, enabling controlled access to the model. By default, a 'default' user group is included in the YAML file, encompassing all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, deploy the Lens using the following `apply` command:

    ```
    dataos-ctl  apply -f ${manifest-file-path}
    ```
  </Accordion>

  <Accordion title="Snowflake" defaultOpen={false}>
    ## Prerequisite

    CLI Version should be `dataos-cli 2.26.39-dev` or greater.

    ## Step 1: Create the Snowflake Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: snowflake-depot
    version: v2alpha
    type: depot
    tags:
      - Snowflake depot
      - user data
    layer: user
    depot:
      name: sftest
      type: snowflake
      description: Depot to fetch data from Snowflake datasource
      secrets:
        - name: sftest-r
          keys:
            - sftest-r
          allKeys: true
        - name: sftest-rw
          keys:
            - sftest-rw
          allKeys: true
      external: true
      snowflake:
        database: TMDC_V1
        url: ABCD23-XYZ8932.snowflakecomputing.com
        warehouse: COMPUTE_WH
        account: ABCD23-XYZ8932
      source: sftest
    ```

    ## Step 2: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure the SQL dialect matches snowflake syntax. Format table names as `schema.table`.

        For example, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
         "retail".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "retail".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        #### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        #### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    After setting up the semantic model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

    ```yaml
    # RESOURCE META SECTION
    version: v1alpha # Lens manifest version (mandatory)
    name: "snowflake-lens" # Lens Resource name (mandatory)
    layer: user # DataOS Layer (optional)
    type: lens # Type of Resource (mandatory)
    tags: # Tags (optional)
      - lens
    description: snowflake depot lens deployment on lens2 # Lens Resource description (optional)

    # LENS-SPECIFIC SECTION
    lens:
      compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
      secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
        - name: bitbucket-cred # Referred Instance Secret name (mandatory)
          allKeys: true # All keys within the secret are required or not (optional)

      source: # Data Source configuration
        type: depot # Source type is depot here
        name: snowflake-depot # Name of the snowflake depot

      repo: # semantic model code repository configuration (mandatory)
        url: https://bitbucket.org/tmdc/sample # URL of repository containing the semantic model (mandatory)
        lensBaseDir: sample/lens/source/depot/snowflake/model # Relative path of the Lens 'model' directory in the repository (mandatory)
        syncFlags: # Additional flags used during synchronization, such as specific branch.
          - --ref=lens # Repository Branch
    ```

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings, deploy the Lens using the following `apply` command:

    ```
    dataos-ctl apply -f ${manifest-file-path}
    ```
  </Accordion>
</AccordionGroup>

### Multi-source

<AccordionGroup>
  <Accordion title="Minerva" defaultOpen={false}>
    ## Prerequisite

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the sematic model folder

    In the model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for access control
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
          "onelakehouse"."retail".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "onelakehouse"."retail".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create views" titleSize="h2">
        Create a `views/` folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        The user\_groups.yml is used manages access levels for the semantic model. It defines user groups that categorize users based on access privileges. Multiple groups can be created, with different users assigned to each, enabling controlled access to the model. By default, a 'default' user group is included in the YAML file, encompassing all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "minervalens"
    layer: user
    type: lens
    tags:
      - lens
    description: minerva deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: minerva #minerva/themis/depot
        name: minervacluster  #name of minerva cluster
        catalog: onelakehouse
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/minerva/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens
    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `onelakehouse`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `minerva`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Minerva Cluster. For example, if the name of your Minerva Cluster is miniature the Source name would be `miniature`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Minerva Cluster that you intend to use. For instance, if the catalog is named onelakehouse, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the semantic model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the semantic model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    <Info>
      Within the Themis and Minerva cluster, all Depots (such as onelakehouse, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one Depot in the \`catalog\` field, as Lens can connect to and utilize Depots from all sources available in the Themis cluster.
    </Info>
  </Accordion>

  <Accordion title="Themis" defaultOpen={false}>
    ## Prerequisites

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for governance
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look as follows:

        ```sql
        SELECT
          *
        FROM
          "onelakehouse"."retail".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "onelakehouse"."retail".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        The user\_groups.yml is used manages access levels for the semantic model. It defines user groups that categorize users based on access privileges. Multiple groups can be created, with different users assigned to each, enabling controlled access to the model. By default, a 'default' user group is included in the YAML file, encompassing all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the user groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "themis-lens"
    layer: user
    type: lens
    tags:
      - lens
    description: themis lens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: themis #minerva/themis/depot
        name: lenstestingthemis
        catalog: onelakehouse
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/themis/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=main #repo-name

    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `onelakehouse`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the Source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `themis`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Themis Cluster. For example, if the name of your Themis Cluster is `clthemis` the Source name would be `clthemis`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Themis Cluster that you intend to use. For instance, if the catalog is named `onelakehouse`, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the semantic model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the semantic model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    The above manifest is intended for a cluster named `lenstestingthemis`, created on the themis source, with the Depot or data catalog named `onelakehouse`. To use this manifest, copy the file and update the source details accordingly.

    <Info>
      Within the Themis and Minerva cluster, all Depots (such as Lakehouse, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one Depot in the \`catalog\` field, as Lens can connect to and utilize Depots from all sources available in the Themis cluster.
    </Info>
  </Accordion>
</AccordionGroup>

## Query Acceleration

<Accordion title="Flash" defaultOpen={false}>
  [Flash](/resources/stacks/flash/) is designed to optimize query performance by leveraging in-memory execution. When used with DataOS Lakehouse and Iceberg-format Depots, it enables efficient handling of large-scale queries by reducing latency and optimizing resource usage. The following explains how to configure Lens using Flash.

  ### **Prerequisites**

  To create a Lens using Flash, ensure that the Flash service is running and has a Persistent Volume attached. A Persistent Volume allows Flash to spill data to disk, enhancing performance by preventing memory constraints from impacting query execution.

  <Info>
    Make sure a Persistent Volume is attached with the Flash. It helps Flash to spill data over disk, which helps with the performance.
  </Info>

  <Steps>
    <Step title="Create Persistent Volume manifest file" titleSize="h2">
      To create a Persistence Volume manifest file copy  the template below and replace with your desired resource name and with the appropriate volume size (e.g., `100Gi`, `20Gi`, etc.), according to your available storage capacity. For the accessMode, you can choose ReadWriteOnce (RWO) for exclusive read-write access by a single node, or ReadOnlyMany (ROX) if the volume needs to be mounted as read-only by multiple nodes.

      ```yaml
      name: <name>  # Name of the Resource
      version: v1beta  # Manifest version of the Resource
      type: volume  # Type of Resource
      tags:  # Tags for categorizing the Resource
        - volume
      description: Common attributes applicable to all DataOS Resources
      layer: user
      volume:
        size: <size>  # Example: 100Gi, 50Mi, 10Ti, 500Mi, 20Gi
        accessMode: <accessMode>  # Example: ReadWriteOnce, ReadOnlyMany
        type: temp
      ```

      <Note>
        The persistent volume size should be at least 2.5 times the total dataset size, rounded to the nearest multiple of 10. To check the dataset size, use the following query:

        `SELECT sum(total_size) FROM "<catalog>"."<schema>"."<table>$partitions";`

        The resultant size will be in the bytes.
      </Note>
    </Step>

    <Step title="Apply the Volume manifest file" titleSize="h2">
      Deploy the persistent volume using the following `apply` command in terminal:

      ```bash
      dataos-ctl apply -f <file path of persistent volume>

      ```

      [](https://dataos.info/resources/lens/data_sources/flash/#__codelineno-2-1)

      Once deployed the Persistent Volume will be available for use by Flash Service.
    </Step>

    <Step title="Create Service manifest file" titleSize="h2">
      Ensure the name of the Persistent Volume you created is referenced correctly in the name attribute of the persistentVolume section. The name used here should match exactly with the name you assigned to the Persistent Volume during its creation.

      ```yaml
      name: flash-service-lens
      version: v1
      type: service
      tags:
        - service
      description: flash service
      workspace: curriculum
      service:
        servicePort: 5433
        replicas: 1
        logLevel: info

        compute: runnable-default

        resources:
          requests:
            cpu: 1000m
            memory: 1024Mi

        persistentVolume:
          name: <persistent_volume_name>
          directory: p_volume

        stack: flash+python:2.0

        stackSpec:
          datasets:
            - address: dataos://onelakehouse:sales360/f_sales    #view
              name: sales

            - address: dataos://onelakehouse:sales360/customer_data_master
              name: customer_data_master

            - address: dataos://onelakehouse:sales360/site_check1
              name: site_check1

            - address: dataos://onelakehouse:sales360/product_data_master
              name: product_data_master

          init:
            - create table if not exists f_sales as (select * from sales)  #table
            - create table if not exists m_customers as (select * from customer_data_master)
            - create table if not exists m_sites as (select * from site_check1)
            - create table if not exists table m_products as (select * from product_data_master)
      ```

      ### **How does this process work?**

      The flow of Flash operates as follows:

      * **Data loading:** The `datasets` attribute specifies the depot `address` of the source data to be loaded into Flash. A dataset `name` is  provided, which Flash uses to generate a view of the source data.

      * **View creation:** Flash creates a view based on the assigned name, allowing for interaction with the source data without directly querying it.

      * **Table creation:** The `init` attribute allows specific columns from the generated view to be selected and defined as tables. These tables are available for use in SQL queries within Lens. Tables are created only **if they don’t already exist**, using the `IF NOT EXISTS` clause. If tables are already present, the process **skips creation** and proceeds with the existing schema.

      * **Usage in Lens model(SQL):**  For example, in the manifest referenced, the `f_sales` table is first loaded from the source, and a view named `sales` is created. A table called `f_sales` is then defined using this sales view. This table is then referenced in SQL models within Lens.
    </Step>

    <Step title="Apply the Flash manifest file" titleSize="h2">
      Deploy the Flash Service, using the following `apply` command:

      ```yaml
      dataos-ctl apply -f <file path of flash service>
      ```

      This will deploy the  Flash Service ready to be used as source in Lens.
    </Step>
  </Steps>

  ## Step 2 Prepare the semantic model folder

  In the model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

  [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

  <Steps>
    <Step title="Load data from the data source" titleSize="h2">
      In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source. For Flash, the table name given in the  `init` will be the source table name.

      For example, a simple data load might look as follows:

      ```sql
      SELECT
        *
      FROM
        channel; # flash source table name
      ```

      Alternatively, you can write more advanced queries that include transformations, such as:

      ```sql
      SELECT
        CAST(customer_id AS VARCHAR) AS customer_id,
        first_name,
        CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
        age,
        CAST(register_date AS TIMESTAMP) AS register_date,
        occupation,
        annual_income,
        city,
        state,
        country,
        zip_code
      FROM
        customer;
      ```
    </Step>

    <Step title="Define the table in the model" titleSize="h2">
      Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

      ```yaml
      table:
        - name: customers
          sql: {{ load_sql('customers') }}
          description: Table containing information about sales transactions.
      ```

      ### Add dimensions and measures

      After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

      ```yaml
      tables:
        - name: sales
          sql: {{ load_sql('sales') }}
          description: Table containing sales records with order details.

          dimensions:
            - name: order_id
              type: number
              description: Unique identifier for each order.
              sql: order_id
              primary_key: true
              public: true

          measures:
            - name: total_orders_count
              type: count
              sql: id
              description: Total number of orders.
      ```

      ### Add segments to filter

      Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

      ```yaml
      segments:
        - name: state_filter
          sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
      ```

      To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
    </Step>

    <Step title="Create the views" titleSize="h2">
      Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

      ```yaml
      views:
        - name: customer_churn_prediction
          description: Contains customer churn information.
          tables:
            - join_path: marketing_campaign
              includes:
                - engagement_score
                - customer_id
            - join_path: customer
              includes:
                - country
                - customer_segments
      ```

      To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
    </Step>

    <Step title="Create user groups" titleSize="h2">
      The `user_groups.yml` manifest file is used to manage access levels for the semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, the 'default' user group in the manifest file includes all users.

      ```yaml
      user_groups:
        - name: default
          description: this is a default user group
          includes: "*"
      ```

      You can create multiple user groups in `user_groups.yml` . To know more about the user groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/).
    </Step>
  </Steps>

  ## Step 3 Create Instance-secret&#x20;

  Before creating the `lens_deployment.yml` file, ensure that an **Instance Secret** is deployed with the credentials required to access a hosted code repository, such as [Bitbucket](https://support.atlassian.com/bitbucket-cloud/docs/push-code-to-bitbucket/), [GitHub](https://docs.github.com/en/migrations/importing-source-code/using-the-command-line-to-import-source-code/adding-locally-hosted-code-to-github), [AWS CodeCommit](https://docs.aws.amazon.com/codecommit/latest/userguide/getting-started.html) etc. The `lens_deployment.yml` configuration relies on this repository to fetch the **semantic model**, making the Instance Secret a critical prerequisite for deployment. 

  <Note>
     If your code repository is private, you will need to create **Instance Secrets** with your repository credentials for later use during deployment. Public code repositories do not require Step 3.
  </Note>

  Define the Instance Secret Resource in a  YAML manifest file. Below is a template you can use for Bitbucket, substituting `${USERNAME}` and `${PASSWORD}` with your actual Bitbucket credentials:

  <CodeGroup>
    ```yaml Syntax
    # RESOURCE META SECTION
    name: ${bitbucket-r } # Secret Resource name (mandatory)
    version: v1 # Secret manifest version (mandatory)
    type: instance-secret # Type of Resource (mandatory)
    description: Bitbucket read secrets for code repository # Secret Resource description (optional)
    layer: user # DataOS Layer (optional)

    # INSTANCE SECRET-SPECIFIC SECTION
    instance-secret: 
      type: key-value # Type of Instance-secret (mandatory)
      acl: r # Access control list (mandatory)
      data: # Data (mandatory)
        GITSYNC_USERNAME: ${USERNAME}
        GITSYNC_PASSWORD: ${PASSWORD}
    ```

    ```yaml Example
    # RESOURCE META SECTION
    name: bitbucket-r # Secret Resource name (mandatory)
    version: v1 # Secret manifest version (mandatory)
    type: instance-secret # Type of Resource (mandatory)
    description: Bitbucket read secrets for code repository # Secret Resource description (optional)
    layer: user # DataOS Layer (optional)

    # INSTANCE SECRET-SPECIFIC SECTION
    instance-secret: 
      type: key-value # Type of Instance-secret (mandatory)
      acl: r # Access control list (mandatory)
      data: # Data (mandatory)
        GITSYNC_USERNAME: iamgroot
        GITSYNC_PASSWORD: <git_token>
    ```
  </CodeGroup>

  ### **Apply the Instance Secret manifest**

  Deploy the Instance Secret to DataOS using the `apply` command.

  ```
  dataos-ctl apply -f {manifest-file-path}
  ```

  <Warning>
     When applying the manifest file for Instance-secret from CLI, make sure you don't specify Workspace as Instance Secrets are [Instance-level Resource](/resources/types/#instance-level-resources).
  </Warning>

  ## Step 4 Create Lens Resource

  To enable Lens to interact with the Flash service, configure the following attributes in the Lens deployment manifest file:

  ### **1. Define Flash as the data source**

  Configure the Flash service as the data source in the `source` attribute of the deployment manifest file.

  **`source`**

  * **`type`** The source section specifies that Flash is used as the data source (`type: flash`). This indicates that data for the semantic model will be loaded from the Flash service.

  * **`name`**: The Flash service is identified by the `name` attribute, here it is flash-service-lens. This name should match the deployed Flash service used for data ingestion. Below is an example configuration.

  ```yaml
  source:
    type: flash  # Specifies the data source type as Flash
    name: flash-test  # Name of the Flash service
  ```

  ### **2. Add environment variables**

  Specify the following environment variables in the Worker, API, and Router sections of the Lens deployment manifest.

  **`envs`**

  The following environment variables are defined under multiple components, including api, worker, router, and iris

  * **`LENS2_SOURCE_WORKSPACE_NAME`** It refers to the workspace where the Flash service is deployed.

  * **`LENS2_SOURCE_FLASH_PORT`** The port number `5433` is specified for the Flash service. This port is used by Lens to establish communication with the Flash service. It ensures that all components—API, worker, router, and iris—can access the Flash service consistently.

  ```yaml
  envs:
    LENS2_SOURCE_WORKSPACE_NAME: public
    LENS2_SOURCE_FLASH_PORT: 5433
  ```

  After configuring with above attributes, the Lens deployment manifest file will be as follows:

  ```yaml
  version: v1alpha
  name: "lens-flash-test-99"
  layer: user
  type: lens
  tags:
    - lens
  description: A sample lens that contains three entities, a view and a few measures for users to test
  lens:
    compute: runnable-default
    secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
      - name: githubr # Referred Instance Secret name (mandatory)
        allKeys: true # All keys within the secret are required or not (optional)
    source:
      type: flash # minerva, themis and depot
      name: flash-service-lens # flash service name
    repo:
      url: https://github.com/tmdc/sample    # repo address
      lensBaseDir: sample/source/flash/model     # location where lens models are kept in the repo
      syncFlags:
        - --ref=main
    api:
      replicas: 1
      logLevel: debug
      envs:
        LENS2_SOURCE_WORKSPACE_NAME: curriculum
        LENS2_SOURCE_FLASH_PORT: 5433
      resources: # optional
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 2000m
          memory: 2048Mi

    worker:
      replicas: 1
      logLevel: debug
      envs:
        LENS2_SOURCE_WORKSPACE_NAME: curriculum
        LENS2_SOURCE_FLASH_PORT: 5433
      resources: # optional
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 6000m
          memory: 6048Mi

    router:
      logLevel: info
      envs:
        LENS2_SOURCE_WORKSPACE_NAME: curriculum
        LENS2_SOURCE_FLASH_PORT: 5433
      resources: # optional
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 6000m
          memory: 6048Mi

    iris:
      logLevel: info  
      envs:
        LENS2_SOURCE_WORKSPACE_NAME: curriculum
        LENS2_SOURCE_FLASH_PORT: 5433
      resources: # optional
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 6000m
          memory: 6048Mi
  ```

  ### Apply the Lens manifest file

  Deploy the Lens by using the following `apply` command:

  ```
  dataos-ctl apply -f {manifest-file-path}
  ```
</Accordion>

<Info>
  🗣️ If working with Lens 1.0 interface, click [here](/interfaces/lens/).
</Info>

## How to consume the semantic model?

After creating a Lens semantic model , the next step is to consume it—this means interacting with the model by running queries. The following section explains the key concepts for querying Lens through various methods, though all queries follow the same general format. Multiple ways are available to explore or interact with the semantic model or its underlying data, allowing to ask meaningful questions of the data and retrieve valuable insights. Exploration can be performed using the following methods:

### BI Tools

<AccordionGroup>
  <Accordion title="Power BI" defaultOpen={false}>
    <Tabs>
      <Tab title="Using CLI">
        ## Prerequisites

        Before proceeding with the integration of Lens with Power BI, ensure that you have the following prerequisites in place:

        * **CURL**: Ensure that CURL is installed on your system. Windows users may need to use `curl.exe`.

        * **Version compatibility:** It is strongly recommended to download Power BI Version `2.132.908.0` or later to fully utilize `.pbip` files.&#x20;

        * **Lens API Endpoint**: The API endpoint provided by Lens to synchronize the semantic model, enabling integration with Power BI.

        * **DataOS API key**: You will need your apikey  for authentication. The API key can be obtained using the following command in your terminal:

        ### Curl command

        Use the following CURL command to synchronize the semantic model with Power BI:

        <CodeGroup>
          ```javascript Command
          curl --location --request POST 'https://<DATAOS_FQDN>/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' \
            --header 'apikey: ${APIKEY}' \
            --output ${FILE_NAME}.zip
          ```

          ```
          curl --location --request POST 'https://</lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' \
            --header 'apikey: ${APIKEY}' \
            --output ${FILE_NAME}.zip
          ```

          ```
          console.log("Hello World");
          ```
        </CodeGroup>

        ## Parameters

        | Parameter      | Description                                                                                                               |
        | -------------- | ------------------------------------------------------------------------------------------------------------------------- |
        | `URL`          | The URL  for syncing Lens with Power BI. It includes:                                                                     |
        | `apikey`       | User's API key for the current context in Lens.                                                                           |
        | `filename.zip` | Here you give the name of the Power BI file with which name you wish to download it. For example, `product-analysis.zip`. |

        The `file.zip` includes essential components for syncing a semantic model with Power BI, organized into folders such as `.Report` and `.SemanticModel`:

        ## Steps

        To begin syncing a semantic model, the following steps should be followed:

        <Steps>
          <Step title="Open the terminal" titleSize="h2">
            Open the terminal and paste the following command:

            ```
            curl --location --request POST 'https://tcp.{context}.dataos.app/lens2/sync/api/v1/power-bi/{workspace_name}:{lens_name}' --header 'apikey: abcdefgh==' --output {file.zip}
            ```

            Replace the placeholders with your current context, workspace, lens name and the apikey.
          </Step>

          <Step title=" Extract the zip file" titleSize="h2">
            Once the command is executed, a zip file will be downloaded to the specified directory. The downloaded file should be unzipped. Three folders will be found inside, all of which are necessary for semantic synchronization with Power BI.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi2.png)

            <Note>
              Ensure that `.pbip` folders are fully extracted before opening them.Failure to do so may result in missing file errors, as shown below:
              ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image.png)
            </Note>
          </Step>

          <Step title="Open the `.pbip` file" titleSize="h2">
            Click on the file with `.pbip` extension to open  the Power BI file in the Power BI Desktop.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi3.png)
          </Step>

          <Step title="Enter credentials" titleSize="h2">
            After opening the file, a popup will prompt for credentials. The DataOS username and API key should be entered.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi4.png)
          </Step>

          <Step title=" Connect to DataOS" titleSize="h2">
            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi6.png)
            Click on the connect button. A popup will appear. Click Ok.
          </Step>

          <Step title="Create dashboards" titleSize="h2">
            Upon successful connection, tables and views will be accessible, displaying dimensions and measures. Now you are ready to create dashboards.
          </Step>
        </Steps>

        ## Important considerations

        * **Measure naming convention:** In Power BI, measures typically have an 'm\_' prefix to indicate they represent a measure. For example, a measure calculating total revenue might be named m\_total\_revenue.

        * **Live connection:** The connection is live, meaning any changes to the underlying data will be reflected in Power BI.

        * **Schema changes:** If schema changes occur, such as the addition of new dimensions and measures, the steps outlined above will need to be repeated.

        * **Row Limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        * **Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        ## Data governance and access control

        Data masking, restrictions, or permissions established by the publisher are automatically enforced for all report viewers, ensuring consistent data security and compliance. The behavior of these data policies, such as masking, may vary based on the user of the Power BI desktop.
      </Tab>

      <Tab title="Using Data Product Hub">
        <Steps>
          <Step title="Navigate to the Data Product Hub" titleSize="h2">
            Access the Home Page of DataOS. From there, navigate to the Data Product Hub to explore the various Data Products available within the platform.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(1\).png)
          </Step>

          <Step title="Browse and select a Data Product" titleSize="h2">
            Browse the list of Data Products and select a specific Data Product to initiate integration with Power BI. For example, selecting 'Sales360' allows detailed exploration and integration of the Sales360 Data Product.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(2\).png)
          </Step>

          <Step title="Access integration options" titleSize="h2">
            Go to the BI Sync option under the Access Options tab. Scroll down to locate the Excel and Power BI section, and click the Download .pbip File button to download the ZIP folder.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/powerbi/Powerbi3.png)
          </Step>

          <Step title="Download and open the ZIP file" titleSize="h2">
            Access the downloaded ZIP file on the local system and extract its contents to the specified directory. The extracted folder will contain three files. Ensure all three files remain in the same directory to maintain semantic synchronization of the Data Product.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(15\).png)

            The folder contains the main components of a Power BI project for syncing the semantic model (here `sales360`) including folders such as the `.Report` and `.SemanticModel`. Following is the brief description of each:

            * **public\_sales360.Report:** This folder contains `definition.pbir` file related to the report definition in Power BI. These files define the visual representation of data, such as tables and charts, without storing actual data. They connect the semantic model and data sources to create the report views.

            * **public-sales360.SemanticModel:** This folder contains files that define the underlying semantic model for your Power BI project. The semantic model plays a crucial role in managing how Power BI interacts with data, setting up relationships, hierarchies, and measures.

              * **definition.bism:** This file represents the Business Intelligence Semantic Model (BISM). It defines the structure of your data, including data sources, relationships, tables, and measures for your semantic model. The `.bism` file holds essential metadata that helps Power BI understand and query the data, forming the core of the semantic model for report creation and analysis.

              * **model.bim:** Power BI uses the `.bim` file to generate queries and manage interactions with the dataset. When you build reports or dashboards in Power BI, it references this semantic model to ensure the correct structure is applied to the data.

            * **public-sales-360-table.pbip:** This file serves as a Power BI project template or configuration file. Power BI uses files `.pbip` or `.pbix` to encapsulate reports, datasets, and visualizations. The `.pbip` file ties together the semantic model and report definitions from the other folders, acting as the entry point for working on the project in Power BI Desktop or the Power BI service.
          </Step>

          <Step title="Enter credentials" titleSize="h2">
            Open the `public_sales360` file in Power BI, once the file is opened, a popup will appear prompting for the DataOS username and API key.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(16\).png)
            Click the connect button. A popup will appear; click OK.
          </Step>

          <Step title="View data in Power BI" titleSize="h2">
            After connecting, users can see tables and views containing dimensions and measures and create dashboards.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(19\).png)
          </Step>
        </Steps>

        ## Important considerations

        * **Measure naming convention:** In Power BI, measures typically have an 'm\_' prefix to indicate they represent a measure. For example, a measure calculating total revenue might be named m\_total\_revenue.

        * **Live connection:** The connection is live, meaning any changes to the underlying data will be reflected in Power BI.

        * **Schema changes:** If schema changes occur, such as the addition of new dimensions and measures, the steps outlined above will need to be repeated.

        * **Row Limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        * **Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        ## Data governance and access control

        Data masking, restrictions, or permissions established by the publisher are automatically enforced for all report viewers, ensuring consistent data security and compliance. The behavior of these data policies, such as masking, may vary based on the user of the Power BI desktop.
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Tableau" defaultOpen={false}>
    <Tabs>
      <Tab title="Using CLI">
        ### **Prerequisites**

        * **Curl**: Ensure that `curl` is installed on the system. For Windows systems, `curl.exe` may be necessary.

        * **Lens API endpoint**: The API endpoint provided by Lens to sync semantic model, enabling integration with Tableau.

        * **Access credentials**: Access credentials such as username, password, project name etc., are required for Tableau.

        * **DataOS API key**: Ensure the DataOS API key is available. Get it by using the following command:

        ```
        dataos-ctl user apikey get
        ```

        ## Steps

        To sync the semantic model with Tableau, follow the steps below:

        <Steps>
          <Step title="Run the curl command" titleSize="h2">
            Open the terminal and run the following command. Make sure to replace the placeholders with the actual credentials.

            <CodeGroup>
              ```yaml Command
              curl --location --request POST 'http://tcp.<DATAOS_FQDN>/lens2/sync/api/v1/tableau-cloud/<WORKSPACE_NAME>:<LENS_NAME>' \
              --header 'apikey: <APIKEY>' \
              --header 'Content-Type: application/json' \
              --data-raw '{
                  "project_name": "<SAMPLE>",
                  "username": "{USER_NAME/EMAIL}",
                  "password": "<PASSWORD>",
                  "site_id": "<SITE_ID>",
                  "server_address": "https://prod-apnortheast-a.online.tableau.com"
              }'
              ```

              ```yaml Example
              curl --location --request POST 'http://tcp.alpha-omega.dataos.app/lens2/sync/api/v1/tableau-cloud/curriculum:dph-insights' \
              --header 'apikey: abcdefghiJ==' \
              --header 'Content-Type: application/json' \
              --data-raw '{
                  "project_name": "Training",
                  "username": "labs@tmdc.io",
                  "password": "******",
                  "site_id": "iamgroot1086a891fef336",
                  "server_address": "https://prod-apnortheast-a.online.tableau.com"
              }' 
              ```
            </CodeGroup>

            **URL:** This URL is used to sync a specific semantic model to Tableau for public access. Here,

            1. `<DATAOS_FQDN>`: Replace with the Fully Qualified Domain Name (FQDN) where Lens is hosted. Example: `liberal-monkey.dataos.app`.

            2. `<WORKSPACE_NAME>` : Replace with the name of the workspace where the semantic model (Lens) is deployed. e.g., `public`, `curriculum`.

            3. `<LENS_NAME>`:Replace with the name of the semantic model to sync. e.g., `sales360`.

            **Headers:**

            1. **`apikey`:** User's API key for the current context in Lens. The DataOS API key for the user can be obtained by executing the below command:

            ```
            dataos-ctl user apikey get
            ```

            1. **`Content-Type application/json`:** Specifies that the data being sent is in JSON format.

            **Raw data payload:** This section defines the details of the user's Tableau credentials and project configuration:

            1. **project\_name:** The name of the Tableau project where the data will be synced.  If the project does not already exist, Tableau will create a new project with the given name.

            2. **username:** Tableau account username, typically the email ID used to log in to Tableau.

            3. **password:** Tableau account password.

            4. **site\_id:** The site ID associated with the current Tableau connection.

            5. **server\_address:** The URL of the Tableau server. Replace with the correct server address (e.g., https://prod-apnortheast-a.online.tableau.com). This information can be obtained upon logging in to Tableau. The URL will appear as follows:> https://prod-apnortheast-a.online.tableau.com/#/site/iamgroot1086a891fef336/homeHere: **iamgroot1086a891fef336** is the **site\_id**.

            As you run the command the sync process starts. To view the semantic model in the &#x20;
          </Step>

          <Step title="Login to Tableau" titleSize="h2">
            Login to your Tableau account and navigate to  'Explore' tab on the left side of the Tableau homepage. The required tables and views will be visible in the project. Here you will see three sources: one for tables and two for views.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau1.png)
          </Step>

          <Step title="Create new workbook" titleSize="h2">
             Navigate to the Home tab on the left side and click on 'New'. Under this option, select 'Workbook' as shown in the below image.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau2.png)
          </Step>

          <Step title="Connect to data" titleSize="h2">
            After selecting 'Workbook', the 'Connect to Data' page will appear. Choose the desired views or tables for exploration or consumption, then click 'Connect' to proceed.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau3.png)

            Upon clicking 'Connect', a prompt will request the username and password. Enter the DataOS username and API key.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau4.png)

            The username will be your userid in the DataOS which you can get by typing this command in your terminal:

            ```
            dataos-ctl user get
            #Expected Output
            |    NAME    │    ID    │  TYPE  │          EMAIL          │          TAGS           |
            |------------│---------│--------│-------------------------│-------------------------|
            | iamgroot  │ iamgroot │ person │ iamgroot@tmdc.io       │ roles:id:user,          |
            |            │         │        │                         │ users:id:iamgroot       |
            ```

            To get the apikey, copy the following command in your terminal:

            ```bash
            dataos-ctl user apikey get

            #Expected_Output
            INFO[0000] 🔑 user apikey get...                         
            INFO[0001] 🔑 user apikey get...complete                 

                                                               TOKEN                                                   │  TYPE  │        EXPIRATION         │                   NAME                    
            ───────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────┼───────────────────────────┼───────────────────────────────────────────
              hLTQwNDktYmU2MS01ZTBlM2U1ZmNhYWE=     │ apikey │ 2025-02-09T05:30:00+05:30 │ token_allegedly_factually_popular_cod     
              jMWFhLTQ1MGYtOGY0Zi0wNDhiNDY4MmY0ODk= │ apikey │ 2025-02-08T05:30:00+05:30 │ token_actively_intensely_humble_honeybee  
              0MmU3LWE3OWQtZGQ5Yzc2MTNlNzg5         │ apikey │ 2025-02-07T05:30:00+05:30 │ token_mildly_safely_pleasing_shiner       
              RG9jdW1lbtOTU4NC04NzIxZjlhODY1NTY=                                     │ apikey │ 2025-01-28T13:30:00+05:30 │ Documentation                             


            ```
          </Step>

          <Step title="Create dashboards" titleSize="h2">
            After entering the credentials and clicking on 'Sign In', the model will be ready for visualization purposes.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau5.png)

            <Info>
              The publisher can embed their credentials (DataOS username and API Token) or ask users to provide credentials whenever they want to access the published Workbook/Sheet/Dashboard. If the publisher has chosen to ‘Embed password for data source’, users can access the published workbook and dashboard without providing credentials. Once the credentials are embedded, they cannot be accessed. You need to overwrite and ‘publish-as’ the workbook to reconfigure the embedding password optionality.
            </Info>
          </Step>
        </Steps>

        ## Supported data types

        | Category       | Data Type               | Support Status             | Recommended Approach                                                                                                                                                                                                                                                                                                              |
        | -------------- | ----------------------- | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
        | Dimension      | `time`                  | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `string`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `number`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `boolean`               | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `max`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `min`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `number`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `sum`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `count`                 | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `boolean`               | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `string`                | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `time`                  | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `avg`                   | Not Supported              | Option 1: To use measure of type ‘avg’, define an additional measure of type 'count' in that entity: `name: count`, `type: count`, `sql: '1'`. Option 2: Use measure of type 'number' and define average logic in SQL: `measures: - name: total_accounts type: number sql: "avg({accounts})”`                                     |
        | Measure        | `count_distinct`        | Not Supported              | Option 1: To use measure of type ‘count\_distinct’, define an additional measure of type 'count' in that entity: `name: count`, `type: count`, `sql: '1'`. Option 2: Use measure of type 'number' and define logic for count\_distinct in SQL: `measures: - name: total_accounts type: number sql: "count(distinct({accounts}))”` |
        | Measure        | `count_distinct_approx` | Not Supported              | NA                                                                                                                                                                                                                                                                                                                                |
        | Rolling Window | -                       | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |

        ## Important considerations for Tableau integration

        **1. Handling Entities without Relationships:** An error will occur during synchronization if any entity in the semantic model lacks a defined relationship. To resolve this issue, the entity can be hidden to avoid synchronization errors.

        **2. Live connection:** The connection between the semantic layer and Tableau Cloud is live meaning that any changes to the underlying data will automatically be reflected in Tableau.

        **3. Schema changes:** If there are schema updates, such as adding new dimensions or measures, the integration steps will need to be repeated to incorporate these changes into Tableau.

        **4. Avoiding cyclic dependencies:** Tableau does not support cyclic dependencies within data models. To prevent integration issues, it is essential to ensure that the semantic model is free of cyclic dependencies prior to syncing with Tableau.

        **5. Visualization with multiple data sources:** You cannot build a visualization that incorporates data from multiple data sources. For live connections, Tableau does not support data blending. Only a single data source can be used to create a visualization.

        **6. Centralized management:** All data sources should be managed and published by the admin on the server, with everyone else using this source.

        **7. Single authority for Desktop publications:** If data sources are published via Tableau Desktop, ensure that all sources are published by a single authority to avoid multiple data source conflicts on the server.

        **8. Row limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        **9. Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        **10. Parameter Action:** Action filters can be defined on measures/dimensions to filter visualizations effectively.

        **11. Default chart types:** All default chart types provided by Tableau can be plotted and visualized without issues.

        **12. Rolling Window Measure:** For querying a rolling window measure, it is necessary to provide a time dimension and apply a date range filter to this time dimension. When querying a rolling window measure, follow these steps:

        * Select the rolling window measure.

        * Select the time dimension.

        * To define granularity, right-click on the selected time dimension and set granularity (choose a granularity where the complete time, along with the year, is shown).

        * Add the time dimension to the filter, and define the range filter.

        <Info>
          Be aware that custom calculations or fields (measures/dimensions) created in BI tools may be lost during re-sync. It is preferable to create custom logic directly in Tableau's Lens.
        </Info>

        ## Error handling

        **Scenario 1: Handling syntactical errors in measures or dimensions**

        If a measure or dimension contains a syntactical error (and is also not functioning in Explore studio of Data Product Hub), the following error will appear when attempting to select it:

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image02.png)

        After correcting the syntactical error in the measure or dimension within Lens, the error will no longer appear. To reflect the changes in Tableau, refreshing the data source and re-selecting the measure or dimension will be necessary to display it in the chart.

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image03.png)

        **Scenario 2: Handling inactive Lens in the environment**

        If the Lens is not active in the environment while working on an existing workbook in Tableau or when attempting to establish a new connection, an error will be encountered. This may prevent access to or querying data from the Lens. Verify that the Lens exists and is active before syncing.

        **Scenario 3: Handling data source errors due to access restrictions**

        If the Account table is set to `public = false`, a data source error will occur in Tableau. The error message will indicate that the 'Account table not found,' which will prevent querying or using data from that table.

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image06.png)

        To resolve this issue, ensure the Account table is accessible (set to `public = true` or assign appropriate permissions) and then resync the Lens in Tableau to regain access.

        ## Governance of model on Tableau Cloud

        When the semantic model is activated via BI Sync in Tableau, data masking, restrictions, and permissions set by the publisher are automatically applied, ensuring consistent data security and compliance. The behavior of these policies (e.g., masking) may vary based on the Tableau user.

        The Tableau management process involves authentication and authorization using the DataOS user ID and API key when accessing synced data models. This ensures that columns redacted by Lens data policies are restricted based on the user's group permissions.

        For example, if a user named **iamgroot** in the **Analyst** group is restricted from viewing the 'Annual Salary' column, this column will not be visible in either the Data Product exploration page or Tableau after syncing. Tableau Cloud requires the DataOS user ID and API key for authentication, ensuring that users can access the full model, except for any columns restricted by any data policies.

        This approach maintains security and guarantees that users only see the data they are authorized to view.
      </Tab>

      <Tab title="Using Data Product Hub ">
        <Steps>
          <Step title=" Navigate to the Data Product Hub" titleSize="h2">
            Access the Home Page of DataOS. From the home page, navigate to the Data Product Hub to explore the various Data Products available.

            ![](/image.png)
          </Step>

          <Step title="Browse and select a Data Product" titleSize="h2">
            Browse through the list of available Data Products. Select a specific Data Product to integrate with Tableau. For instance, **Product 360** can be chosen to explore the Data Product on Tableau for data visualisation and getting insights.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau0.png)
          </Step>

          <Step title="Navigate to the Access Options tab" titleSize="h2">
            After selecting a Data Product, navigate to the **BI Sync** option in the **Access Options** tab. Scroll through the BI Sync and locate the **Tableau Cloud** option. Now, Click on the **Add Connection** button

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau1.png)
          </Step>

          <Step title="Enter connection details and click Activate button" titleSize="h2">
            A connection window will open, prompting the entry of the necessary connection details. There are following two ways to pass the connection details:

            <Tabs>
              <Tab title="Using  username and password">
                Following are the connection details of the Tableau username and password:
              </Tab>

              <Tab title="Using Personal Access Token">
                In addition to using standard Tableau credentials, users can also opt to use Personal Access Tokens (PAT) for authentication.&#x20;

                ## Prerequisites

                **Tableau Personal Access Token (PAT):** Before integrating the semantic model with Tableau using a PAT, ensure that you generate a PAT in Tableau by following the instructions provided in [this guide](https://help.tableau.com/current/online/en-us/security_personal_access_tokens.htm).
              </Tab>
            </Tabs>

            * **Project Name**: Enter the Tableau project name. If a project with that name does not already exist, it will be created automatically. If you do not specify a name, the project will default to the name of the Data Product, which will register all associated data sources under that project.

            For optimal organization within Tableau, we recommend providing a custom project name it facilitates easier navigation of your data sources.

            * **Server Name**: The address of the Tableau server (e.g., `https://prod-apnortheast-a.online.tableau.com`).

            * **Site ID**: The site ID (e.g., `tableausuer@123`).

            * **Username**: The Tableau username.(e.g., `labs@tmdc.io`)

            * **Password**: The password associated with the Tableau account.

            These details can be obtained upon logging into Tableau. The URL format will appear as follows:

            [`https://prod-apnortheast-a.online.tableau.com/#/site/site_id`](https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123)

            **Sample URL**:

            [`https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123/home`](https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123/home)

            In this example, `tableuuser@123` represents the **site\_id**.

            After entering the required credentials, click the Activate button to establish the connection. A confirmation message will appear upon successful connection.
          </Step>
        </Steps>

        ## Exploring the semantic model on Tableau&#x20;

        Once the sync is successful, the data source is published to the Tableau cloud/server:

        <Steps>
          <Step title="Log in to Tableau Cloud" titleSize="h2">
            Users should log in to Tableau Cloud using the same credentials of Tableau. This will redirect to the Tableau Cloud home page.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.png)
          </Step>

          <Step title="Manage projects" titleSize="h2">
            Click on the Manage Projects option on the home page.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(13\).png)
          </Step>

          <Step title="Access the project interface" titleSize="h2">
            This will open an interface displaying all projects, including the newly created project titled **Product Analysis**.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.1.png)
          </Step>

          <Step title="Select the project" titleSize="h2">
            Click on the project to view the available data sources for dashboard creation. This project will contains semantic model and all it's views (entities and metrics).

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.2.png)
          </Step>

          <Step title="Create a new workbook" titleSize="h2">
            Click on the menu option in the upper right corner of the data source and select the New Workbook option.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau6.png)
          </Step>

          <Step title="Provide credentials" titleSize="h2">
            To create a new workbook where dashboard creation can commence, users will be prompted to provide their DataOS username and API key as the password to access the data source. The API can be retrieved by navigating to the profile page in the bottom left corner of the Data Product Hub.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tabelau7.png)
          </Step>

          <Step title="Start creating the dashboard" titleSize="h2">
            Now, users can create dashboard and extract relevant insights.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/tableau8.png)
          </Step>

          <Step title="Publishing workbook/dashboard" titleSize="h2">
            The publisher can embed their credentials (DataOS username and API Token) or ask users to provide credentials whenever they want to access the published Workbook/Sheet/Dashboard. If the publisher has chosen to ‘Embed password for data source’, users can access the published workbook and dashboard without providing credentials.

            <Note>
              Callout Content
            </Note>
          </Step>
        </Steps>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Superset" defaultOpen={false}>
    <Tabs>
      <Tab title="Using curl">
        ## Prerequisites

        * **Curl**: Ensure `curl` is installed on the system. For Windows users, `curl.exe` may be required.

        * **Lens API endpoint**: The API endpoint provided by Lens to sync semantic model, enabling integration with Superset.

        * **Access credentials**: Superset requires the login credentials (username and password) and the host address where Superset is hosted.&#x20;

        ## Steps

        To sync the semantic model with Superset, follow these steps:

        <Steps>
          <Step title="Run the curl command" titleSize="h2">
            Copy the curl command syntax below and replace the placeholders with the actual values.

            <CodeGroup>
              ```yaml Syntax
              curl --location --request POST 'https://<DATAOS_FQDN>/lens2/sync/api/v1/superset/<WORKSPACE_NAME>:<LENS-NAME>' \
              --header 'apikey: <apikey>' \
              --header 'Content-Type: application/json' \
              --data-raw '
              {
                  "username": "<superset username>",
                  "password": "<superset password>",
                  "host": "https://superset-<DATAOS_FQDN>"
              }
              ```

              ```
              console.log("Hello World");
              ```
            </CodeGroup>

            **Command paramters:**

            * **`URL`**: `https://liberal-monkey.dataos.app/lens2/sync/api/v1/superset/public:quality360`. This is the endpoint for syncing with Superset.

            * **`DataOS FQDN`**: Current DataOS FQDN, e.g., `liberal-monkey.dataos.app`.

            * **`--header 'Content-Type: application/json'`**: Specifies the content type as JSON.

            * **`Lens_Name`**: Name of the Lens, e.g., `quality360`.

            * **`API_Key`**: DataOS API key. The DataOS API key for the user can be obtained by executing the command below.

            ```bash
            dataos-ctl user apikey get
            ```

            Upon initiation, a response will be received:

            ```bash
            {
                "message": "started"
            }
            ...
            {
                "message": "Superset project creation and sync completed successfully."
            }
            ```

            Once the command is executed in the terminal, results will be visible in the Superset app as demonstrated below:
          </Step>

          <Step title="Go to DataOS" titleSize="h2">
            Go to DataOS and select Superset.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/superset2.png)
          </Step>

          <Step title=" Navigate to Datasets tab" titleSize="h2">
            Here, each entity will appear as a dataset.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/superset3.png)

            The setup is complete. Further exploration and analysis can be performed in Superset.
          </Step>
        </Steps>

        ## Data policies and security

        Any data masking, restrictions, or permissions defined by the publisher will automatically be enforced for all viewers of the report, ensuring consistent data security and compliance. However, the behavior of data policies (e.g., masking) depends on who is the user of the Superset.
      </Tab>

      <Tab title="Using Data Product Hub">
        <Steps>
          <Step title="Access Superset Integration" titleSize="h2">
            Go to the 'Access Options' tab for your Data product in Data Product Hub. Under the 'BI Sync' section, locate the 'Superset' option.

            ![superset\_sync.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset_sync.png)
          </Step>

          <Step title="Initiate the Connection" titleSize="h2">
            Click on 'Add Connection' under the Superset option. This action will open a new window where you’ll enter your credentials to link DataOS with Superset.

            ![superset\_conn.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset_conn.png)
          </Step>

          <Step title="Enter Superset Credentials" titleSize="h2">
            In the setup window, fill in the required credentials:

            * **Username**: The Superset account username.

            * **Password**: The corresponding password for this account.

            <Info>
              You may need to consult your DataOS Administrator for the username and password of the Superset.
            </Info>
          </Step>

          <Step title="Activate the Data Product" titleSize="h2">
            Once you’ve entered all the credentials, click 'Activate' to complete the setup. This will link the *Product Affinity* semantic model with Superset.

            ![superset-connections.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset-connections.png)
          </Step>

          <Step title="Access Data Product in Superset" titleSize="h2">
            After activation, go to the DataOS homepage. Scroll to the 'Apache Superset' section, click on 'Datasets', and locate your activated data product available as datasets. You’re now ready to start visualizing and building analytical dashboards.
          </Step>
        </Steps>
      </Tab>
    </Tabs>
  </Accordion>
</AccordionGroup>

### Graphql

The [GraphQL guide](/resources/lens/exploration_of_deployed_lens_using_graphql/) provides detailed instructions on how to interact with the Lens GraphQL API to leverage a variety of tools.

### Python

Lens supports interaction through Python, allowing one to use libraries such as requests for making API calls and handling responses programmatically. This method is ideal for more complex queries and automation tasks. For detailed instructions on setting up and using Python with Lens, refer to the full [**Python guide**.](/resources/lens/exploration_of_deployed_lens_using_python/)

### REST APIs

Lens offers a REST API for querying data, retrieving metadata, and managing resources programmatically. To know step-by-step guidance on interacting with Lens through API calls, details of available endpoints, parameters, and request-response formats please refer to this [link](/resources/lens/explore_lens_using_rest_apis/).

### SQL APIs

Lens provides a PostgreSQL-compatible interface, enabling users to interact with the semantic model using standard PostgreSQL syntax. For detailed guidance on how to set up and use SQL APIs with Lens, refer to the full [**SQL API guide**](/resources/lens/expoloration_of_lens_using_sql_api/).

## Semantic modeling in Lens

Semantic data modeling is the process of defining and structuring raw data into organized and meaningful business definitions. It involves creating logical schemas, relationships, and aggregations to represent how data is stored, processed, and accessed. Click [here](/resources/lens/concepts/) to learn more about the semantic modeling concepts.

## Configure Lens&#x20;

The Lens can be configured to connect to different sources using data source attributes and configurable attributes in the`lens_deployment.yml` manifest files. For a comprehensive list of supported properties, please [click here](/resources/lens/lens_manifest_attributes/).

## Observability

Monitoring in Lens operates at three levels, each focusing on different aspects of system performance and reliability.

1. **Metric monitoring:**

   A Lens Resource can be monitored using Monitor and Pager Resources. To create a Monitor and Pager on Lens, click 

   [here](https://dataosinfo.mintlify.app/resources/lens/observability)

   .

2. **Operational monitoring:**

   Operational monitoring can be done through the Operations App and CLI, providing visibility into the state and behavior of Lens Resources, enabling workload performance tracking, historical runtime analysis, and troubleshooting. To know how to do operational monitoring of Lens click here

3. **Infrastructure monitoring:**

   Infrastructure powered by Grafana, focuses on the underlying system infrastructure, tracking server health, memory usage, and overall system stability.

## Governance

Data governance in DataOS is structured across three layers to protect and control sensitive information:

* **Physical data layer:**

  Governance policies are applied directly at the physical data layer, where the data resides in sources such as BigQuery, Postgres, Snowflake, or other data storage platforms. These policies enforce data protection at the source, controlling access and masking sensitive information before it is processed or exposed further. To know about how to govern the Lens click here.

*

**Model-first Data Product layer:**

At this layer, governance policies are applied to the semantic model (Lens) derived from the underlying source tables. To manage access effectively

[user groups](/resources/lens/working_with_user_groups/)

define different levels of exploration permissions within Lens.

* **Activation layer:**

  Governance at the activation layer focuses on enforcing data protection within the Business Intelligence (BI) tool. This ensures that sensitive data remains masked or restricted based on user roles when presented through dashboards or reports, controlling how the data is accessed and displayed in the final interface.

## How to catalog a semantic model built using Lens?

The metadata of a semantic model or Lens Resource is cataloged in Metis, which stores historical runtime and operations data in the Metis DB. This cataloging provides an aggregated view of metadata, enabling access to historical and operational insights about Resources. To learn more about cataloging semantic models and Lens Resource click here.

## Best practices

Click [here](/resources/lens/dos_and_donts/) to explore recommended guidelines and techniques to create efficient and scalable semantic models, along with a concise list of do's and don'ts.&#x20;

## Troubleshooting

Click [here](/resources/lens/errors/) to understand and resolve common errors while creating semantic model and deploying Lens.