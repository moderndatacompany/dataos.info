---
title: "Introduction"
description: "Lens Resource in DataOS is a logical modeling layer for accessing tabular data in data warehouses or lakehouses. It operates on top of physical tables, allowing the extension of these tables into logical tables by adding logical columns (measures) and relationships. It empowers analytical engineers, the key architects of business intelligence, with a model-first approach. "
---

<Tip>
  **Lens in the Data Product Lifecycle**

  Lens operates in the consumption layer of the Data Product Life Cycle within DataOS, By leveraging Lens, Data Products can be created to inform decision-making, ensuring data is well-organized and aligned with business objectives. To consume it, Lens exposes APIs such as Postgres, REST, and Graphql.
</Tip>

**Why Lens?**

The semantic modeling layer of the Lens serves as an interface that overlays the underlying data, consistently presenting business users with familiar and well-defined terms like `product`, `customer`, or `revenue`. This abstraction enables users to access and consume data in a way that aligns with their understanding, facilitating self-service analytics and reducing dependence on data engineers for ad-hoc data requests.

As a resource within the DataOS ecosystem, Lens enhances Data Product consumption by delivering improvements in how Data Products are accessed and utilized. It streamlines the developer experience in consumption patterns, focusing specifically on refining the use and interaction with data products.

## Key features of Lens

Lens is engineered to handle complex and large-scale data models with ease. Key features include:

* **Code modularity:** Lens supports modular code structures, simplifying the maintenance of extensive models, particularly when dealing with real-world entities (represented as tables), dimensions, and measures. This modularity enables efficient development and management, allowing teams to navigate large codebases with reduced complexity.

* **Segments:** [Segments](/resources/lens/working_with_segments/) are predefined filters that enable the definition of complex filtering logic in SQL. They allow you to create specific subsets of data, such as users from a particular city, which can be reused across different queries and reports. This feature helps streamline the data exploration process by simplifying the creation of reusable filters.

* **API support:** Lens enhances interoperability by simplifying application development with support for [Postgres API](/resources/lens/exploration_of_deployed_lens_using_sql_apis/), [REST API](/resources/lens/exploration_of_deployed_lens_using_rest_apis/), and [Graphql](https://dashboard.mintlify.com/dataosinfo/dataosinfo/editor/v4#graphql). Additionally, learned how to [work with payloads](/resources/lens/working_with_payload/) for querying and interacting with the system in the API Documentation.

* **Governance and access control:** Lens ensures data governance through[ user group management and data policies](/resources/lens/working_with_user_groups_and_data_policies/), enabling precise control over who can access and interact with data models.

* **BI integration:** Lens improves interoperability through robust integration with PowerBI, Tableau and Superset. This ensures that data models can be easily utilized across various BI platforms, enhancing the overall analytics experience. For more details on BI integration, visit the [BI Integration guide](/resources/lens/bi_integration/).

* **Performance optimization through Flash:** Designed to work with DataOS Lakehouse and Iceberg-format depots, [Flash](/resources/stacks/flash/) improves query performance by leveraging in-memory execution. This optimization ensures that data teams can efficiently handle large-scale queries with enhanced speed and performance.

## How to build a Semantic model using Lens?

The process begins with creating a new Lens project and generating a data model. Once the model is prepared, it will be tested within the development environment to ensure it is error-free before deployment.

### Single source

<AccordionGroup>
  <Accordion title="Redshift" defaultOpen={false}>
    ## Step 1: Create the AWS Redshift Depot

    If the Depot is inactive, you must create one using the provided template.

    ```yaml
    name: ${{redshift-depot-name}}
    version: v2alpha
    type: depot
    tags:
    - ${{redshift}}
    layer: user
    description: ${{Redshift Sample data}}
    depot:
    type: REDSHIFT
    redshift:
      host: ${{hostname}}
      subprotocol: ${{subprotocol}}
      port: ${{5439}}
      database: ${{sample-database}}
      bucket: ${{tmdc-dataos}}
      relativePath: ${{development/redshift/data_02/}}
    external: ${{true}}
    secrets:
      - name: ${{redshift-instance-secret-name}}-r
        allkeys: true

      - name: ${{redshift-instance-secret-name}}-rw
        allkeys: true

    ```

    ## Step 2: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](https://dataosinfo.mintlify.app/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    ## Step 1: Create the AWS Redshift Depot

    If the Depot is inactive, you must create one using the provided template.

    ```yaml
    name: ${{redshift-depot-name}}
    version: v2alpha
    type: depot
    tags:
    - ${{redshift}}
    layer: user
    description: ${{Redshift Sample data}}
    depot:
    type: REDSHIFT
    redshift:
      host: ${{hostname}}
      subprotocol: ${{subprotocol}}
      port: ${{5439}}
      database: ${{sample-database}}
      bucket: ${{tmdc-dataos}}
      relativePath: ${{development/redshift/data_02/}}
    external: ${{true}}
    secrets:
      - name: ${{redshift-instance-secret-name}}-r
        allkeys: true

      - name: ${{redshift-instance-secret-name}}-rw
        allkeys: true
    ```

    ## Step 2: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    Once the Lens with semantic model is prepared, create a `lens_deployment.yml` file parallel to the model folder to configure the deployment using the YAML template below.

    ```yaml
    version: v1alpha
    name: "redshiftlens"
    layer: user
    type: lens
    tags:
      - lens
    description: redshiftlens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: depot # source type is depot here
        name: redshiftdepot # name of the redshift depot
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/depot/redshift/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional    
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi
      worker: # optional
        replicas: 2 # optional
        logLevel: debug  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      router: # optional
        logLevel: info  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      metric:
        logLevel: info  # Logging level for the metric component
    ```

    Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

    * **Defining the Source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the AWS Redshift Depot created.

    * **Setting Up Compute and Secrets:**

      * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

      * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

    <Code>
      ```yaml Command
      dataos-ctl resource apply -f ${manifest-file-path}
      ```
    </Code>

    ## Docker compose manifest file

    <Accordion title="Click here to see the full docker compose manifest file" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-donkey.dataos.app

        # Overview
        LENS2_NAME: ${redshiftlens}
        LENS2_DESCRIPTION: "Ecommerce use case on sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "iamgroot"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: ${depot}  
        LENS2_SOURCE_NAME: ${redshiftdepot}
        LENS2_SOURCE_CATALOG_NAME: ${redshiftdepot}

        # Log
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        #LENS_DB_QUERY_TIMEOUT: 15m
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-02
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model

      ```
    </Accordion>

    ## Check query statistics for AWSRedshift

    <Note>
      Ensure the user has AWS console access before proceeding.
    </Note>

    <Steps>
      <Step title="Log in to AWS Console ">
        Login to the AWS Console and search for ‘Redshift’ in the AWS Console search bar to access the Redshift.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled1.png)
      </Step>

      <Step title="Select Redshift Cluster">
        Click on ‘Amazon Redshift’ from the search results. You will be directed to the Redshift dashboard. Select the appropriate region and choose the desired cluster name from the list.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled2.png)
      </Step>

      <Step title="Access Query Monitoring">
        Select the cluster you want to monitor. Navigate to the ‘Query monitoring’ tab to view query statistics.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled3.png)
      </Step>

      <Step title="View running and completed queries">
        In the ‘Query monitoring’ tab, you will see a list of running and completed queries.&#x20;

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled4.png)
      </Step>

      <Step title="Monitor specific query">
        Click on the query you want to monitor and view the query statistics, as shown in the example below.

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/awsredshift/Untitled5.png)
      </Step>
    </Steps>
  </Accordion>

  <Accordion title="BigQuery" defaultOpen={false}>
    ## Step 1: Create the Bigquery Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: ${{bigquerydepot}}
    version: v2alpha
    type: depot
    tags:
      - ${{dropzone}}
      - ${{bigquery}}
    owner: ${{owner-name}}
    layer: user
    depot:
      type: BIGQUERY                 
          description: ${{description}} # optional
      external: ${{true}}
      secrets:
        - name: ${{bq-instance-secret-name}}-r
          allkeys: true

        - name: ${{bq-instance-secret-name}}-rw
          allkeys: true
      bigquery:  # optional                         
        project: ${{project-name}} # optional
        params: # optional
          ${{"key1": "value1"}}
          ${{"key2": "value2"}}

    ```

    ## Step 2: Prepare the Lens model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure, only the necessary columns are extracted, and the SQL dialect is specific to the bigquery. For instance,

        * &#x20;format table names as: `project_id.dataset.table`.

        * Use `STRING` for text data types instead of `VARCHAR`.

        * Replace generic functions with BigQuery’s `EXTRACT` function.

        For instance, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For instance, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For instance, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](/resources/lens/working_with_user_groups_and_data_policies/).

        ## Step 3: Deployment manifest file

        After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

        ```yaml
        # RESOURCE META SECTION
        version: v1alpha # Lens manifest version (mandatory)
        name: "bigquery-lens" # Lens Resource name (mandatory)
        layer: user # DataOS Layer (optional)
        type: lens # Type of Resource (mandatory)
        tags: # Tags (optional)
          - lens
        description: bigquery depot lens deployment on lens2 # Lens Resource description (optional)

        # LENS-SPECIFIC SECTION
        lens:
          compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
          secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
            - name: bitbucket-cred # Referred Instance Secret name (mandatory)
              allKeys: true # All keys within the secret are required or not (optional)

          source: # Data Source configuration
            type: depot # Source type is depot here
            name: bigquerydepot # Name of the bigquery depot

          repo: # Lens model code repository configuration (mandatory)
            url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
            lensBaseDir: sample/lens/source/depot/bigquery/model # Relative path of the Lens 'model' directory in the repository (mandatory)
            syncFlags: # Additional flags used during synchronization, such as specific branch.
              - --ref=lens # Repository Branch

          api: # API Instances configuration (optional)
            replicas: 1 # Number of API instance replicas (optional)
            logLevel: info  # Logging granularity (optional)
            resources: # CPU and memory configurations for API Instances (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 2000m
                memory: 2048Mi

          worker: # Worker configuration (optional)
            replicas: 2 # Number of Worker replicas (optional)
            logLevel: debug # Logging level (optional)
            resources: # CPU and memory configurations for Worker (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          router: # Router configuration (optional)
            logLevel: info  # Level of log detail (optional)
            resources: # CPU and memory resource specifications for the router (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          iris:
            logLevel: info # Level of log detail (optional)
            resources: # CPU and memory resource specifications for the iris board (optional)
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 6000m
                memory: 6048Mi

          metric:    #optional
            logLevel: info
        ```

        Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

        * **Defining the Source:**

          * **Source type:** The `type` attribute in the `source` section must be explicitly set to `depot`.

          * **Source name:** The `name` attribute in the `source` section should specify the name of the Bigquery Depot created .

        * **Setting Up Compute and Secrets:**

          * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

          * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

        * **Defining Repository:**

          * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

          * **`lensBaseDir`:** The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/bigquery/model`.

          * **`secretId`:** The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub) . It specifies the secret needed to securely authenticate and access the repository.

          * **`syncFlags`**: Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model rsides in the dev branch.

        * **Configuring API, Worker and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.
      </Step>
    </Steps>

    ## Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command. Replace the placeholder with the&#x20;

    ```
    dataos-ctl apply -f {manifest-file-path}}
    ```

    ## Docker compose manifest file

    Ensure that highlighted attributes are in the Docker Compose Manifest file for proper configuration during the connection setup process.

    <Accordion title="Click here to see the full docker-compose manifest" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-monkey.dataos.app
        # Overview
        LENS2_NAME: sales360
        LENS2_DESCRIPTION: "Ecommerce use case on sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "rakeshvishvakarma, iamgroot"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: ${depot} #source name - depot
        LENS2_SOURCE_NAME: ${bigquerydepot} #name of the bigquery depot
        DATAOS_RUN_AS_APIKEY: ${A1ZjMDliZTFhZWJhMQ==}
        # LogZjAtNDY4My05
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-05
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model
            # - ./scripts/commons.js:/app/scripts/commons.js
            # - ./scripts/bootstrap.js:/app/scripts/bootstrap.js
            # - ./scripts/config.js:/app/scripts/config.js

      ```
    </Accordion>
  </Accordion>

  <Accordion title="Postgres" defaultOpen={false}>
    ## Step 1: Create Postgres Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: ${{postgresdb}}
    version: v2alpha
    type: depot
    layer: user
    depot:
      type: JDBC                  
      description: ${{To write data to postgresql database}}
      external: ${{true}}
      secrets:
        - name: ${{sf-instance-secret-name}}-r
          allkeys: true

        - name: ${{sf-instance-secret-name}}-rw
          allkeys: true
      postgresql:                        
        subprotocol: "postgresql"
        host: ${{host}}
        port: ${{port}}
        database: ${{postgres}}
        params: #Required 
          sslmode: ${{disable}}
    ```

    While creating Lens on Postgres Depot the following aspects need to be considered:

    * The SQL dialect used in the `model/sql` folder to load data from the Postgres source should be of the Postgres dialect.

    * The table naming in the `model/table` should be of the format: `schema.table`.

    ## Step 2: Prepare the semantic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

    ```
    dataos-ctl  apply -f ${manifest-file-path}
    ```

    ## Docker compose manifest file

    <Accordion title="Click here to see the full docker compose manifest file" defaultOpen={false}>
      ```yaml
      version: "2.2"

      x-lens2-environment: &lens2-environment
        # DataOS
        DATAOS_FQDN: liberal-monkey.dataos.app
        # Overview
        LENS2_NAME: sales360
        LENS2_DESCRIPTION: "Ecommerce use case on Adventureworks sales data"
        LENS2_TAGS: "lens2, ecom, sales and customer insights"
        LENS2_AUTHORS: "iamgroot, iamloki"
        LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
        # Data Source
        LENS2_SOURCE_TYPE: depot
        LENS2_SOURCE_NAME: postgreslens2
        DATAOS_RUN_AS_APIKEY: bGVuc3NzLmUzMDA1ZjMzLTZiZjAtNDY4My05ZjhhLWNhODliZTFhZWJhMQ==
        LENS2_DB_SSL : "true"
        # Log
        LENS2_LOG_LEVEL: error
        CACHE_LOG_LEVEL: "trace"
        # Operation
        LENS2_DEV_MODE: true
        LENS2_DEV_MODE_PLAYGROUND: false
        LENS2_REFRESH_WORKER: true
        LENS2_SCHEMA_PATH: model
        LENS2_PG_SQL_PORT: 5432
        CACHE_DATA_DIR: "/var/work/.store"
        NODE_ENV: production
        LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
      services:
        api:
          restart: always
          image: rubiklabs/lens2:0.35.41-05
          ports:
            - 4000:4000
            - 25432:5432
            - 13306:13306
          environment:
            <<: *lens2-environment   
          volumes:
            - ./model:/etc/dataos/work/model
      ```
    </Accordion>
  </Accordion>

  <Accordion title="Snowflake" defaultOpen={false}>
    ## Prerequisite

    CLI Version should be `dataos-cli 2.26.39-dev` or greater.

    ## Step 1: Create the Snowflake Depot

    If the Depot is not active, you need to create one using the provided template.

    ```yaml
    name: snowflake-depot
    version: v2alpha
    type: depot
    tags:
      - Snowflake depot
      - user data
    layer: user
    depot:
      name: sftest
      type: snowflake
      description: Depot to fetch data from Snowflake datasource
      secrets:
        - name: sftest-r
          keys:
            - sftest-r
          allKeys: true
        - name: sftest-rw
          keys:
            - sftest-rw
          allKeys: true
      external: true
      snowflake:
        database: TMDC_V1
        url: ABCD23-XYZ8932.snowflakecomputing.com
        warehouse: COMPUTE_WH
        account: ABCD23-XYZ8932
      source: sftest
    ```

    ## Step 2: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure the SQL dialect matches snowflake syntax. Format table names as `schema.table`.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
         "sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        #### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        #### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create user groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 3: Deployment manifest file

    After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

    [](https://dataos.info/resources/lens/data_sources/snowflake/#__codelineno-2-1)

    ```yaml
    # RESOURCE META SECTION
    version: v1alpha # Lens manifest version (mandatory)
    name: "snowflake-lens" # Lens Resource name (mandatory)
    layer: user # DataOS Layer (optional)
    type: lens # Type of Resource (mandatory)
    tags: # Tags (optional)
      - lens
    description: snowflake depot lens deployment on lens2 # Lens Resource description (optional)

    # LENS-SPECIFIC SECTION
    lens:
      compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
      secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
        - name: bitbucket-cred # Referred Instance Secret name (mandatory)
          allKeys: true # All keys within the secret are required or not (optional)

      source: # Data Source configuration
        type: depot # Source type is depot here
        name: snowflake-depot # Name of the snowflake depot

      repo: # Lens model code repository configuration (mandatory)
        url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
        lensBaseDir: sample/lens/source/depot/snowflake/model # Relative path of the Lens 'model' directory in the repository (mandatory)
        syncFlags: # Additional flags used during synchronization, such as specific branch.
          - --ref=lens # Repository Branch

      api: # API Instances configuration (optional)
        replicas: 1 # Number of API instance replicas (optional)
        logLevel: info  # Logging granularity (optional)
        resources: # CPU and memory configurations for API Instances (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi

      worker: # Worker configuration (optional)
        replicas: 2 # Number of Worker replicas (optional)
        logLevel: debug # Logging level (optional)
        resources: # CPU and memory configurations for Worker (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      router: # Router configuration (optional)
        logLevel: info  # Level of log detail (optional)
        resources: # CPU and memory resource specifications for the router (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      iris:
        logLevel: info # Level of log detail (optional)
        resources: # CPU and memory resource specifications for the iris board (optional)
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      metric:    #optional
        logLevel: info
    ```

    ### Step 4: Apply the Lens deployment manifest file

    After configuring the deployment file with the necessary settings, apply the manifest using the following command:

    ```
    dataos-ctl apply -f ${manifest-file-path}
    ```
  </Accordion>
</AccordionGroup>

### Multi source

<AccordionGroup>
  <Accordion title="Minerva" defaultOpen={false}>
    ## Prerequisite

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the sematic model folder

    In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for access control
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the Model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the Views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "minervalens"
    layer: user
    type: lens
    tags:
      - lens
    description: minerva deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: minerva #minerva/themis/depot
        name: minervacluster  #name of minerva cluster
        catalog: icebase
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/minerva/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=lens

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional 
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi

      worker: # optional
        replicas: 2 # optional
        logLevel: debug  # optional

        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      router: # optional
        logLevel: info  # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `minerva`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Minerva Cluster. For example, if the name of your Minerva Cluster is miniature the Source name would be `miniature`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Minerva Cluster that you intend to use. For instance, if the catalog is named icebase, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    <Info>
      Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
    </Info>

    ## Docker compose manifest file

    <Accordion title="Click here to view the complete docker compose  manifest and it's procedure" defaultOpen={false}>
      ```
      Prerequisites
      The hostname for the Trino database server.
      The username for the DataOS User.
      The name of the database to use with the Minerva query engine database server.
      Docker compose configuration
      Add the following environment variables to your Lens (.env) file
      Environment variables attribute
      Example
      trino.zip

      ```
    </Accordion>

    ## Check Query Stats for Minerva

    To check the query statistics, please follow the steps below:

    <Steps>
      <Step title="Open the Operations app" titleSize="h2">
        By default, the **User Space** tab is displayed upon accessing the Operations app. In User Space click on the 'Minerva Queries' tab to view query execution details.

        Set the following filters:

        * Source: `lens2`

        * Dialect: `trino_sql`

        Optionally, refine your results by filtering based on **Cluster**, **Username**, or other available criteria as needed.

        ![](https://mintlify.s3-us-west-1.amazonaws.com/dataosinfo/resources/lens/data_sources/minerva/Untitled1.png)
      </Step>

      <Step title="Select the query id" titleSize="h2">
        Identify and choose the **Query ID** of interest from the **Minerva Queries** tab. Once selected, the system will display detailed execution statistics, providing insights into performance, execution time, and resource utilization.
      </Step>
    </Steps>
  </Accordion>

  <Accordion title="Themis" defaultOpen={false}>
    ## Prerequisites

    Ensure you have an active and running Minerva Cluster.

    ## Step 1: Prepare the semantic model folder

    Organize the semantic model folder with the following structure to define tables, views, and user groups:

    ```
    model
    ├── sqls
    │   └── sample.sql  # SQL script for table dimensions
    ├── tables
    │   └── sample_table.yml  # Logical table definition (joins, dimensions, measures, segments)
    ├── views
    │   └── sample_view.yml  # Logical views referencing tables
    └── user_groups.yml  # User group policies for governance
    ```

    <Steps>
      <Step title="Load data from the data source" titleSize="h2">
        In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

        For example, a simple data load might look like this:

        ```sql
        SELECT
          *
        FROM
          "icebase"."sales_360".channel;
        ```

        Alternatively, you can write more advanced queries that include transformations, such as:

        ```sql
        SELECT
          CAST(customer_id AS VARCHAR) AS customer_id,
          first_name,
          CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
          age,
          CAST(register_date AS TIMESTAMP) AS register_date,
          occupation,
          annual_income,
          city,
          state,
          country,
          zip_code
        FROM
          "icebase"."sales_360".customer;
        ```
      </Step>

      <Step title="Define the table in the model" titleSize="h2">
        Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

        ```yaml
        table:
          - name: customers
            sql: {{ load_sql('customers') }}
            description: Table containing information about sales transactions.
        ```

        ### Add dimensions and measures

        After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

        ```yaml
        tables:
          - name: sales
            sql: {{ load_sql('sales') }}
            description: Table containing sales records with order details.

            dimensions:
              - name: order_id
                type: number
                description: Unique identifier for each order.
                sql: order_id
                primary_key: true
                public: true

            measures:
              - name: total_orders_count
                type: count
                sql: id
                description: Total number of orders.
        ```

        ### Add segments to filter

        Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

        ```yaml
        segments:
          - name: state_filter
            sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
        ```

        To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
      </Step>

      <Step title="Create the views" titleSize="h2">
        Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

        ```yaml
        views:
          - name: customer_churn_prediction
            description: Contains customer churn information.
            tables:
              - join_path: marketing_campaign
                includes:
                  - engagement_score
                  - customer_id
              - join_path: customer
                includes:
                  - country
                  - customer_segments
        ```

        To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
      </Step>

      <Step title="Create User groups" titleSize="h2">
        This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

        ```yaml
        user_groups:
          - name: default
            description: this is default user group
            includes: "*"
        ```

        To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
      </Step>
    </Steps>

    ## Step 2: Create a deployment manifest file

    After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

    ```yaml
    version: v1alpha
    name: "themis-lens"
    layer: user
    type: lens
    tags:
      - lens
    description: themis lens deployment on lens2
    lens:
      compute: runnable-default
      secrets:
        - name: bitbucket-cred
          allKeys: true
      source:
        type: themis #minerva/themis/depot
        name: lenstestingthemis
        catalog: icebase
      repo:
        url: https://bitbucket.org/tmdc/sample
        lensBaseDir: sample/lens/source/themis/model 
        # secretId: lens2_bitbucket_r
        syncFlags:
          - --ref=main #repo-name

      api:   # optional
        replicas: 1 # optional
        logLevel: info  # optional    
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi
      worker: # optional
        replicas: 2 # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      router: # optional
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
      iris:
        logLevel: info  
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
    ```

    The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

    Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

    * **Defining the Source:**

      * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `themis`.

      * **`name`:** The `name` attribute in the `source` section should specify the name of the Themis Cluster. For example, if the name of your Themis Cluster is `clthemis` the Source name would be `clthemis`.

      * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Themis Cluster that you intend to use. For instance, if the catalog is named `lakehouse_retail`, ensure this is accurately reflected in the catalog field.

    * **Defining Repository:**

      * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

      * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

      * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

      * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

    * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

    The above manifest is intended for a cluster named `lenstestingthemis`, created on the themis source, with the depot or data catalog named `icebase`. To use this manifest, copy the file and update the source details accordingly.

    <Info>
      Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
    </Info>
  </Accordion>
</AccordionGroup>

<Info>
  🗣️ If working with Lens 1.0 interface, click [here](/interfaces/lens/).
</Info>

## How to consume the semantic model?

After creating a Lens data model, the next step is to consume it—this means interacting with the model by running queries. The following section explains the key concepts for querying Lens through various methods, though all queries follow the same general format. Multiple ways are available to explore or interact with the Lens model or its underlying data, allowing you to ask meaningful questions of the data and retrieve valuable insights. Exploration can be performed using the following methods:

### BI Tools

<AccordionGroup>
  <Accordion title="Power BI" defaultOpen={false}>
    <Tabs>
      <Tab title="Using CLI">
        ## Prerequisites

        Before proceeding with the integration of Lens with Power BI, ensure that you have the following prerequisites in place:

        * **CURL**: Ensure that CURL is installed on your system. Windows users may need to use `curl.exe`.

        * **Version compatibility:** It is strongly recommended to download Power BI Version `2.132.908.0` or later to fully utilize `.pbip` files.&#x20;

        * **Lens API Endpoint**: The API endpoint provided by Lens to synchronize the semantic model, enabling integration with Power BI.

        * **DataOS API key**: You will need your apikey  for authentication. The API key can be obtained using the following command in your terminal:

        ### Curl command

        Use the following CURL command to synchronize the semantic model with Power BI:

        <CodeGroup>
          ```javascript Command
          curl --location --request POST 'https://<DATAOS_FQDN>/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' \
            --header 'apikey: ${APIKEY}' \
            --output ${FILE_NAME}.zip
          ```

          ```
          curl --location --request POST 'https://</lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' \
            --header 'apikey: ${APIKEY}' \
            --output ${FILE_NAME}.zip
          ```

          ```
          console.log("Hello World");
          ```
        </CodeGroup>

        ## Parameters

        | Parameter      | Description                                                                                                               |
        | -------------- | ------------------------------------------------------------------------------------------------------------------------- |
        | `URL`          | The URL  for syncing Lens with Power BI. It includes:                                                                     |
        | `apikey`       | User's API key for the current context in Lens.                                                                           |
        | `filename.zip` | Here you give the name of the Power BI file with which name you wish to download it. For example, `product-analysis.zip`. |

        The `file.zip` includes essential components for syncing a Lens Model with Power BI, organized into folders such as `.Report` and `.SemanticModel`:

        ## Steps

        To begin syncing a Lens model, the following steps should be followed:

        <Steps>
          <Step title="Open the terminal" titleSize="h2">
            Open the terminal and paste the following command:

            ```
            curl --location --request POST 'https://tcp.{context}.dataos.app/lens2/sync/api/v1/power-bi/{workspace_name}:{lens_name}' --header 'apikey: abcdefgh==' --output {file.zip}
            ```

            Replace the placeholders with your current context, workspace, lens name and the apikey.
          </Step>

          <Step title=" Extract the zip file" titleSize="h2">
            Once the command is executed, a zip file will be downloaded to the specified directory. The downloaded file should be unzipped. Three folders will be found inside, all of which are necessary for semantic synchronization with Power BI.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi2.png)

            <Note>
              Ensure that `.pbip` folders are fully extracted before opening them.Failure to do so may result in missing file errors, as shown below:
              ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image.png)
            </Note>
          </Step>

          <Step title="Open the `.pbip` file" titleSize="h2">
            Click on the file with `.pbip` extension to open  the Power BI file in the Power BI Desktop.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi3.png)
          </Step>

          <Step title="Enter credentials" titleSize="h2">
            After opening the file, a popup will prompt for credentials. The DataOS username and API key should be entered.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi4.png)
          </Step>

          <Step title=" Connect to DataOS" titleSize="h2">
            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi6.png)
            Click on the connect button. A popup will appear. Click Ok.
          </Step>

          <Step title="Create dashboards" titleSize="h2">
            Upon successful connection, tables and views will be accessible, displaying dimensions and measures. Now you are ready to create dashboards.
          </Step>
        </Steps>

        ## Important considerations

        * **Measure naming convention:** In Power BI, measures typically have an 'm\_' prefix to indicate they represent a measure. For example, a measure calculating total revenue might be named m\_total\_revenue.

        * **Live connection:** The connection is live, meaning any changes to the underlying data will be reflected in Power BI.

        * **Schema changes:** If schema changes occur, such as the addition of new dimensions and measures, the steps outlined above will need to be repeated.

        * **Row Limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        * **Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        ## Data governance and access control

        Data masking, restrictions, or permissions established by the publisher are automatically enforced for all report viewers, ensuring consistent data security and compliance. The behavior of these data policies, such as masking, may vary based on the user of the Power BI desktop.
      </Tab>

      <Tab title="Using Data Product Hub">
        <Steps>
          <Step title="Navigate to the Data Product Hub" titleSize="h2">
            Access the Home Page of DataOS. From there, navigate to the Data Product Hub to explore the various Data Products available within the platform.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(1\).png)
          </Step>

          <Step title="Browse and select a Data Product" titleSize="h2">
            Browse the list of Data Products and select a specific Data Product to initiate integration with Power BI. For example, selecting 'Sales360' allows detailed exploration and integration of the Sales360 Data Product.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(2\).png)
          </Step>

          <Step title="Access integration options" titleSize="h2">
            Go to the BI Sync option under the Access Options tab. Scroll down to locate the Excel and Power BI section, and click the Download .pbip File button to download the ZIP folder.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/powerbi/Powerbi3.png)
          </Step>

          <Step title="Download and open the ZIP file" titleSize="h2">
            Access the downloaded ZIP file on the local system and extract its contents to the specified directory. The extracted folder will contain three files. Ensure all three files remain in the same directory to maintain semantic synchronization of the Data Product.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(15\).png)

            The folder contains the main components of a Power BI project for syncing the Lens model (here `sales360`) including folders like the `.Report` and `.SemanticModel`. Following is the brief description of each:

            * **public\_sales360-table.Report:** This folder contains `definition.pbir` file related to the report definition in Power BI. These files define the visual representation of data, such as tables and charts, without storing actual data. They connect the semantic model and data sources to create the report views.

            * **public-sales360-table.SemanticModel:** This folder contains files that define the underlying data model for your Power BI project. The semantic model plays a crucial role in managing how Power BI interacts with data, setting up relationships, hierarchies, and measures.

              * **definition.bism:** This file represents the Business Intelligence Semantic Model (BISM). It defines the structure of your data, including data sources, relationships, tables, and measures for your Lens semantic model. The `.bism` file holds essential metadata that helps Power BI understand and query the data, forming the core of the data model for report creation and analysis.

              * **model.bim:** Power BI uses the `.bim` file to generate queries and manage interactions with the dataset. When you build reports or dashboards in Power BI, it references this semantic model to ensure the correct structure is applied to the data.

            * **public-sales-360-table.pbip:** This file serves as a Power BI project template or configuration file. Power BI uses files like `.pbip` or `.pbix` to encapsulate reports, datasets, and visualizations. The `.pbip` file ties together the semantic model and report definitions from the other folders, acting as the entry point for working on the project in Power BI Desktop or the Power BI service.
          </Step>

          <Step title="Enter credentials" titleSize="h2">
            Open the `public_sales360` file in Power BI, once the file is opened, a popup will appear prompting for the DataOS username and API key.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(16\).png)
            Click the connect button. A popup will appear; click OK.
          </Step>

          <Step title="View data in Power BI" titleSize="h2">
            After connecting, users can see tables and views containing dimensions and measures and create dashboards.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Untitled%20\(19\).png)
          </Step>
        </Steps>

        ## Important considerations

        * **Measure naming convention:** In Power BI, measures typically have an 'm\_' prefix to indicate they represent a measure. For example, a measure calculating total revenue might be named m\_total\_revenue.

        * **Live connection:** The connection is live, meaning any changes to the underlying data will be reflected in Power BI.

        * **Schema changes:** If schema changes occur, such as the addition of new dimensions and measures, the steps outlined above will need to be repeated.

        * **Row Limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        * **Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        ## Data governance and access control

        Data masking, restrictions, or permissions established by the publisher are automatically enforced for all report viewers, ensuring consistent data security and compliance. The behavior of these data policies, such as masking, may vary based on the user of the Power BI desktop.
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Tableau" defaultOpen={false}>
    <Tabs>
      <Tab title="Using CLI">
        ### **Prerequisites**

        * **Curl**: Ensure that `curl` is installed on the system. For Windows systems, `curl.exe` may be necessary.

        * **Lens API endpoint**: The API endpoint provided by Lens to sync semantic model, enabling integration with Tableau.

        * **Access credentials**: Access credentials such as username, password, project name etc., are required for Tableau.

        * **DataOS API key**: Ensure the DataOS API key is available. Get it by using the following command:

        ```
        dataos-ctl user apikey get
        ```

        ## Steps

        To sync the semantic model with Tableau, follow the steps below:

        <Steps>
          <Step title="Run the curl command">
            Open the terminal and run the following command. Make sure to replace the placeholders with the actual credentials.

            <CodeGroup>
              ```yaml Command
              curl --location --request POST 'http://tcp.<DATAOS_FQDN>/lens2/sync/api/v1/tableau-cloud/<WORKSPACE_NAME>:<LENS_NAME>' \
              --header 'apikey: <APIKEY>' \
              --header 'Content-Type: application/json' \
              --data-raw '{
                  "project_name": "<SAMPLE>",
                  "username": "{USER_NAME/EMAIL}",
                  "password": "<PASSWORD>",
                  "site_id": "<SITE_ID>",
                  "server_address": "https://prod-apnortheast-a.online.tableau.com"
              }'
              ```

              ```yaml Example
              curl --location --request POST 'http://tcp.alpha-omega.dataos.app/lens2/sync/api/v1/tableau-cloud/curriculum:dph-insights' \
              --header 'apikey: abcdefghiJ==' \
              --header 'Content-Type: application/json' \
              --data-raw '{
                  "project_name": "Training",
                  "username": "labs@tmdc.io",
                  "password": "******",
                  "site_id": "iamgroot1086a891fef336",
                  "server_address": "https://prod-apnortheast-a.online.tableau.com"
              }' 
              ```
            </CodeGroup>

            **URL:** This URL is used to sync a specific semantic model to Tableau for public access. Here,

            1. `<DATAOS_FQDN>`: Replace with the Fully Qualified Domain Name (FQDN) where Lens is hosted. Example: `liberal-monkey.dataos.app`.

            2. `<WORKSPACE_NAME>` : Replace with the name of the workspace where the semantic model (Lens) is deployed. e.g., `public`, `curriculum`.

            3. `<LENS_NAME>`:Replace with the name of the semantic model to sync. e.g., `sales360`.

            **Headers:**

            1. **`apikey`:** User's API key for the current context in Lens. The DataOS API key for the user can be obtained by executing the below command.

            ```
            dataos-ctl user apikey get
            ```

            1. **`Content-Type application/json`:** Specifies that the data being sent is in JSON format.

            **Raw data payload:** This section defines the details of the user's Tableau credentials and project configuration:

            1. **project\_name:** The name of the Tableau project where the data will be synced.  If the project does not already exist, Tableau will create a new project with the given name.

            2. **username:** Tableau account username, typically the email ID used to log in to Tableau.

            3. **password:** Tableau account password.

            4. **site\_id:** The site ID associated with the current Tableau connection.

            5. **server\_address:** The URL of the Tableau server. Replace with the correct server address (e.g., https://prod-apnortheast-a.online.tableau.com). This information can be obtained upon logging in to Tableau. The URL will appear as follows:> https://prod-apnortheast-a.online.tableau.com/#/site/iamgroot1086a891fef336/homeHere: **iamgroot1086a891fef336** is the **site\_id**.

            As you run the command the sync process starts. To view the semantic model in the &#x20;
          </Step>

          <Step title="Login to Tableau">
            Login to your Tableau account and navigate to  'Explore' tab on the left side of the Tableau homepage. The required tables and views will be visible in the project. Here you will see three sources: one for tables and two for views.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau1.png)
          </Step>

          <Step title="Create new workbook">
             Navigate to the Home tab on the left side and click on 'New'. Under this option, select 'Workbook' as shown in the below image.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau2.png)
          </Step>

          <Step title="Connect to data">
            After selecting 'Workbook', the 'Connect to Data' page will appear. Choose the desired views or tables for exploration or consumption, then click 'Connect' to proceed.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau3.png)

            Upon clicking 'Connect', a prompt will request the username and password. Enter the DataOS username and API key.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau4.png)

            The username will be your userid in the DataOS which you can get by typing this command in your terminal:

            ```
            dataos-ctl user get
            #Expected Output
            |    NAME    │    ID    │  TYPE  │          EMAIL          │          TAGS           |
            |------------│---------│--------│-------------------------│-------------------------|
            | iamgroot  │ iamgroot │ person │ iamgroot@tmdc.io       │ roles:id:user,          |
            |            │         │        │                         │ users:id:iamgroot       |
            ```

            To get the apikey, copy the following command in your terminal:

            ```bash
            dataos-ctl user apikey get

            #Expected_Output
            INFO[0000] 🔑 user apikey get...                         
            INFO[0001] 🔑 user apikey get...complete                 

                                                               TOKEN                                                   │  TYPE  │        EXPIRATION         │                   NAME                    
            ───────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────┼───────────────────────────┼───────────────────────────────────────────
              hLTQwNDktYmU2MS01ZTBlM2U1ZmNhYWE=     │ apikey │ 2025-02-09T05:30:00+05:30 │ token_allegedly_factually_popular_cod     
              jMWFhLTQ1MGYtOGY0Zi0wNDhiNDY4MmY0ODk= │ apikey │ 2025-02-08T05:30:00+05:30 │ token_actively_intensely_humble_honeybee  
              0MmU3LWE3OWQtZGQ5Yzc2MTNlNzg5         │ apikey │ 2025-02-07T05:30:00+05:30 │ token_mildly_safely_pleasing_shiner       
              RG9jdW1lbtOTU4NC04NzIxZjlhODY1NTY=                                     │ apikey │ 2025-01-28T13:30:00+05:30 │ Documentation                             


            ```
          </Step>

          <Step title="Create dashboards">
            After entering the credentials and clicking on 'Sign In', the model will be ready for visualization purposes.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/tableau5.png)

            <Info>
              The publisher can embed their credentials (DataOS username and API Token) or ask users to provide credentials whenever they want to access the published Workbook/Sheet/Dashboard. If the publisher has chosen to ‘Embed password for data source’, users can access the published workbook and dashboard without providing credentials. Once the credentials are embedded, they cannot be accessed. You need to overwrite and ‘publish-as’ the workbook to reconfigure the embedding password optionality.
            </Info>
          </Step>
        </Steps>

        ## Supported data types

        | Category       | Data Type               | Support Status             | Recommended Approach                                                                                                                                                                                                                                                                                                              |
        | -------------- | ----------------------- | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
        | Dimension      | `time`                  | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `string`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `number`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Dimension      | `boolean`               | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `max`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `min`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `number`                | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `sum`                   | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `count`                 | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `boolean`               | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `string`                | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `time`                  | Auto-converts to Dimension | NA                                                                                                                                                                                                                                                                                                                                |
        | Measure        | `avg`                   | Not Supported              | Option 1: To use measure of type ‘avg’, define an additional measure of type 'count' in that entity: `name: count`, `type: count`, `sql: '1'`. Option 2: Use measure of type 'number' and define average logic in SQL: `measures: - name: total_accounts type: number sql: "avg({accounts})”`                                     |
        | Measure        | `count_distinct`        | Not Supported              | Option 1: To use measure of type ‘count\_distinct’, define an additional measure of type 'count' in that entity: `name: count`, `type: count`, `sql: '1'`. Option 2: Use measure of type 'number' and define logic for count\_distinct in SQL: `measures: - name: total_accounts type: number sql: "count(distinct({accounts}))”` |
        | Measure        | `count_distinct_approx` | Not Supported              | NA                                                                                                                                                                                                                                                                                                                                |
        | Rolling Window | -                       | Supported                  | NA                                                                                                                                                                                                                                                                                                                                |

        ## Important considerations for Tableau integration

        **1. Handling Entities without Relationships:** An error will occur during synchronization if any entity in the data model lacks a defined relationship. To resolve this issue, the entity can be hidden to avoid synchronization errors.

        **2. Live connection:** The connection between the Lens semantic layer and Tableau Cloud is live meaning that any changes to the underlying data will automatically be reflected in Tableau.

        **3. Schema changes:** If there are schema updates, such as adding new dimensions or measures, the integration steps will need to be repeated to incorporate these changes into Tableau.

        **4. Avoiding cyclic dependencies:** Tableau does not support cyclic dependencies within data models. To prevent integration issues, it is essential to ensure that the data model is free of cyclic dependencies prior to syncing with Tableau.

        **5. Visualization with multiple data sources:** You cannot build a visualization that incorporates data from multiple data sources. For live connections, Tableau does not support data blending. Only a single data source can be used to create a visualization.

        **6. Centralized management:** All data sources should be managed and published by the admin on the server, with everyone else using this source.

        **7. Single authority for Desktop publications:** If data sources are published via Tableau Desktop, ensure that all sources are published by a single authority to avoid multiple data source conflicts on the server.

        **8. Row limit:** The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

        **9. Selection:** It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

        **10. Parameter Action:** Action filters can be defined on measures/dimensions to filter visualizations effectively.

        **11. Default chart types:** All default chart types provided by Tableau can be plotted and visualized without issues.

        **12. Rolling Window Measure:** For querying a rolling window measure, it is necessary to provide a time dimension and apply a date range filter to this time dimension. When querying a rolling window measure, follow these steps:

        * Select the rolling window measure.

        * Select the time dimension.

        * To define granularity, right-click on the selected time dimension and set granularity (choose a granularity where the complete time, along with the year, is shown).

        * Add the time dimension to the filter, and define the range filter.

        <Info>
          Be aware that custom calculations or fields (measures/dimensions) created in BI tools may be lost during re-sync. It is preferable to create custom logic directly in Tableau's Lens.
        </Info>

        ## Error handling

        **Scenario 1: Handling syntactical errors in measures or dimensions**

        If a measure or dimension contains a syntactical error (and is also not functioning in Explore studio of DPH), the following error will appear when attempting to select it:

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image02.png)

        After correcting the syntactical error in the measure or dimension within Lens, the error will no longer appear. To reflect the changes in Tableau, refreshing the data source and re-selecting the measure or dimension will be necessary to display it in the chart.

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image03.png)

        **Scenario 2: Handling inactive Lens in the environment**

        If the Lens is not active in the environment while working on an existing workbook in Tableau or when attempting to establish a new connection, an error will be encountered. This may prevent access to or querying data from the Lens. Verify that the Lens exists and is active before syncing.

        **Scenario 3: Handling data source errors due to access restrictions**

        If the Account table is set to `public = false`, a data source error will occur in Tableau. The error message will indicate that the 'Account table not found,' which will prevent querying or using data from that table.

        ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image06.png)

        To resolve this issue, ensure the Account table is accessible (set to `public = true` or assign appropriate permissions) and then resync the Lens in Tableau to regain access.

        ## Governance of model on Tableau Cloud

        When the semantic model is activated via BI Sync in Tableau, data masking, restrictions, and permissions set by the publisher are automatically applied, ensuring consistent data security and compliance. The behavior of these policies (e.g., masking) may vary based on the Tableau user.

        The Tableau management process involves authentication and authorization using the DataOS user ID and API key when accessing synced data models. This ensures that columns redacted by Lens data policies are restricted based on the user's group permissions.

        For example, if a user named **iamgroot** in the **Analyst** group is restricted from viewing the 'Annual Salary' column, this column will not be visible in either the Data Product exploration page or Tableau after syncing. Tableau Cloud requires the DataOS user ID and API key for authentication, ensuring that users can access the full model, except for any columns restricted by any data policies.

        This approach maintains security and guarantees that users only see the data they are authorized to view.
      </Tab>

      <Tab title="Using Data Product Hub ">
        <Steps>
          <Step title=" Navigate to the Data Product Hub">
            Access the Home Page of DataOS. From the home page, navigate to the Data Product Hub to explore the various Data Products available.

            ![](/image.png)
          </Step>

          <Step title="Browse and select a Data Product">
            Browse through the list of available Data Products. Select a specific Data Product to integrate with Tableau. For instance, **Product 360** can be chosen to explore the Data Product on Tableau for data visualisation and getting insights.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau0.png)
          </Step>

          <Step title="Navigate to the Access Options tab">
            After selecting a Data Product, navigate to the **BI Sync** option in the **Access Options** tab. Scroll through the BI Sync and locate the **Tableau Cloud** option. Now, Click on the **Add Connection** button

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau1.png)
          </Step>

          <Step title="Enter connection details and click Activate button">
            A connection window will open, prompting the entry of the necessary connection details. There are following two ways to pass the connection details:

            <Tabs>
              <Tab title="Using  username and password">
                Following are the connection details of the Tableau username and password:
              </Tab>

              <Tab title="Using Personal Access Token">
                In addition to using standard Tableau credentials, users can also opt to use Personal Access Tokens (PAT) for authentication.&#x20;

                ## Prerequisites

                **Tableau Personal Access Token (PAT):** Before integrating the semantic model with Tableau using a PAT, ensure that you generate a PAT in Tableau by following the instructions provided in [this guide](https://help.tableau.com/current/online/en-us/security_personal_access_tokens.htm).
              </Tab>
            </Tabs>

            * **Project Name**: Enter the Tableau project name. If a project with that name does not already exist, it will be created automatically. If you do not specify a name, the project will default to the name of the Data Product, which will register all associated data sources under that project.

            For optimal organization within Tableau, we recommend providing a custom project name it facilitates easier navigation of your data sources.

            * **Server Name**: The address of the Tableau server (e.g., `https://prod-apnortheast-a.online.tableau.com`).

            * **Site ID**: The site ID (e.g., `tableausuer@123`).

            * **Username**: The Tableau username.(e.g., `labs@tmdc.io`)

            * **Password**: The password associated with the Tableau account.

            These details can be obtained upon logging into Tableau. The URL format will appear as follows:

            [`https://prod-apnortheast-a.online.tableau.com/#/site/site_id`](https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123)

            **Sample URL**:

            [`https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123/home`](https://prod-apnortheast-a.online.tableau.com/#/site/tableauuser@123/home)

            In this example, `tableuuser@123` represents the **site\_id**.

            After entering the required credentials, click the Activate button to establish the connection. A confirmation message will appear upon successful connection.
          </Step>
        </Steps>

        ## Exploring the semantic model on Tableau&#x20;

        Once the sync is successful, the data source is published to the Tableau cloud/server:

        <Steps>
          <Step title="Log in to Tableau Cloud">
            Users should log in to Tableau Cloud using the same credentials of Tableau. This will redirect to the Tableau Cloud home page.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.png)
          </Step>

          <Step title="Manage projects">
            Click on the Manage Projects option on the home page.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/image%20\(13\).png)
          </Step>

          <Step title="Access the project interface">
            This will open an interface displaying all projects, including the newly created project titled **Product Analysis**.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.1.png)
          </Step>

          <Step title="Select the project">
            Click on the project to view the available data sources for dashboard creation. This project will contains semantic model and all it's views (entities and metrics).

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau5.2.png)
          </Step>

          <Step title="Create a new workbook">
            Click on the menu option in the upper right corner of the data source and select the New Workbook option.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tableau6.png)
          </Step>

          <Step title="Provide credentials">
            To create a new workbook where dashboard creation can commence, users will be prompted to provide their DataOS username and API key as the password to access the data source. The API can be retrieved by navigating to the profile page in the bottom left corner of the Data Product Hub.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/Tabelau7.png)
          </Step>

          <Step title="Start creating the dashboard">
            Now, users can create dashboard and extract relevant insights.

            ![DPH](https://dataos.info/interfaces/data_product_hub/activation/bi_sync/Tableau/tableau8.png)
          </Step>

          <Step title="Publishing workbook/dashboard">
            The publisher can embed their credentials (DataOS username and API Token) or ask users to provide credentials whenever they want to access the published Workbook/Sheet/Dashboard. If the publisher has chosen to ‘Embed password for data source’, users can access the published workbook and dashboard without providing credentials.

            <Note>
              Callout Content
            </Note>
          </Step>
        </Steps>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Superset" defaultOpen={false}>
    <Tabs>
      <Tab title="Using curl">
        ## Prerequisites

        * **Curl**: Ensure `curl` is installed on the system. For Windows users, `curl.exe` may be required.

        * **Lens API endpoint**: The API endpoint provided by Lens to sync semantic model, enabling integration with Superset.

        * **Access credentials**: Superset requires the login credentials (username and password) and the host address where Superset is hosted.&#x20;

        ## Steps

        To sync the Lens model with Superset, follow these steps:

        <Steps>
          <Step title="Run the curl command">
            Copy the curl command syntax below and replace the placeholders with the actual values.

            <CodeGroup>
              ```yaml Syntax
              curl --location --request POST 'https://<DATAOS_FQDN>/lens2/sync/api/v1/superset/<WORKSPACE_NAME>:<LENS-NAME>' \
              --header 'apikey: <apikey>' \
              --header 'Content-Type: application/json' \
              --data-raw '
              {
                  "username": "<superset username>",
                  "password": "<superset password>",
                  "host": "https://superset-<DATAOS_FQDN>"
              }
              ```

              ```
              curl --location --request POST 'https://liberal-monkey.dataos.app/lens2/sync/api/v1/superset/public:company-intelligence' \
              --header 'apikey: aueniekQa==' \
              --header 'Content-Type: application/json' \
              --data-raw '
              {
                  "username": "adder_1",
                  "password": "adder_1",
                  "host": "https://superset-liberal-monkey.dataos.app"
              }
              ```
            </CodeGroup>

            **Command paramters:**

            * **`URL`**: `https://liberal-monkey.dataos.app/lens2/sync/api/v1/superset/public:quality360`. This is the endpoint for syncing with Superset.

            * **`DataOS FQDN`**: Current DataOS FQDN, e.g., `liberal-monkey.dataos.app`.

            * **`--header 'Content-Type: application/json'`**: Specifies the content type as JSON.

            * **`Lens_Name`**: Name of the Lens, e.g., `quality360`.

            * **`API_Key`**: DataOS API key. The DataOS API key for the user can be obtained by executing the command below.

            ```bash
            dataos-ctl user apikey get
            ```

            Upon initiation, a response will be received:

            ```bash
            {
                "message": "started"
            }
            ...
            {
                "message": "Superset project creation and sync completed successfully."
            }
            ```

            Once the command is executed in the terminal, results will be visible in the Superset app as demonstrated below:
          </Step>

          <Step title="Go to DataOS">
            Go to DataOS and select Superset.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/superset2.png)
          </Step>

          <Step title=" Navigate to Datasets tab">
            Here, each entity will appear as a dataset.

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/superset3.png)

            The setup is complete. Further exploration and analysis can be performed in Superset.
          </Step>
        </Steps>

        ## Data policies and security

        Any data masking, restrictions, or permissions defined by the publisher will automatically be enforced for all viewers of the report, ensuring consistent data security and compliance. However, the behavior of data policies (e.g., masking) depends on who is the user of the Superset.
      </Tab>

      <Tab title="Using Data Product Hub">
        <Steps>
          <Step title="Access Superset Integration">
            Go to the 'Access Options' tab for your Data product in Data Product Hub. Under the 'BI Sync' section, locate the 'Superset' option.

            ![superset\_sync.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset_sync.png)
          </Step>

          <Step title="Initiate the Connection">
            Click on 'Add Connection' under the Superset option. This action will open a new window where you’ll enter your credentials to link DataOS with Superset.

            ![superset\_conn.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset_conn.png)
          </Step>

          <Step title="Enter Superset Credentials">
            In the setup window, fill in the required credentials:

            * **Username**: The Superset account username.

            * **Password**: The corresponding password for this account.

            <Info>
              You may need to consult your DataOS Administrator for the username and password of the Superset.
            </Info>
          </Step>

          <Step title="Activate the Data Product">
            Once you’ve entered all the credentials, click 'Activate' to complete the setup. This will link the *Product Affinity* semantic model with Superset.

            ![superset-connections.png](https://dataos.info/learn/dp_consumer_learn_track/integrate_bi_tools/superset/superset-connections.png)
          </Step>

          <Step title="Access Data Product in Superset">
            After activation, go to the DataOS homepage. Scroll to the 'Apache Superset' section, click on 'Datasets', and locate your activated data product available as datasets. You’re now ready to start visualizing and building analytical dashboards.
          </Step>
        </Steps>
      </Tab>
    </Tabs>
  </Accordion>
</AccordionGroup>

### Graphql

The [GraphQL guide](/resources/lens/exploration_of_deployed_lens_using_graphql/) provides detailed instructions on how to interact with the Lens GraphQL API, enabling you to leverage a variety of tools.

### Python

Lens supports interaction through Python, allowing you to use libraries like requests for making API calls and handling responses programmatically. This method is ideal for more complex queries and automation tasks. For detailed instructions on setting up and using Python with Lens, refer to the full [**Python API guide**.](/resources/lens/exploration_of_deployed_lens_using_python/)

### REST APIs

Lens offers a REST API for querying data, retrieving metadata, and managing resources programmatically. The [Exploration of Lens Using REST API](/resources/lens/exploration_of_deployed_lens_using_rest_apis/) section provides step-by-step guidance on interacting with Lens through API calls, details of available endpoints, parameters, and request-response formats.

### SQL APIs

Lens provides a PostgreSQL-compatible interface, enabling you to interact with the semantic model using standard PostgreSQL syntax. For detailed guidance on how to set up and use SQL APIs with Lens, refer to the full [**SQL API guide**](/resources/lens/expoloration_of_lens_using_sql_api/).

## How to do data modeling in the semantic model built using Lens?

Data modeling is the process of defining and structuring raw data into organized and meaningful business definitions. It involves creating logical schemas, relationships, and aggregations to represent how data is stored, processed, and accessed. Click [here](/resources/lens/concepts/) to learn more about the Data modeling concepts.

## How to configure a semantic model built using Lens?

The Lens can be configured to connect to different sources using data source attributes and configurable attributes in the`lens_deployment.yml` manifest files. Here is a [comprehensive ](https://dataosinfo.mintlify.app/resources/lens/lens_manifest_attributes/) supported properties.

## How to observe and monitor a semantic model built using Lens?

Monitoring in Lens operates at three levels, each focusing on different aspects of system performance and reliability.

1. **Metric monitoring:** A Lens Resource can be monitored using Monitor and Pager Resources. To learn how to create and configure these monitors, click [here](https://dataosinfo.mintlify.app/resources/lens/observability).

2. **Operational monitoring:** Operational monitoring can be done through the Operations App and CLI, providing visibility into the state and behavior of Lens Resources, enabling workload performance tracking, historical runtime analysis, and troubleshooting.

3. **Infrastructure monitoring:** Infrastructure powered by Grafana, focuses on the underlying system infrastructure, tracking server health, memory usage, and overall system stability.

## How to govern a semantic model built using Lens?

Data governance in DataOS is structured across three layers to protect and control sensitive information:

* **Physical data layer:** Governance policies are applied directly at the physical data layer, where the data resides in sources such as BigQuery, Postgres, Snowflake, Lakehouse, or other data storage platforms. These policies enforce data protection at the source, controlling access and masking sensitive information before it is processed or exposed further.&#x20;

* &#x20;**Model-first Data Product layer:** At this layer, governance policies are applied to the semantic model (Lens) derived from the underlying source tables. To manage access effectively [**user groups**](/resources/lens/working_with_user_groups/) are used to define different levels of exploration permissions within Lens.&#x20;

* **Activation layer (BI tool):** Governance at the activation layer focuses on enforcing data protection within the Business Intelligence (BI) tool. This ensures that sensitive data remains masked or restricted based on user roles when presented through dashboards or reports, controlling how the data is accessed and displayed in the final interface.

## How to catalog a semantic model built using Lens?

The metadata of a semantic model (Lens Resource) is cataloged in Metis, which stores historical runtime and operations data in the Metis DB. This cataloging provides an aggregated view of metadata, enabling access to historical and operational insights about Resources. To learn more about how to catalog semantic models and Lens Resources click here

## Optimize the query performance of a semantic model using Flash

The Lens semantic layer provides several optimization techniques that can significantly enhance the performance of data queries. [This page](/resources/lens/fine_tuning_a_lens_model/) explores strategies for optimizing query performance for maximum efficiency.

## Best Practices

Click [here](/resources/lens/dos_and_donts/) to explore recommended guidelines and techniques to create efficient and scalable data models, along with a concise list of do's and don'ts.&#x20;

## Troubleshooting

Click [here](/resources/lens/errors/) to understand and resolve common errors in data modeling.