---
title: "Introduction"
description: "Lens Resource in DataOS is a logical modeling layer for accessing tabular data in data warehouses or lakehouses. It operates on top of physical tables, allowing the extension of these tables into logical tables by adding logical columns (measures) and relationships. It empowers analytical engineers, the key architects of business intelligence, with a model-first approach."
---

<Tip>
  **Lens in the Data Product Lifecycle**

  Lens operates in the consumption layer of the Data Product Life Cycle within DataOS, By leveraging Lens, Data Products can be created to inform decision-making, ensuring data is well-organized and aligned with business objectives. To consume it, Lens exposes APIs such as Postgres, REST, and GraphQL.
</Tip>

**Why Lens?**

The data modeling layer serves as an interface that overlays the underlying data, consistently presenting business users with familiar and well-defined terms like `product`, `customer`, or `revenue`. This abstraction enables users to access and consume data in a way that aligns with their understanding, facilitating self-service analytics and reducing dependence on data engineers for ad-hoc data requests.

As a resource within the DataOS ecosystem, Lens enhances Data Product consumption by delivering improvements in how Data Products are accessed and utilized. It streamlines the developer experience in consumption patterns, focusing specifically on refining the use and interaction with data products.

## Key features of Lens

Lens  is engineered to handle complex and large-scale data models with ease. Key features include:

* **Code modularity:** Lens supports modular code structures, simplifying the maintenance of extensive models, particularly when dealing with real-world entities (represented as tables), dimensions, and measures. This modularity enables efficient development and management, allowing teams to navigate large codebases with reduced complexity.

* **Segments:** [Segments](/resources/lens/working_with_segments/) are predefined filters that enable the definition of complex filtering logic in SQL. They allow you to create specific subsets of data, such as users from a particular city, which can be reused across different queries and reports. This feature helps streamline the data exploration process by simplifying the creation of reusable filters.

* **API Support:** Lens enhances interoperability by simplifying application development with support for [Postgres API](/resources/lens/exploration_of_deployed_lens_using_sql_apis/), [REST API](/resources/lens/exploration_of_deployed_lens_using_rest_apis/), and [GraphQL](/resources/lens/exploration_of_deployed_lens_using_graphql/). Additionally, learn how to [work with payloads](/resources/lens/working_with_payload/) for querying and interacting with the system in the API Documentation.

* **Governance and Access Control:** Lens ensures data governance through[ user group management and data policies](/resources/lens/working_with_user_groups_and_data_policies/), enabling precise control over who can access and interact with data models.

* **BI integration:** Lens improves interoperability through robust integration with Superset, Tableau and PowerBI. This ensures that data models can be easily utilized across various BI platforms, enhancing the overall analytics experience. For more details on BI integration, visit the [BI Integration Guide](/resources/lens/bi_integration/).

* **Performance optimization through Flash:** Designed to work with DataOS Lakehouse and Iceberg-format depots, [Flash](/resources/stacks/flash/) improves query performance by leveraging in-memory execution. This optimization ensures that data teams can efficiently handle large-scale queries with enhanced speed and performance.

## How to build a Semantic model?

The process begins with creating a new Lens project and generating a data model. Once the model is prepared, it will be tested within the development environment to ensure it is error-free before deployment.

#### **Lens model folder set-up**

The initial step involves setting up the folder structure for your Lens project. [This section](/resources/lens/lens_model_folder_setup/) will guide you through organizing your project files, including the model configuration files and necessary resources, in a structured and maintainable way.

<Accordion title="Single source" defaultOpen={false}>
  <Tabs>
    <Tab title="AWS Redshift">
      ## Step 1: Create the Depot

      If the Depot is inactive, you must create one using the provided template.

      ```yaml
      name: ${{redshift-depot-name}}
      version: v2alpha
      type: depot
      tags:
        - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
        type: REDSHIFT
        redshift:
          host: ${{hostname}}
          subprotocol: ${{subprotocol}}
          port: ${{5439}}
          database: ${{sample-database}}
          bucket: ${{tmdc-dataos}}
          relativePath: ${{development/redshift/data_02/}}
        external: ${{true}}
        secrets:
          - name: ${{redshift-instance-secret-name}}-r
            allkeys: true

          - name: ${{redshift-instance-secret-name}}-rw
            allkeys: true
      ```

      ## Step 2: Prepare the sematic model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 3: Deployment manifest file

      Once the Lens with semantic model is prepared, create a `lens_deployment.yml` file parallel to the model folder to configure the deployment using the YAML template below.

      ```yaml
      version: v1alpha
      name: "redshiftlens"
      layer: user
      type: lens
      tags:
        - lens
      description: redshiftlens deployment on lens2
      lens:
        compute: runnable-default
        secrets:
          - name: bitbucket-cred
            allKeys: true
        source:
          type: depot # source type is depot here
          name: redshiftdepot # name of the redshift depot
        repo:
          url: https://bitbucket.org/tmdc/sample
          lensBaseDir: sample/lens/source/depot/redshift/model 
          # secretId: lens2_bitbucket_r
          syncFlags:
            - --ref=lens

        api:   # optional
          replicas: 1 # optional
          logLevel: info  # optional    
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi
        worker: # optional
          replicas: 2 # optional
          logLevel: debug  # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        router: # optional
          logLevel: info  # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        iris:
          logLevel: info  
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        metric:
          logLevel: info  # Logging level for the metric component
      ```

      Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

      * **Defining the Source:**

        * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

        * **`name`:** The `name` attribute in the `source` section should specify the name of the AWS Redshift Depot created.

      * **Setting Up Compute and Secrets:**

        * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

        * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

      * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      ## Docker compose manifest file

      <Accordion title="Click here to view the complete docker manifest" defaultOpen={false}>
        ```
        version: "2.2"

        x-lens2-environment: &lens2-environment
          # DataOS
          DATAOS_FQDN: liberal-donkey.dataos.app

          # Overview
          LENS2_NAME: ${redshiftlens}
          LENS2_DESCRIPTION: "Ecommerce use case on sales data"
          LENS2_TAGS: "lens2, ecom, sales and customer insights"
          LENS2_AUTHORS: "iamgroot"
          LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
          # Data Source
          LENS2_SOURCE_TYPE: ${depot}  
          LENS2_SOURCE_NAME: ${redshiftdepot}
          LENS2_SOURCE_CATALOG_NAME: ${redshiftdepot}

          # Log
          LENS2_LOG_LEVEL: error
          CACHE_LOG_LEVEL: "trace"
          # Operation
          #LENS_DB_QUERY_TIMEOUT: 15m
          LENS2_DEV_MODE: true
          LENS2_DEV_MODE_PLAYGROUND: false
          LENS2_REFRESH_WORKER: true
          LENS2_SCHEMA_PATH: model
          LENS2_PG_SQL_PORT: 5432
          CACHE_DATA_DIR: "/var/work/.store"
          NODE_ENV: production
          LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
        services:
          api:
            restart: always
            image: rubiklabs/lens2:0.35.41-02
            ports:
              - 4000:4000
              - 25432:5432
              - 13306:13306
            environment:
              <<: *lens2-environment   
            volumes:
              - ./model:/etc/dataos/work/model
        ```
      </Accordion>

      ## Check query statistics for AWSRedshift

      > Note: Ensure the user has AWS console access before proceeding.

      <Steps>
        <Step title="Log in to AWS Console and access Redshift">
          Login to the AWS Console and  search 'Redshift' in the AWS Console search bar.

          ![](/resources/lens/data_sources/awsredshift/Untitled1.png)
        </Step>

        <Step title=" Select Redshift Cluster">
          Click on 'Amazon Redshift' from the search results. You will be directed to the Redshift dashboard.  Select the appropriate region and choose the desired cluster name from the list.

          ![](/resources/lens/data_sources/awsredshift/Untitled2.png)
        </Step>

        <Step title="Access Query Monitoring">
          Select the cluster you want to monitor. Navigate to the 'Query monitoring' tab to view query statistics.

          ![](/resources/lens/data_sources/awsredshift/Untitled3.png)
        </Step>

        <Step title=" View running and completed queries">
          In the 'Query monitoring' tab, you will see a list of running and completed queries.

          ![](/resources/lens/data_sources/awsredshift/Untitled4.png)
        </Step>

        <Step title="Monitor specific query">
          Click on the query you want to monitor. View the query statistics, as shown in the example below.

          ![](/resources/lens/data_sources/awsredshift/Untitled5.png)
        </Step>
      </Steps>
    </Tab>

    <Tab title="Bigquery" />

    <Tab title="Postgres" />

    <Tab title="Snowflake" />
  </Tabs>
</Accordion>

<Accordion title="Multi source" defaultOpen={false}>
  <Tabs>
    <Tab title="Minerva " />

    <Tab title="Themis" />
  </Tabs>
</Accordion>

<Accordion title="Query Accelration" defaultOpen={false} />

#### **Develop Lens within DataOS**

[This section](/resources/lens/lens_deployment/) involves the step-by-step guide on building and deploying Lens models within the DataOS environment. You will learn how to use Lens to generate and deploy data models, making sure they integrate seamlessly with the broader DataOS ecosystem.

## Configurations

Lens can be configured to connect to different sources using data source attributes and configurable attributes in the `docker-compose.yml` or `lens.yml` manifest files. Here is a comprehensive guide to APIs and configuring supported properties.

* [Configuration Fields of the Deployment Manifest File (YAML) for Lens Resource](/resources/lens/lens_manifest_attributes/)
  Understand the various configuration fields available in the deployment manifest file for Lens resources.

* [Configuration Fields of the Docker Compose File](/resources/lens/docker_compose_manifest_attributes/)
  Review the configuration fields and settings in the Docker Compose file for orchestrating multi-container applications.

<Info>
  🗣️ If working with Lens 1.0 interface, click [here](https://dataos.info/interfaces/lens/).
</Info>

## Exploration of deployed Lens

After creating a Lens data model, the next step is to explore it—this means interacting with the model by running queries. The following section explains the key concepts for querying Lens through various methods, though all queries follow the same general format. Multiple ways are available to explore or interact with the Lens model or its underlying data, allowing you to ask meaningful questions of the data and retrieve valuable insights. Exploration can be performed using the following methods:

* [Exploration of deployed Lens using SQL APIs](/resources/lens/exploration_of_deployed_lens_using_sql_apis/)

* [Exploration of deployed Lens using Python](/resources/lens/exploration_of_deployed_lens_using_python/)

* [Exploration of deployed Lens using Rest APIs](/resources/lens/exploration_of_deployed_lens_using_rest_apis/)

* [Exploration of deployed Lens using GraphQL](/resources/lens/exploration_of_deployed_lens_using_graphql/)

## Data modelling

[Data modeling](/resources/lens/overview/) is the process of defining and structuring raw data into organized and meaningful business definitions. It involves creating logical schemas, relationships, and aggregations to represent how data is stored, processed, and accessed. Effective data modeling ensures optimal performance for queries and allows users to extract valuable insights without modifying the underlying data structure. Below are resources to guide you through essential aspects of data modeling to optimize performance and accuracy.

* [Data modelling concepts](/resources/lens/concepts/) and [Overview](/resources/lens/overview/):
  Understand the core principles and methodologies essential for designing effective data models.

* [Best practices:](/resources/lens/best_practices/)
  Explore recommended guidelines and techniques to create efficient and scalable data models.

* [Do's and don'ts:](/resources/lens/dos_and_donts/)
  A concise list of actions to follow and pitfalls to avoid when designing your data model.

* [Error reference:](/resources/lens/errors/)
  A quick reference for understanding and resolving common errors in data modeling.

## Supported data sources

Lens integrates with a variety of data sources to streamline data analysis and management. Below is a list of supported data sources.

* [AWS Redshift](/resources/lens/data_sources/awsredshift/)

* [Bigquery](/resources/lens/data_sources/bigquery/)

* [Flash](/resources/lens/data_sources/flash/)

* [Minerva](/resources/lens/data_sources/minerva/)

* [Postgres](/resources/lens/data_sources/postgres/)

* [Themis](/resources/lens/data_sources/themis/)

## Optimizing Lens model

The Lens semantic layer provides several optimization techniques that can significantly enhance the performance of data queries. The following page explores best practices and strategies for fine-tuning your Lens model to maximize efficiency.

* [Optimizing Lens model: Best practices for the Semantic Layer](/resources/lens/fine_tuning_a_lens_model/)