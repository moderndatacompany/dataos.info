---
title: "Introduction"
description: "Lens Resource in DataOS is a logical modeling layer for accessing tabular data in data warehouses or lakehouses. It operates on top of physical tables, allowing the extension of these tables into logical tables by adding logical columns (measures) and relationships. It empowers analytical engineers, the key architects of business intelligence, with a model-first approach."
---

<Tip>
  **Lens in the Data Product Lifecycle**

  Lens operates in the consumption layer of the Data Product Life Cycle within DataOS, By leveraging Lens, Data Products can be created to inform decision-making, ensuring data is well-organized and aligned with business objectives. To consume it, Lens exposes APIs such as Postgres, REST, and GraphQL.
</Tip>

**Why Lens?**

The data modeling layer serves as an interface that overlays the underlying data, consistently presenting business users with familiar and well-defined terms like `product`, `customer`, or `revenue`. This abstraction enables users to access and consume data in a way that aligns with their understanding, facilitating self-service analytics and reducing dependence on data engineers for ad-hoc data requests.

As a resource within the DataOS ecosystem, Lens enhances Data Product consumption by delivering improvements in how Data Products are accessed and utilized. It streamlines the developer experience in consumption patterns, focusing specifically on refining the use and interaction with data products.

## Key features of Lens

Lens  is engineered to handle complex and large-scale data models with ease. Key features include:

* **Code modularity:** Lens supports modular code structures, simplifying the maintenance of extensive models, particularly when dealing with real-world entities (represented as tables), dimensions, and measures. This modularity enables efficient development and management, allowing teams to navigate large codebases with reduced complexity.

* **Segments:** [Segments](/resources/lens/working_with_segments/) are predefined filters that enable the definition of complex filtering logic in SQL. They allow you to create specific subsets of data, such as users from a particular city, which can be reused across different queries and reports. This feature helps streamline the data exploration process by simplifying the creation of reusable filters.

* **API Support:** Lens enhances interoperability by simplifying application development with support for [Postgres API](/resources/lens/exploration_of_deployed_lens_using_sql_apis/), [REST API](/resources/lens/exploration_of_deployed_lens_using_rest_apis/), and [GraphQL](/resources/lens/exploration_of_deployed_lens_using_graphql/). Additionally, learn how to [work with payloads](/resources/lens/working_with_payload/) for querying and interacting with the system in the API Documentation.

* **Governance and Access Control:** Lens ensures data governance through[ user group management and data policies](/resources/lens/working_with_user_groups_and_data_policies/), enabling precise control over who can access and interact with data models.

* **BI integration:** Lens improves interoperability through robust integration with PowerBI, Tableau and PowerBI. This ensures that data models can be easily utilized across various BI platforms, enhancing the overall analytics experience. For more details on BI integration, visit the [BI Integration Guide](/resources/lens/bi_integration/).

* **Performance optimization through Flash:** Designed to work with DataOS Lakehouse and Iceberg-format depots, [Flash](/resources/stacks/flash/) improves query performance by leveraging in-memory execution. This optimization ensures that data teams can efficiently handle large-scale queries with enhanced speed and performance.

## How to build a Semantic model?

The process begins with creating a new Lens project and generating a data model. Once the model is prepared, it will be tested within the development environment to ensure it is error-free before deployment.

<Accordion title="Single source" defaultOpen={false}>
  <Tabs>
    <Tab title="AWS Redshift">
      ## Step 1: Create the AWS Redshift Depot

      If the Depot is inactive, you must create one using the provided template.

      ```yaml
      name: ${{redshift-depot-name}}
      version: v2alpha
      type: depot
      tags:
      - ${{redshift}}
      layer: user
      description: ${{Redshift Sample data}}
      depot:
      type: REDSHIFT
      redshift:
        host: ${{hostname}}
        subprotocol: ${{subprotocol}}
        port: ${{5439}}
        database: ${{sample-database}}
        bucket: ${{tmdc-dataos}}
        relativePath: ${{development/redshift/data_02/}}
      external: ${{true}}
      secrets:
        - name: ${{redshift-instance-secret-name}}-r
          allkeys: true

        - name: ${{redshift-instance-secret-name}}-rw
          allkeys: true
      ```

      ## Step 2: Prepare the sematic model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 3: Deployment manifest file

      Once the Lens with semantic model is prepared, create a `lens_deployment.yml` file parallel to the model folder to configure the deployment using the YAML template below.

      ```yaml
      version: v1alpha
      name: "redshiftlens"
      layer: user
      type: lens
      tags:
        - lens
      description: redshiftlens deployment on lens2
      lens:
        compute: runnable-default
        secrets:
          - name: bitbucket-cred
            allKeys: true
        source:
          type: depot # source type is depot here
          name: redshiftdepot # name of the redshift depot
        repo:
          url: https://bitbucket.org/tmdc/sample
          lensBaseDir: sample/lens/source/depot/redshift/model 
          # secretId: lens2_bitbucket_r
          syncFlags:
            - --ref=lens

        api:   # optional
          replicas: 1 # optional
          logLevel: info  # optional    
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi
        worker: # optional
          replicas: 2 # optional
          logLevel: debug  # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        router: # optional
          logLevel: info  # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        iris:
          logLevel: info  
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        metric:
          logLevel: info  # Logging level for the metric component
      ```

      Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

      * **Defining the Source:**

        * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

        * **`name`:** The `name` attribute in the `source` section should specify the name of the AWS Redshift Depot created.

      * **Setting Up Compute and Secrets:**

        * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

        * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

      * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      ## Step 4: Apply the Lens deployment manifest file

      After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

      <CodeGroup>
        ```javascript Command
        dataos-ctl resource apply -f ${manifest-file-path}
        ```

        ```
        dataos-ctl apply -f ${manifest-file-path}
        ```

        ```
        dataos-ctl apply -f /lens/lens_deployment.yml -w curriculum
        # Expected output
        INFO[0000] ðŸ›  apply...                                   
        INFO[0000] ðŸ”§ applying(curriculum) sales360:v1alpha:lens... 
        INFO[0001] ðŸ”§ applying(curriculum) sales360:v1alpha:lens...created 
        INFO[0001] ðŸ›  apply...complete
        ```
      </CodeGroup>

      ## Docker compose manifest file

      <Accordion title="Click here to view the complete docker manifest" defaultOpen={false}>
        ```
        version: "2.2"

        x-lens2-environment: &lens2-environment
          # DataOS
          DATAOS_FQDN: liberal-donkey.dataos.app

          # Overview
          LENS2_NAME: ${redshiftlens}
          LENS2_DESCRIPTION: "Ecommerce use case on sales data"
          LENS2_TAGS: "lens2, ecom, sales and customer insights"
          LENS2_AUTHORS: "iamgroot"
          LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
          # Data Source
          LENS2_SOURCE_TYPE: ${depot}  
          LENS2_SOURCE_NAME: ${redshiftdepot}
          LENS2_SOURCE_CATALOG_NAME: ${redshiftdepot}

          # Log
          LENS2_LOG_LEVEL: error
          CACHE_LOG_LEVEL: "trace"
          # Operation
          #LENS_DB_QUERY_TIMEOUT: 15m
          LENS2_DEV_MODE: true
          LENS2_DEV_MODE_PLAYGROUND: false
          LENS2_REFRESH_WORKER: true
          LENS2_SCHEMA_PATH: model
          LENS2_PG_SQL_PORT: 5432
          CACHE_DATA_DIR: "/var/work/.store"
          NODE_ENV: production
          LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
        services:
          api:
            restart: always
            image: rubiklabs/lens2:0.35.41-02
            ports:
              - 4000:4000
              - 25432:5432
              - 13306:13306
            environment:
              <<: *lens2-environment   
            volumes:
              - ./model:/etc/dataos/work/model
        ```
      </Accordion>

      ## Check query statistics for AWSRedshift

      > Note: Ensure the user has AWS console access before proceeding.

      <Steps>
        <Step title="Log in to AWS Console and access Redshift">
          Login to the AWS Console and  search 'Redshift' in the AWS Console search bar.

          ![](/resources/lens/data_sources/awsredshift/Untitled1.png)
        </Step>

        <Step title=" Select Redshift Cluster">
          Click on 'Amazon Redshift' from the search results. You will be directed to the Redshift dashboard.  Select the appropriate region and choose the desired cluster name from the list.

          ![](/resources/lens/data_sources/awsredshift/Untitled2.png)
        </Step>

        <Step title="Access Query Monitoring">
          Select the cluster you want to monitor. Navigate to the 'Query monitoring' tab to view query statistics.

          ![](/resources/lens/data_sources/awsredshift/Untitled3.png)
        </Step>

        <Step title=" View running and completed queries">
          In the 'Query monitoring' tab, you will see a list of running and completed queries.

          ![](/resources/lens/data_sources/awsredshift/Untitled4.png)
        </Step>

        <Step title="Monitor specific query">
          Click on the query you want to monitor. View the query statistics, as shown in the example below.

          ![](/resources/lens/data_sources/awsredshift/Untitled5.png)
        </Step>
      </Steps>
    </Tab>

    <Tab title="Bigquery">
      ## Step 1: Create the Bigquery Depot

      If the Depot is not active, you need to create one using the provided template.

      ```yaml
      name: ${{bigquerydepot}}
      version: v2alpha
      type: depot
      tags:
        - ${{dropzone}}
        - ${{bigquery}}
      owner: ${{owner-name}}
      layer: user
      depot:
        type: BIGQUERY                 
            description: ${{description}} # optional
        external: ${{true}}
        secrets:
          - name: ${{bq-instance-secret-name}}-r
            allkeys: true

          - name: ${{bq-instance-secret-name}}-rw
            allkeys: true
        bigquery:  # optional                         
          project: ${{project-name}} # optional
          params: # optional
            ${{"key1": "value1"}}
            ${{"key2": "value2"}}
      ```

      ## Step 2: Prepare the Lens model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 3: Deployment manifest file

      After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

      ```yaml
      # RESOURCE META SECTION
      version: v1alpha # Lens manifest version (mandatory)
      name: "bigquery-lens" # Lens Resource name (mandatory)
      layer: user # DataOS Layer (optional)
      type: lens # Type of Resource (mandatory)
      tags: # Tags (optional)
        - lens
      description: bigquery depot lens deployment on lens2 # Lens Resource description (optional)

      # LENS-SPECIFIC SECTION
      lens:
        compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
        secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
          - name: bitbucket-cred # Referred Instance Secret name (mandatory)
            allKeys: true # All keys within the secret are required or not (optional)

        source: # Data Source configuration
          type: depot # Source type is depot here
          name: bigquerydepot # Name of the bigquery depot

        repo: # Lens model code repository configuration (mandatory)
          url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
          lensBaseDir: sample/lens/source/depot/bigquery/model # Relative path of the Lens 'model' directory in the repository (mandatory)
          syncFlags: # Additional flags used during synchronization, such as specific branch.
            - --ref=lens # Repository Branch

        api: # API Instances configuration (optional)
          replicas: 1 # Number of API instance replicas (optional)
          logLevel: info  # Logging granularity (optional)
          resources: # CPU and memory configurations for API Instances (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi

        worker: # Worker configuration (optional)
          replicas: 2 # Number of Worker replicas (optional)
          logLevel: debug # Logging level (optional)
          resources: # CPU and memory configurations for Worker (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        router: # Router configuration (optional)
          logLevel: info  # Level of log detail (optional)
          resources: # CPU and memory resource specifications for the router (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        iris:
          logLevel: info # Level of log detail (optional)
          resources: # CPU and memory resource specifications for the iris board (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        metric:    #optional
          logLevel: info
      ```

      Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

      * **Defining the Source:**

        * **Source type:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

        * **Source name:** The `name` attribute in the `source` section should specify the name of the Bigquery Depot created .

      * **Setting Up Compute and Secrets:**

        * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

        * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/bigquery/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub) . It specifies the secret needed to securely authenticate and access the repository.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model rsides in the dev branch.

      * **Configuring API, Worker and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      ## Step 4: Apply the Lens deployment manifest file

      After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

      <CodeGroup>
        ```bash Command
        dataos-ctl resource apply -f ${manifest-file-path}
        ```

        ```bash Alternative command
        dataos-ctl apply -f ${manifest-file-path}
        ```

        ```bash Example usage 
        dataos-ctl apply -f /lens/lens_deployment.yml -w curriculum
        # Expected output
        INFO[0000] ðŸ›  apply...                                   
        INFO[0000] ðŸ”§ applying(curriculum) sales360:v1alpha:lens... 
        INFO[0001] ðŸ”§ applying(curriculum) sales360:v1alpha:lens...created 
        INFO[0001] ðŸ›  apply...complete
        ```
      </CodeGroup>
    </Tab>

    <Tab title="Postgres">
      ## Step 1: Create Postgres Depot

      If the Depot is not active, you need to create one using the provided template.

      ```yaml
      name: ${{postgresdb}}
      version: v2alpha
      type: depot
      layer: user
      depot:
        type: JDBC                  
        description: ${{To write data to postgresql database}}
        external: ${{true}}
        secrets:
          - name: ${{sf-instance-secret-name}}-r
            allkeys: true

          - name: ${{sf-instance-secret-name}}-rw
            allkeys: true
        postgresql:                        
          subprotocol: "postgresql"
          host: ${{host}}
          port: ${{port}}
          database: ${{postgres}}
          params: #Required 
            sslmode: ${{disable}}
      ```

      ## Step 2: Prepare the Lens model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each sub folder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      While creating Lens on Postgres Depot the following aspects need to be considered:

      * The SQL dialect used in the `model/sql` folder to load data from the Postgres source should be of the Postgres dialect.

      * The table naming in the `model/table`  should be of the format: `schema.table`.

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model. By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 4: Apply the Lens deployment manifest file

      After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

      <CodeGroup>
        ```bash Command
        dataos-ctl resource apply -f ${manifest-file-path}
        ```

        ```bash Alternative command
        dataos-ctl apply -f ${manifest-file-path}
        ```

        ```bash Example usage 
        dataos-ctl apply -f /lens/lens_deployment.yml -w curriculum
        # Expected output
        INFO[0000] ðŸ›  apply...                                   
        INFO[0000] ðŸ”§ applying(curriculum) sales360:v1alpha:lens... 
        INFO[0001] ðŸ”§ applying(curriculum) sales360:v1alpha:lens...created 
        INFO[0001] ðŸ›  apply...complete
        ```
      </CodeGroup>
    </Tab>

    <Tab title="Snowflake">
      ## Prerequisite

      CLI Version should be `dataos-cli 2.26.39-dev` or greater.

      ## Step 1: Create the Snowflake Depot

      If the Depot is not active, you need to create one using the provided template.

      ```yaml
      name: snowflake-depot
      version: v2alpha
      type: depot
      tags:
        - Snowflake depot
        - user data
      layer: user
      depot:
        name: sftest
        type: snowflake
        description: Depot to fetch data from Snowflake datasource
        secrets:
          - name: sftest-r
            keys:
              - sftest-r
            allKeys: true
          - name: sftest-rw
            keys:
              - sftest-rw
            allKeys: true
        external: true
        snowflake:
          database: TMDC_V1
          url: ABCD23-XYZ8932.snowflakecomputing.com
          warehouse: COMPUTE_WH
          account: ABCD23-XYZ8932
        source: sftest
      ```

      ## Step 2: Prepare the sematic model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 3: Deployment manifest file

      After setting up the Lens model folder, the next step is to configure the deployment manifest. Below is the YAML template for configuring a Lens deployment.

      ```yaml
      # RESOURCE META SECTION
      version: v1alpha # Lens manifest version (mandatory)
      name: "snowflake-lens" # Lens Resource name (mandatory)
      layer: user # DataOS Layer (optional)
      type: lens # Type of Resource (mandatory)
      tags: # Tags (optional)
        - lens
      description: snowflake depot lens deployment on lens2 # Lens Resource description (optional)

      # LENS-SPECIFIC SECTION
      lens:
        compute: runnable-default # Compute Resource that Lens should utilize (mandatory)
        secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
          - name: bitbucket-cred # Referred Instance Secret name (mandatory)
            allKeys: true # All keys within the secret are required or not (optional)

        source: # Data Source configuration
          type: depot # Source type is depot here
          name: snowflake-depot # Name of the snowflake depot

        repo: # Lens model code repository configuration (mandatory)
          url: https://bitbucket.org/tmdc/sample # URL of repository containing the Lens model (mandatory)
          lensBaseDir: sample/lens/source/depot/snowflake/model # Relative path of the Lens 'model' directory in the repository (mandatory)
          syncFlags: # Additional flags used during synchronization, such as specific branch.
            - --ref=lens # Repository Branch

        api: # API Instances configuration (optional)
          replicas: 1 # Number of API instance replicas (optional)
          logLevel: info  # Logging granularity (optional)
          resources: # CPU and memory configurations for API Instances (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi

        worker: # Worker configuration (optional)
          replicas: 2 # Number of Worker replicas (optional)
          logLevel: debug # Logging level (optional)
          resources: # CPU and memory configurations for Worker (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        router: # Router configuration (optional)
          logLevel: info  # Level of log detail (optional)
          resources: # CPU and memory resource specifications for the router (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        iris:
          logLevel: info # Level of log detail (optional)
          resources: # CPU and memory resource specifications for the iris board (optional)
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        metric:    #optional
          logLevel: info
      ```

      Each section of the YAML template defines key aspects of the Lens deployment. Below is a detailed explanation of its components:

      * **Defining the Source:**

        * **Source type:**  The `type` attribute in the `source` section must be explicitly set to `depot`.

        * **Source name:** The `name` attribute in the `source` section should specify the name of the snowflake Depot created.

      * **Setting Up Compute and Secrets:**

        * Define the compute settings, such as which engine (e.g., `runnable-default`) will process the data.

        * Include any necessary secrets (e.g., credentials for Bitbucket or AWS) for secure access to data and repositories.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/snowflake/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub) . It specifies the secret needed to securely authenticate and access the repository.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model rsides in the dev branch.

      * **Configuring API, Worker and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      ## Step 4: Apply the Lens deployment manifest file

      After configuring the deployment file with the necessary settings and specifications, apply the manifest using the following command:

      <CodeGroup>
        ```bash Command
        dataos-ctl resource apply -f ${manifest-file-path}
        ```

        ```bash Alternative command
        dataos-ctl apply -f ${manifest-file-path}
        ```

        ```bash Example usage 
        dataos-ctl apply -f /lens/lens_deployment.yml -w curriculum
        # Expected output
        INFO[0000] ðŸ›  apply...                                   
        INFO[0000] ðŸ”§ applying(curriculum) sales360:v1alpha:lens... 
        INFO[0001] ðŸ”§ applying(curriculum) sales360:v1alpha:lens...created 
        INFO[0001] ðŸ›  apply...complete
        ```
      </CodeGroup>
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Multi source" defaultOpen={false}>
  <Tabs>
    <Tab title="Minerva ">
      If you have data from multiple sources and want to create a semantic model using all these sources, configure your Lens project through the Minerva cluster, as it support depots from all sources, such as Redshift, and Snowflake.

      ## Step 1: Prepare the sematic model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 2: Create a deployment manifest file

      After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

      ```yaml
      version: v1alpha
      name: "minervalens"
      layer: user
      type: lens
      tags:
        - lens
      description: minerva deployment on lens2
      lens:
        compute: runnable-default
        secrets:
          - name: bitbucket-cred
            allKeys: true
        source:
          type: minerva #minerva/themis/depot
          name: minervacluster  #name of minerva cluster
          catalog: icebase
        repo:
          url: https://bitbucket.org/tmdc/sample
          lensBaseDir: sample/lens/source/minerva/model 
          # secretId: lens2_bitbucket_r
          syncFlags:
            - --ref=lens

        api:   # optional
          replicas: 1 # optional
          logLevel: info  # optional 
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi

        worker: # optional
          replicas: 2 # optional
          logLevel: debug  # optional

          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi

        router: # optional
          logLevel: info  # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        iris:
          logLevel: info  
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
      ```

      The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

      Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

      * **Defining the Source:**

        * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `minerva`.

        * **`name`:** The `name` attribute in the `source` section should specify the name of the Minerva Cluster. For example, if the name of your Minerva Cluster is miniature the Source name would be `miniature`.

        * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Minerva Cluster that you intend to use. For instance, if the catalog is named icebase, ensure this is accurately reflected in the catalog field.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

      * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      <Info>
        Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
      </Info>

      ## Docker compose manifest file

      <Accordion title="Click here to view the complete docker compose  manifest and it's procedure" defaultOpen={false}>
        ```
        Prerequisites
        The hostname for the Trino database server.
        The username for the DataOS User.
        The name of the database to use with the Minerva query engine database server.
        Docker compose configuration
        Add the following environment variables to your Lens (.env) file
        Environment variables attribute
        Example
        trino.zip

        ```
      </Accordion>

      ### Check Query Stats for Minerva

      To check the query statistics, please follow the steps below:

      **1. Access Minerva queries:** Navigate to the operation section and then to Minerva queries. Set the filters as follows:

      <Frame>
        <img src="/resources/lens/data_sources/minerva/Untitled1.png" />
      </Frame>

      * Source: `lens2`

      * Dialect: `trino_sql`

      * You can also filter by cluster, username, and other criteria as per your choice.

      **2. Select the query ID:** Choose the query ID you are interested in. You will then be able to check the statistics, as shown in the example below:

      <Frame>
        <img src="/resources/lens/data_sources/minerva/Untitled1.png" />
      </Frame>
    </Tab>

    <Tab title="Themis">
      If you have data from multiple sources and want to create a semantic model using all these sources, configure your Lens project through the Minerva cluster, as it support depots from all sources, such as Redshift, and Snowflake.

      ## Step 1: Prepare the sematic model folder

      In the Model folder, the Lens semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens  template to quickly get started.

      [lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

      <Steps>
        <Step title="Load data from the data source" titleSize="h3">
          In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

          For example, a simple data load might look like this:

          ```sql
          SELECT
            *
          FROM
            "icebase"."sales_360".channel;
          ```

          Alternatively, you can write more advanced queries that include transformations, such as:

          ```sql
          SELECT
            CAST(customer_id AS VARCHAR) AS customer_id,
            first_name,
            CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
            age,
            CAST(register_date AS TIMESTAMP) AS register_date,
            occupation,
            annual_income,
            city,
            state,
            country,
            zip_code
          FROM
            "icebase"."sales_360".customer;
          ```
        </Step>

        <Step title="Define the Table in the Model" titleSize="h2">
          Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

          ```yaml
          table:
            - name: customers
              sql: {{ load_sql('customers') }}
              description: Table containing information about sales transactions.
          ```

          ### Add Dimensions and Measures

          After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look like this:

          ```yaml
          tables:
            - name: sales
              sql: {{ load_sql('sales') }}
              description: Table containing sales records with order details.

              dimensions:
                - name: order_id
                  type: number
                  description: Unique identifier for each order.
                  sql: order_id
                  primary_key: true
                  public: true

              measures:
                - name: total_orders_count
                  type: count
                  sql: id
                  description: Total number of orders.
          ```

          ### Add Segments to filter

          Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment like this:

          ```yaml
          segments:
            - name: state_filter
              sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
          ```

          To know more about Segments click [here](https://dataos.info/resources/lens/working_with_segments/).
        </Step>

        <Step title="Create the Views" titleSize="h2">
          Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

          ```yaml
          views:
            - name: customer_churn_prediction
              description: Contains customer churn information.
              tables:
                - join_path: marketing_campaign
                  includes:
                    - engagement_score
                    - customer_id
                - join_path: customer
                  includes:
                    - country
                    - customer_segments
          ```

          To know more about the Views click [here](https://dataos.info/resources/lens/working_with_views/).
        </Step>

        <Step title="Create User Groups" titleSize="h2">
          This YAML manifest file is used to manage access levels for the Lens semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

          ```yaml
          user_groups:
            - name: default
              description: this is default user group
              includes: "*"
          ```

          To know more about the User Groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
        </Step>
      </Steps>

      ## Step 2: Create a deployment manifest file

      After preparing the Lens semantic model create a `lens_deployemnt.yml` parallel to the `model` folder.

      ```yaml
      version: v1alpha
      name: "themis-lens"
      layer: user
      type: lens
      tags:
        - lens
      description: themis lens deployment on lens2
      lens:
        compute: runnable-default
        secrets:
          - name: bitbucket-cred
            allKeys: true
        source:
          type: themis #minerva/themis/depot
          name: lenstestingthemis
          catalog: icebase
        repo:
          url: https://bitbucket.org/tmdc/sample
          lensBaseDir: sample/lens/source/themis/model 
          # secretId: lens2_bitbucket_r
          syncFlags:
            - --ref=main #repo-name

        api:   # optional
          replicas: 1 # optional
          logLevel: info  # optional    
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 2000m
              memory: 2048Mi
        worker: # optional
          replicas: 2 # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        router: # optional
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
        iris:
          logLevel: info  
          resources: # optional
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 6000m
              memory: 6048Mi
      ```

      The YAML manifest provided is designed for a cluster named `minervacluster`, created on the `Minerva` source, with a data catalog named `icebase`. To utilize this manifest, duplicate the file and update the source details as needed.

      Each section of the YAML template outlines essential elements of the Lens deployment. Below is a detailed breakdown of its components:

      * **Defining the Source:**

        * **`type`:**  The `type` attribute in the `source` section must be explicitly set to `themis`.

        * **`name`:** The `name` attribute in the `source` section should specify the name of the Themis Cluster. For example, if the name of your Themis Cluster is `clthemis` the Source name would be `clthemis`.

        * **`catalog`:** The `catalog` attribute must define the specific catalog name within the Themis Cluster that you intend to use. For instance, if the catalog is named `lakehouse_retail`, ensure this is accurately reflected in the catalog field.

      * **Defining Repository:**

        * **`url`** The `url` attribute in the repo section specifies the Git repository where the Lens model files are stored. For instance, if your repo name is lensTutorial then the repo `url` will be  [https://bitbucket.org/tmdc/lensTutorial](https://bitbucket.org/tmdc/lensTutorial)

        * **`lensBaseDir`:**  The `lensBaseDir` attribute refers to the directory in the repository containing the Lens model. Example: `sample/lens/source/depot/awsredshift/model`.

        * **`secretId`:**  The `secretId` attribute is used to access private repositories (e.g., Bitbucket, GitHub). It specifies the secret needed to authenticate and access the repository securely.

        * **`syncFlags`**:  Specifies additional flags to control repository synchronization. Example: `--ref=dev` specifies that the Lens model resides in the dev branch.

      * **Configure API, Worker, and Metric Settings (Optional):** Set up replicas, logging levels, and resource allocations for APIs, workers, routers, and other components.

      The above manifest is intended for a cluster named `lenstestingthemis`, created on the themis source, with the depot or data catalog named `icebase`. To use this manifest, copy the file and update the source details accordingly.

      <Info>
        Within the Themis and Minerva cluster, all depots (such as Icebase, Redshift, Snowflake, etc.) are integrated. When configuring Lens, you only need to specify one depot in the \`catalog\` field, as Lens can connect to and utilize depots from all sources available in the Themis cluster.
      </Info>

      ## Docker compose manifest file

      <Accordion title="Click here to see the complete docker compose" defaultOpen={false}>
        ```
        LENS2_VERSION=0.34.60-10-scratch2
        LENS2_CACHE_VERSION=0.34.60-amd64v8

        # source
        LENS2_DB_HOST=tcp.alpha-omega.dataos.app
        LENS2_DB_PORT=7432
        LENS2_DB_USER=iamgroot
        LENS2_DB_PASS=mypassword
        LENS2_DB_PRESTO_CATALOG=adventureworks
        LENS2_DB_SSL=true
        LENS2_DB_TYPE=themis

        LENS2_NAME=themislens
        LENS2_DESCRIPTION="Ecommerce use case on sales data"
        LENS2_TAGS="lens2, ecom, sales and customer insights"
        LENS2_AUTHORS="iamgroot, thor"
        LENS2_LOG_LEVEL=trace
        LENS2_LOADER_LOG_LEVEL=debug

        LENS2_HEIMDALL_BASE_URL="https://alpha-omega.dataos.app/heimdall"

        LENS2_SCHEDULED_REFRESH_DEFAULT="false"
        LENS2_API_SECRET=1245ABDJSJDIR56

        CACHE_TELEMETRY=false
        CACHE_LOG_LEVEL=error
        LENS2_BOARD_PATH=boards
        LENS2_BASE_URL="http://localhost:4000/lens2/api"
        LENS2_META_PATH="/v2/meta"
        LENS2_RILL_PATH=boards
        LENS2_CHECKS_PATH=checks

        DATAOS_USER_APIKEY="abcdefghijklmnopqrstuvwxyz"
        DATAOS_USER_NAME="iamgroot"

        LENS2_LOCAL_PG_DB_NAME=db
        LENS2_LOCAL_PG_HOST=localhost
        LENS2_LOCAL_PG_PORT=15432
        LENS2_LOCAL_PG_PASSWORD="abcdefghijklmnopqrstuvwxyz"
        LENS2_LOCAL_PG_USER=iamgroot
        ```
      </Accordion>

      ## Check Query Stats for Themis

      <Info>
        Please ensure you have the required permission to access the Operations.
      </Info>

      To check the query statistics, please follow the steps below:

      1. **Access the Themis Cluster:** Navigate to the Themis cluster. You should see a screen similar to the image below:

      2. **Select the Running Driver:** Choose the running driver. **This driver will always be the same, regardless of the user, as queries will be directed to the creator of the Themis cluster**. The running driver remains consistent for all users.

      3. **View the Spark UI:** Go to terminal and use the following command to view the spark UI:

      ```yaml
      dataos-ctl -t cluster -w public -n themislens --node themis-themislens-iamgroot-default-a650032d-ad6b-4668-b2d2-cd372579020a-driver view sparkui

      dataos-ctl -t cluster -w public -n themis_cluster_name --node  driver_name view sparkui
      ```

      You should see the following interface:

      ![](/resources/lens/data_sources/Themis/Untitled\(9\).png)
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Query acceleration: Flash" defaultOpen={false}>
  <Info>
    Flash serves query optimization to the Lens.
  </Info>

  ## Prerequisites

  To create a Lens using Flash Service, ensure that the Flash Service is running as given in the below manifest. Ensure to replace the placeholders and modify the configurations as needed.

  ## Example Service manifest file

  Below is an example of the YAML configuration for the Flash Service:

  ```yaml
  name: flash-service-lens
  version: v1
  type: service
  tags:
    - service
  description: flash service
  workspace: curriculum
  service:
    servicePort: 5433
    replicas: 1
    logLevel: info
    compute: runnable-default
    resources:
      requests:
        cpu: 1000m
        memory: 1024Mi
    stack: flash+python:2.0
    stackSpec:
      datasets:
        - address: dataos://icebase:sales360/f_sales    #view
          name: sales
      
        - address: dataos://icebase:sales360/customer_data_master
          name: customer_data_master
      
        - address: dataos://icebase:sales360/site_check1
          name: site_check1
      
        - address: dataos://icebase:sales360/product_data_master
          name: product_data_master
      
      init:
        - create or replace table f_sales as (select * from sales)  #table
        - create or replace table m_customers as (select * from customer_data_master)
        - create or replace table m_sites as (select * from site_check1)
        - create or replace table m_products as (select * from product_data_master)
  ```

  ### **How does this process work?**

  The flow of Flash operates as follows:

  * **Data Loading:** The `datasets` the attribute specifies the depot `address` of the source data to be loaded into Flash. A dataset `name` is also provided, which Flash uses to generate a view of the source data.

  * **View Creation:** Flash creates a view based on the assigned name, allowing for interaction with the source data without directly querying it.

  * **Table Creation:** Specific columns from the generated view can be selected to define tables for further operations using `init` attribute.

  * **Usage in Lens Model(SQL):** The tables created through the `init` the attribute is used in SQL queries within Lens.

    For example, in the manifest referenced, the `f_sales` table is first loaded from the source, and a view named `sales` is created. A table called `f_sales` is then defined using this sales view. This table is then referenced in SQL models within Lens.

    > <b>Note</b> Flash directly uses the deployment.yml manifest file to create a Lens.

    ## Deployment manifest file

    ```yaml
    version: v1alpha
    name: "lens-flash-test-99"
    layer: user
    type: lens
    tags:
      - lens
    description: A sample lens that contains three entities, a view and a few measures for users to test
    lens:
      compute: runnable-default
      secrets: # Referred Instance-secret configuration (**mandatory for private code repository, not required for public repository)
        - name: githubr # Referred Instance Secret name (mandatory)
          allKeys: true # All keys within the secret are required or not (optional)
      source:
        type: flash # minerva, themis and depot
        name: flash-service-lens # flash service name
      repo:
        url: https://github.com/tmdc/sample    # repo address
        lensBaseDir: sample/source/flash/model     # location where lens models are kept in the repo
        syncFlags:
          - --ref=main
      api:
        replicas: 1
        logLevel: debug
        envs:
          LENS2_SOURCE_WORKSPACE_NAME: curriculum
          LENS2_SOURCE_FLASH_PORT: 5433
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 2000m
            memory: 2048Mi

      worker:
        replicas: 1
        logLevel: debug
        envs:
          LENS2_SOURCE_WORKSPACE_NAME: curriculum
          LENS2_SOURCE_FLASH_PORT: 5433
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      router:
        logLevel: info
        envs:
          LENS2_SOURCE_WORKSPACE_NAME: curriculum
          LENS2_SOURCE_FLASH_PORT: 5433
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi

      iris:
        logLevel: info  
        envs:
          LENS2_SOURCE_WORKSPACE_NAME: curriculum
          LENS2_SOURCE_FLASH_PORT: 5433
        resources: # optional
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 6000m
            memory: 6048Mi
    ```

    ### **Key configurations**

    #### **`Source`**

    * **`type`** The source section specifies that Flash is used as the data source (`type: flash`). This indicates that data for the Lens model will be loaded from the Flash service.

    * **`name`**: The Flash service is identified by the `name` attribute, here it is flash-service-lens. This name should match the deployed Flash service used for data ingestion.

    #### **`envs`**

    The following environment variables are defined under multiple components, including api, worker, router, and iris

    * **`LENS2_SOURCE_WORKSPACE_NAME`**  It refers to the workspace where the Flash service is deployed.

    * **`LENS2_SOURCE_FLASH_PORT`** The port number `5433` is specified for the Flash service. This port is used by Lens to establish communication with the Flash service. It ensures that all componentsâ€”API, worker, router, and irisâ€”can access the Flash service consistently.
</Accordion>

## Configurations

Lens can be configured to connect to different sources using data source attributes and configurable attributes in the `docker-compose.yml` or `lens.yml` manifest files. Here is a comprehensive guide to APIs and configuring supported properties.

* [Configuration Fields of the Deployment Manifest File (YAML) for Lens Resource](/resources/lens/lens_manifest_attributes/)
  Understand the various configuration fields available in the deployment manifest file for Lens resources.

* [Configuration Fields of the Docker Compose File](/resources/lens/docker_compose_manifest_attributes/)
  Review the configuration fields and settings in the Docker Compose file for orchestrating multi-container applications.

<Info>
  ðŸ—£ï¸ If working with Lens 1.0 interface, clickÂ [here](https://dataos.info/interfaces/lens/).
</Info>

## How to consume the semantic model?

After creating a Lens data model, the next step is to consume itâ€”this means interacting with the model by running queries. The following section explains the key concepts for querying Lens through various methods, though all queries follow the same general format. Multiple ways are available to explore or interact with the Lens model or its underlying data, allowing you to ask meaningful questions of the data and retrieve valuable insights. Exploration can be performed using the following methods:

<AccordionGroup>
  <Accordion title="BI tools" defaultOpen={false}>
    <Tabs>
      <Tab title="PowerBI">
        <Tabs>
          <Tab title="Using Curl">
            ## Prerequisites

            * **Curl**: Ensure you haveÂ `curl`Â installed on your system. Windows users may need to useÂ `curl.exe`.

            * **Lens API endpoint**: The API endpoint provided by Lens to sync semantic model, enabling integration with Power BI. It will be of the following format:

            ```
            https://<DATAOS_FQDN>/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name> 
            ```

            * **Access credentials**: You will need access credentials such as username, password, and host for Power BI.

            * **DataOS API key**: Ensure you have your DataOS API key. The API key can be obtained by executing the command below.

            ```
            dataos-ctl user apikey get
            ```

            **Curl Command**

            Following is the command to integrate the semantic model with Power BI.

            <CodeGroup>
              ```javascript Syntax
              curl --location --request POST '${URL}' --header 'apikey: ${APIKEY}' --output ${FILE_NAME}.zip
              ```

              ```
              curl --location --request POST 'https://liberal-monkey.dataos.app/lens2/sync/api/v1/power-bi/curriculum:sales360' --header 'apikey: abcdegfghiJk==' --output powerbi.zip 
              ```

              ```
              curl --location --request POST 'https://liberal-monkey.dataos.app/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>' --header 'apikey: ${APIKEY}' --output ${FILE_NAME}.zip 
              ```
            </CodeGroup>

            **Parameters:**

            **URL:**Â This is the API endpoint for syncing lens with PowerBI. It contains DATAOS\_FQDN, name and workspace of lens.

            ```
            https://<DATAOS_FQDN>/lens2/sync/api/v1/power-bi/<workspace_name>:<lens_name>
            ```

            1. **DATAOS\_FQDN:**Â ReplaceÂ with the current Fully Qualified Domain Name (FQDN) where the Lens is deployed. For example, `liberal-monkey.dataos.app`,. is the FQDN andÂ `liberal monkey`Â is the context name.

            2. **WORKSPACE\_NAME:**Â ReplaceÂ with the actual workspace where Lens has been deployed. for e.g.,Â `public`,Â `sandbox`,Â `curriculum`.

            3. **LENS\_NAME:**Â The name of the Lens model to be synced with Tableau. For example `sales360`.

            **API Key:** The DataOS API Key for the user  can be obtained by executing the command below.
            [](https://dataos.info/resources/lens/bi_integration/powerbi/#__codelineno-3-1)

            ```
            dataos-ctl user apikey get
            ```

            **Output:**Â AÂ `file.zip`Â archive is downloaded, containing the main components of a Power BI project. The name of the zip file can be specified during the curl command execution, and it will be saved accordingly.

            TheÂ `file.zip`Â includes essential components for syncing a Lens Model with Power BI, organized into folders such asÂ `.Report`Â andÂ `.SemanticModel`:

            * **public\_sales360-table.Report:**Â This folder contains theÂ `definition.pbir`Â file, which is related to the report definition in Power BI. These files define the visual representation of data, such as tables and charts, without storing the actual data. They link the semantic model and data sources to create report views.

            * **public-sales360-table.SemanticModel:**Â This folder includes files that establish the underlying data model for a Power BI project. The Semantic Model is crucial for managing data interactions, including the setup of relationships, hierarchies, and measures.

              * **definition.bism:**Â This file represents the Business Intelligence Semantic Model (BISM). It defines the structure of the data, detailing data sources, relationships, tables, and measures. TheÂ `.bism`Â file contains essential metadata that enables Power BI to understand and query the data, forming the foundation of the data model for report creation and analysis.

              * **model.bim:**Â TheÂ `.bim`Â file is utilized to generate queries and manage interactions with the dataset. This semantic model is referenced to ensure the correct structure is applied to the data during report or dashboard creation in Power BI.

            * **public-sales-360-table.pbip:**Â This file serves as a Power BI project template or configuration file. Files such asÂ `.pbip`Â orÂ `.pbix`Â encapsulate reports, datasets, and visualizations. TheÂ `.pbip`Â file integrates the semantic model and report definitions from the other folders, acting as the entry point for project work in Power BI Desktop or the Power BI service.

            ## Steps

            To begin syncing a Lens model, the following steps should be followed:

            <Steps>
              <Step title=" Run the curl command:">
                Â For example, if the lens namedÂ `sales360`Â is located in theÂ `public`Â workspace deployed on theÂ `liberal-monkey`Â context, the curl command would be:

                ```
                curl --location --request POST 'https://tcp.liberal-monkey.dataos.app/lens2/sync/api/v1/power-bi/public:sales360' --header 'apikey: abcdefgh==' --output file.zip
                ```
              </Step>

              <Step title="Download the zip file:">
                Once the command is executed, a zip file will be downloaded to the specified directory. The downloaded file should be unzipped. Three folders will be found inside, all of which are necessary for semantic synchronization with Power BI.

                ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi2.png)
              </Step>

              <Step title="Open the PowerBI file">
                Open the Power BI file in Power BI Desktop. Upon opening, you will be prompted to provide credentials. Enter your DataOS username and API key to proceed.

                ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi6.png)
              </Step>

              <Step title="Connect to DataOS:">
                Â Click on the connect button. A popup will appear. Click Ok.

                ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi6.png)
              </Step>

              <Step title="Access tables with dimensions and measures:">
                Upon successful connection, tables and views will be accessible, displaying dimensions and measures. Now, you would be able to create dashboards in the Power BI.

                ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/powerbi7.png)
              </Step>
            </Steps>

            ## Important considerations

            * In Power BI, measures typically have an 'm\_' prefix to indicate they represent a measure. For example, a measure calculating total revenue might be namedÂ `m_total_revenue`.

            * The connection is live, meaning any changes to the underlying data will be reflected in Power BI.

            * If schema changes occur, such as the addition of new dimensions and measures, the steps outlined above will need to be repeated.

            ## Best practices

            Adhering to best practices ensures that you effectively utilize the Data Product Hub and maintain compatibility with the latest features and updates. Following these guidelines will help optimize your workflow, enhance performance, and prevent potential issues.

            ### **Version compatibility**

            * Power BI versions released afterÂ **June 15, 2023**, support .pbib files. It is advisable to use a version released after this date.

            * Beginning with Version 2.132.908.0 (August 2024), .pbip files have moved from preview to general availability. This transition allows for the use of .pbip files without the need to enable any preview settings. It is strongly recommended to download Power BI Version 2.132.908.0 or later to fully utilize .pbip files. In earlier versions, enabling a preview feature was necessary, but this is no longer required in the latest version.

            ### **File handling**

            Ensure thatÂ `.pbip`Â folders are fully extracted before opening them. Failure to do so may result in missing file errors, as shown below:

            ![Superset Configuration](https://dataos.info/resources/lens/bi_integration/image.png)

            ### **Data retrieval and field selection considerations**

            * **Row Limit:**Â The Lens API has a maximum return limit of 50,000 rows per request. To obtain additional data, it is necessary to set an offset. This row limit is in place to manage resources efficiently and ensure optimal performance.

            * **Selection:**Â It is important to select fields from tables that are directly related or logically joined, as the system does not automatically identify relationships between tables through transitive joins. Selecting fields from unrelated tables may result in incorrect or incomplete results.

            ### **Data policies and security**

            Data masking, restrictions, or permissions established by the publisher are automatically enforced for all report viewers, ensuring consistent data security and compliance. The behavior of these data policies, such as masking, may vary based on the user of the Power BI desktop.

            ### **Regular testing and validation**

            Regular testing and validation of reports are recommended after changes are made to the Lens definitions. This practice ensures that updates to dimensions, measures, or data models are accurately reflected in the reports and helps identify any issues early in the process.
          </Tab>

          <Tab title="Using Data Product Hub" />
        </Tabs>
      </Tab>

      <Tab title="Tableau" />
    </Tabs>
  </Accordion>

  <Accordion title="GraphQL" defaultOpen={false} />

  <Accordion title="Python" defaultOpen={false} />

  <Accordion title="SQL APIs" defaultOpen={false} />

  <Accordion title="REST APIs" defaultOpen={false} />
</AccordionGroup>

## Data modelling

[Data modeling](/resources/lens/overview/) is the process of defining and structuring raw data into organized and meaningful business definitions. It involves creating logical schemas, relationships, and aggregations to represent how data is stored, processed, and accessed. Effective data modeling ensures optimal performance for queries and allows users to extract valuable insights without modifying the underlying data structure. Below are resources to guide you through essential aspects of data modeling to optimize performance and accuracy.

* [Data modelling concepts](/resources/lens/concepts/) and [Overview](/resources/lens/overview/):
  Understand the core principles and methodologies essential for designing effective data models.

* [Best practices:](/resources/lens/best_practices/)
  Explore recommended guidelines and techniques to create efficient and scalable data models.

* [Do's and don'ts:](/resources/lens/dos_and_donts/)
  A concise list of actions to follow and pitfalls to avoid when designing your data model.

* [Error reference:](/resources/lens/errors/)
  A quick reference for understanding and resolving common errors in data modeling.

## Optimizing Lens model

The Lens semantic layer provides several optimization techniques that can significantly enhance the performance of data queries. The following page explores best practices and strategies for fine-tuning your Lens model to maximize efficiency.

* [Optimizing Lens model: Best practices for the Semantic Layer](/resources/lens/fine_tuning_a_lens_model/)