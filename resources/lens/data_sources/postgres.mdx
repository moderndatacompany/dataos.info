---
title: "Postgres "
description: "The following document guides to build the semantic model and deploy the Lens on Postgres Source."
---

## Step 1: Create Postgres Depot

If the Depot is not active, create one using the provided template.

```yaml
name: ${{postgresdb}}
version: v2alpha
type: depot
layer: user
depot:
  type: JDBC                  
  description: ${{To write data to postgresql database}}
  external: ${{true}}
  secrets:
    - name: ${{sf-instance-secret-name}}-r
      allkeys: true

    - name: ${{sf-instance-secret-name}}-rw
      allkeys: true
  postgresql:                        
    subprotocol: "postgresql"
    host: ${{host}}
    port: ${{port}}
    database: ${{postgres}}
    params: #Required 
      sslmode: ${{disable}}
```

While creating Lens on Postgres Depot the following aspects need to be considered:

* The SQL dialect used in the `model/sql` folder to load data from the Postgres source should be of the Postgres dialect.

* The table naming in the `model/table` should be of the format: `schema.table`.

## Step 2: Prepare the semantic model folder

In the Model folder, the semantic model will be defined, encompassing SQL mappings, logical tables, logical views, and user groups. Each subfolder contains specific files related to the Lens model. You can download the Lens template to quickly get started.

[lens template](/resources/lens/lens_model_folder_setup/lens-project-template.zip)

<Steps>
  <Step title="Load data from the data source" titleSize="h2">
    In the `sqls` folder, create `.sql` files for each logical table, where each file is responsible for loading or selecting the relevant data from the source. Ensure that only the necessary columns are extracted, and the SQL dialect is specific to the data source.

    For example, a simple data load might look as follows:

    ```sql
    SELECT
      *
    FROM
      "onelakehouse"."retail".channel;
    ```

    Alternatively, you can write more advanced queries that include transformations, such as:

    ```sql
    SELECT
      CAST(customer_id AS VARCHAR) AS customer_id,
      first_name,
      CAST(DATE_PARSE(birth_date, '%d-%m-%Y') AS TIMESTAMP) AS birth_date,
      age,
      CAST(register_date AS TIMESTAMP) AS register_date,
      occupation,
      annual_income,
      city,
      state,
      country,
      zip_code
    FROM
      "onelakehouse"."retail".customer; #catalog_name
    ```
  </Step>

  <Step title="Define the table in the Model" titleSize="h2">
    Create a `tables` folder to store logical table definitions, with each table defined in a separate YAML file outlining its dimensions, measures, and segments. For example, to define a table for `sales `data:

    ```yaml
    table:
      - name: customers
        sql: {{ load_sql('customers') }}
        description: Table containing information about sales transactions.
    ```

    ### Add dimensions and measures

    After defining the base table, add the necessary dimensions and measures. For example, to create a table for sales data with measures and dimensions, the YAML definition could look as follows:

    ```yaml
    tables:
      - name: sales
        sql: {{ load_sql('sales') }}
        description: Table containing sales records with order details.

        dimensions:
          - name: order_id
            type: number
            description: Unique identifier for each order.
            sql: order_id
            primary_key: true
            public: true

        measures:
          - name: total_orders_count
            type: count
            sql: id
            description: Total number of orders.
    ```

    ### Add segments to filter

    Segments are filters that allow for the application of specific conditions to refine the data analysis. By defining segments, you can focus on particular subsets of data, ensuring that only the relevant records are included in your analysis. For example, to filter for records where the state is either Illinois or Ohio, you can define a segment as follows:

    ```yaml
    segments:
      - name: state_filter
        sql: "{TABLE}.state IN ('Illinois', 'Ohio')"
    ```

    To know more about segments click [here](https://dataos.info/resources/lens/working_with_segments/).
  </Step>

  <Step title="Create the Views" titleSize="h2">
    Create a **views** folder to store all logical views, with each view defined in a separate YAML file (e.g., `sample_view.yml`). Each view references dimensions, measures, and segments from multiple logical tables. For instance the following`customer_churn` view is created.

    ```yaml
    views:
      - name: customer_churn_prediction
        description: Contains customer churn information.
        tables:
          - join_path: marketing_campaign
            includes:
              - engagement_score
              - customer_id
          - join_path: customer
            includes:
              - country
              - customer_segments
    ```

    To know more about the views click [here](https://dataos.info/resources/lens/working_with_views/).
  </Step>

  <Step title="Create User groups" titleSize="h2">
    This YAML manifest file is used to manage access levels for the semantic model. It defines user groups that organize users based on their access privileges. In this file, you can create multiple groups and assign different users to each group, allowing you to control access to the model.By default, there is a 'default' user group in the YAML file that includes all users.

    ```yaml
    user_groups:
      - name: default
        description: this is default user group
        includes: "*"
    ```

    To know more about the User groups click [here](https://dataos.info/resources/lens/working_with_user_groups_and_data_policies/)
  </Step>
</Steps>

### Step 4: Apply the Lens deployment manifest file

After configuring the deployment file with the necessary settings and specifications, deploy the Lens using the following `apply` command:

```bash
dataos-ctl  apply -f ${manifest-file-path}
```

## Docker-compose manifest file

<Accordion title="Click here to see the full docker-compose manifest file" defaultOpen={false}>
  ```yaml
  version: "2.2"

  x-lens2-environment: &lens2-environment
    # DataOS
    DATAOS_FQDN: liberal-monkey.dataos.app
    # Overview
    LENS2_NAME: sales360
    LENS2_DESCRIPTION: "Ecommerce use case on Adventureworks sales data"
    LENS2_TAGS: "lens2, ecom, sales and customer insights"
    LENS2_AUTHORS: "iamgroot, iamloki"
    LENS2_SCHEDULED_REFRESH_TIMEZONES: "UTC,America/Vancouver,America/Toronto"
    # Data Source
    LENS2_SOURCE_TYPE: depot
    LENS2_SOURCE_NAME: postgreslens2
    DATAOS_RUN_AS_APIKEY: bGVuc3NzLmUzMDA1ZjMzLTZiZjAtNDY4My05ZjhhLWNhODliZTFhZWJhMQ==
    LENS2_DB_SSL : "true"
    # Log
    LENS2_LOG_LEVEL: error
    CACHE_LOG_LEVEL: "trace"
    # Operation
    LENS2_DEV_MODE: true
    LENS2_DEV_MODE_PLAYGROUND: false
    LENS2_REFRESH_WORKER: true
    LENS2_SCHEMA_PATH: model
    LENS2_PG_SQL_PORT: 5432
    CACHE_DATA_DIR: "/var/work/.store"
    NODE_ENV: production
    LENS2_ALLOW_UNGROUPED_WITHOUT_PRIMARY_KEY: "true"
  services:
    api:
      restart: always
      image: rubiklabs/lens2:0.35.41-05
      ports:
        - 4000:4000
        - 25432:5432
        - 13306:13306
      environment:
        <<: *lens2-environment   
      volumes:
        - ./model:/etc/dataos/work/model
  ```
</Accordion>