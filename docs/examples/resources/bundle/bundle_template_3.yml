version: v1alpha
name: "dataosbundlejobsv14"
type: bundle
tags:
  - sampletest
description: This is bundle, dag of resources
layer: "user"
bundle:
  schedule:
    initialState: create
    timezone: Asia/Kolkata
    create:
     cron: '*/12 * * * *'
  workspaces:
    - name: testbundle14
      description: "This workspace runs dataos bundle based resources"
      tags:
        - bundleJob
        - bundleResource
      labels:
        "name" : "dataosBundleJobsv14"
      layer: "user"
  resources:
    - id: create-lakehouse-depot
      file: depot.yaml
    - id: write-into-lakehouse
      dependencies:
        - create-lakehouse-depot
      workspace: testbundle14
      file: read_lakehouse_write_snowflake.yaml
    - id: read-from-lakehouse
      dependencies:
        - write-into-lakehouse
      workspace: testbundle14
      spec:
        version: v1
        name: sanity-read-workflow-v14
        type: workflow
        tags:
          - Sanity
          - Lakehouse
        title: Sanity read from lakehouse
        description: |
          The purpose of this workflow is to verify if we are able to read different
          file formats Lakehouse.
        workflow:
          dag:
            - name: sanity-read-job-v14
              title: Sanity read files from lakehouse
              description: |
                The purpose of this job is to verify if we are able to read different
                file formats from lakehouse.
              spec:
                tags:
                  - Sanity
                  - lakehouse
                stack: flare:7.0
                compute: runnable-default
                stackSpec:
                  job:
                    explain: true
                    logLevel: INFO
                    showPreviewLines: 2
                    inputs:
                      - name: a_city_csv
                        dataset: dataos://bundleicebergv14:retail14/city1
                        format: Iceberg
                      - name: a_city_json
                        dataset: dataos://bundleicebergv14:retail14/city2
                        format: Iceberg
                      - name: a_city_parquet
                        dataset: dataos://bundleicebergv14:retail14/city3
                        format: Iceberg

                    outputs:
                      # csv
                      - name: finalDf_csv
                        dataset: dataos://bundleicebergv14:temp14/temp?acl=rw
                        format: Iceberg
                        options:
                          saveMode: overwrite
                          partitionBy:
                            - version
                        tags:
                          - Sanity
                          - Lakehouse
                          - CSV
                        title:  Csv read sanity
                        description: Csv read sanity
                      # json
                      - name: finalDf_json
                        dataset: dataos://bundleicebergv14:temp14/temp2?acl=rw
                        format: Iceberg
                        options:
                          saveMode: overwrite
                          partitionBy:
                            - version
                        tags:
                          - Sanity
                          - Lakehouse
                          - JSON
                        title: Json read sanity
                        description: Json read sanity
                      # parquet
                      - name: finalDf_parquet
                        dataset: dataos://bundleicebergv14:temp14/temp3?acl=rw
                        format: Iceberg
                        options:
                          saveMode: overwrite
                          partitionBy:
                            - version
                        tags:
                          - Sanity
                          - Lakehouse
                          - Parquet
                        title: Parquet read sanity
                        description: Parquet read sanity
                    steps:
                      - sequence:
                          - name: finalDf_csv
                            sql: SELECT * FROM a_city_csv LIMIT 10
                            functions:
                              - name: drop
                                columns:
                                  - "__metadata_dataos_run_mapper_id"
                          - name: finalDf_json
                            sql: SELECT * FROM a_city_json LIMIT 10
                            functions:
                              - name: drop
                                columns:
                                  - "__metadata_dataos_run_mapper_id"
                          - name: finalDf_parquet
                            sql: SELECT * FROM a_city_parquet LIMIT 10
                            functions:
                              - name: drop
                                columns:
                                  - "__metadata_dataos_run_mapper_id"