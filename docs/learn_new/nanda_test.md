# Frequently Asked Questions

This document provides a comprehensive FAQs for various aspects of DataOS, ensuring clarity and ease of use for users.

##  Data Ingestion and Quality

**Q:** Can data products be created without actual data ingestion?

**A:** Yes, data products can be created by leveraging existing customer data sources, without requiring the data to be ingested into the platform. This means you can build data products directly on top of your data where it resides.

**Q:** How is data quality ensured?

**A:** Data quality is ensured through quality checks run on ingested data using predefined validation rules and monitoring systems. The platform allows you to define these rules to check the data against your expectations, ensuring data integrity.

**Q:** How does data get cataloged?

**A:** When using Flare jobs, anything written is automatically cataloged by a service running behind the scenes. This automatic cataloging makes data discovery and management more efficient.

##  Monitoring and Observability

 **Q: What is the difference between monitoring and observability?**

**A:** **Monitoring** tracks predefined system metrics, while **observability** includes logging, tracing, and alerting to diagnose system behavior. Observability provides a broader view, allowing you to understand system states and diagnose root causes rather than just reacting to known problems.

 **Q: What happens when a monitor condition is met?**

**A:** When a monitor condition is met, an **incident** is generated and stored in a **Pulsar topic**. If a **pager** is configured, it triggers an alert.

 **Q: Are monitors always running?**

**A:** **No.** Monitors execute based on a **defined schedule** to prevent system overload. This schedule is configured using a **cron-like expression**.

 **Q: Where are incidents stored?**

**A:** Incidents are stored in a **Pulsar topic**. This topic is **not directly accessible via the UI** but can be accessed using **Workbench**.

 **Q: How are monitors scheduled?**

**A:** Monitors are scheduled using **cron-like expressions**.

**Q: What are the different types of monitors available?**

**A:** There are **three types** of monitors:

1. **Equation Monitors** – Use mathematical expressions to evaluate conditions.
2. **Report Monitors** – Monitor APIs and endpoint health.
3. **Streaming Monitors** – Monitor real-time data streams (topics).

---

##  Pagers and Alerts

 **Q: What is the role of a pager?**

**A:** A pager listens for **incidents generated by monitors** and sends **alerts** via channels such as:

- Slack
- Microsoft Teams
- Email
- Webhooks

 **Q: Can incidents be tracked if no pager is configured?**

**A:** **Yes.** Incidents are still stored internally, but no **real-time notifications** will be sent.

 **Q: How do pagers connect to monitors?**

**A:** Pagers connect to monitors by matching **incident details** such as name, severity, and incident type.

 **Q: Can pagers be scheduled?**

**A:** **No.** Pagers **continuously listen** for incidents.

 **Q: How do I set up Slack, email, or webhook notifications?**

**A:**

- **Slack:** Requires a **webhook URL** in the correct template format.
- **Email/Webhooks:** Require proper **server configuration**.

---

##  System Architecture and Access Control

 **Q: How is access control enforced?**

**A:** **Heimdall** is the governance engine that enforces access control in DataOS. It acts as the **Policy Decision Point (PDP)**, ensuring:

- Dataset, API, and service access is properly authorized.
- A **permission handler** verifies access at every step.

 **Q: Why was a custom orchestrator (Poros) built instead of using Airflow?**

**A:** Airflow did not meet all platform requirements, so **Poros** was built as:

- A **single point of contact** for operations.
- A system that **stores manifest files as desired states**.
- A **lifecycle manager** for workflows and resources.

 **Q: What are the different layers of the DataOS system?**

**A:** DataOS has **three logical layers**:

1. **User Space** – Where **data products** are created.
2. **Core Kernel** – Includes services like **Handle and Poros**.
3. **Cloud Kernel** – Abstracts **cloud-specific details** to provide cloud agnostic platform.

 **Q: What is the difference between system and miniature clusters?**

**A:**

- **They are the same type of cluster**, but:
    - **Miniature** clusters are for **general use**.
    - **System** clusters handle **core processes**.

---

##  Resource Management and Execution

 **Q: How do I delete a resource?**

**A:** You can delete a resource by its **name or identifier**. If an identifier is used, the system **auto-detects the resource type**.

 **Q: What is the best practice for updating resources?**

**A:** **Delete and recreate resources** instead of relying on updates.

 **Q: How are changes made to an existing resource?**

**A:** The best approach is to **delete the resource and reapply it with modifications**.

---

##  Metadata Management and System Services

 **Q: What role does Metis play in metadata management?**

**A:** **Metis** is the **metadata manager** in DataOS. It:

- Collects **operational, technical, and business metadata**.
<!-- - Works with **Odin (Knowledge Graph Service)** to create a **semantic web** for data. -->

---

##  Workflow Failures and Query Auditing

 **Q: How can I debug a failed workflow?**

**A:**

1. **Check Operations app logs** for **current failures**.
2. **Check Metis logs** for **historical errors**.
3. If **no logs appear**, the **pod likely failed to start**.

 **Q: How are queries optimized?**

**A:** **Trino-based queries** are divided into **stages, tasks, and splits** for performance efficiency.

 **Q: How do pods relate to jobs?**

**A:** Each job has an associated pod. Execution details can be checked at the **pod level**.

##  Metadata and Lineage

 **Q: Can you get metadata directly from source data?**

**A:** Yes, metadata can be extracted directly from source data using the **DataOS Scanner stack**. This enables developers to retrieve metadata from external systems such as:

- **Relational Databases (RDBMS)**
- **Data Warehouses**
- **Messaging Services**
- **Dashboards**
It also collects metadata from internal components and services:
- **DataOS Resources and Data Products within the DataOS environment**

 **Metadata Extraction Workflows:**

- **Depot Scan Workflow:** Uses depots to scan datasets. Requires the depot name or address to connect to the data source.
- **Non-Depot Scan Workflow:** Requires connection details (e.g., host URL, project ID, credentials) in a YAML file to access the metadata source.

 **Extracted Metadata Includes:**

- **Dataset/Table Information:** Names, owners, tags, schemas, column names, descriptions
- **Data Quality & Profiling:** Query usage, user information
- **Resources:** Metadata about the deployed DataOS Resources, their historical runtime and operations data.
- **Data Products:** Inputs, outputs & SLOs (Service Level Objectives)

 **Q: How do you show lineage for both data and metadata?**

**A:** **Metis** provides a **visual representation** of data and metadata lineage, enabling users to trace:

- The **source** of a dataset
- **Transformations and dependencies**
- **Jobs** that created or modified datasets

 **Key Features:**

- **Graphical lineage visualization** for better understanding of data flow
- **Interactive navigation** to explore dataset relationships
- **Metadata versioning** to track historical changes


---

##  Data Products and Usage

**Q: How do you prevent creating too many data products?**

**A:** To avoid redundant data products:

1. **Leverage existing data products** if they meet your use case.
2. **Discover available data products** by **domain, use case, or custom criteria** before creating new ones.
3. **Request updates** to existing products instead of creating new ones.
4. **Create source-level data products** that support multiple consumer-aligned data products.
5. **Review data products regularly** to ensure they remain relevant and effective.

The **Data Product Owner** manages lifecycle, usage, and quality control.

 **Q: Can multiple consumer-aligned data products use the same set of source-aligned data products?**

**A:** Yes, DataOS supports shared data sources across multiple **consumer-aligned** data products.

- **Example:** Customer, Product, and Sales data can support **marketing campaigns, cross-sell opportunities, and customer 360 use cases**.

 **Q: How do developers typically work on data products?**

**A:**

- **Design to Deployment:** Developers design and build data products.
- **Avoid duplication:** Before creating a new data product, developers should explore existing products within the organization.

 **Q: How do you ensure no duplicates across data products?**

**A:** There is **no direct duplicate detection**, but:

- Data products can be **filtered by domain** in **Data Product Hub**.
- Developers can **review input/output details, models, and metadata** for similarities.

 **Q: When data products are created, can automated testing be integrated into the pipeline (e.g., Jenkins)?**

**A:** **Currently not available**, but guidelines for this integration are being finalized.

---

##  Functionality and Tools

 **Q: Are there modeling tools or templates available?**

**A:** **Yes**, DataOS offers **Lens** for modeling capabilities.

 **Q: Can filters be applied to limit cataloging specific columns or tables (e.g., sensitive columns)?**

**A:** Yes. Filters can be applied when defining **logical semantic models** and **physical table mappings**.

- SQL queries can exclude sensitive columns.
- **Data policies** allow user-based access control to specific columns.

 **Q: Is there support for dynamic tagging of records or metadata?**

**A:** Yes, **DataOS supports tagging** via:

 **1. Manual Tagging (Metis)**

- Users can create **custom tag categories** based on taxonomy.
- Tags can be applied to dataset columns from a predefined glossary.

 **2. Auto-Tagging (Fingerprinting)**

- **Analyzes column names and values** to identify data types.
- **Auto-classification** suggests attributes (e.g., identifying PII like email or income).
- Data owners can **approve or reject** auto-generated tags.

 **3. Column Tagging at Data Ingestion (Flare YAMLs)**

- Users can **predefine tags** for columns in **Flare Workflow YAMLs** for governance during ingestion.

 **Q: How can data masking policies be applied?**

1. **Masking Operators:**
    
    Specify the type of masking using the `operator` field (e.g., `hash`, `redact`, `pass_through`). Additional configurations may be required for certain operators.
    
    | **Operator** | **Additional Properties** |
    | --- | --- |
    | **hash** | `algo: sha256`  *(Currently, only the SHA-256 algorithm is supported.)* |
    | **redact** | No additional configuration.  *(Redacts column values based on their type.)* |
    | **pass_through** | No additional configuration.  *(Allows access to values tagged with PII fingerprints.)* |
    | **bucket_date** | `precision: "day"`  *(Precision can be `year`, `quarter`, `month`, or `day`.)* |
    | **bucket_number** | `buckets: [10, 20, 30]`  *(Specify bucket numbers as needed.)* |
    | **regex_replace** | `pattern: <regex pattern>`  `replacement: "#"`. |
    | **rand_pattern** | `pattern: "###-####-#####"`  *(Define pattern as required.)* |
    | **rand_regexify** | `pattern: "{a |
2. **Creating Custom Data Masking Policies:**
    
    If predefined policies do not meet your needs, you can define custom tags and create tailored masking policies. A regular expression (Regex) masking will find and replace certain strings of characters to protect sensitive information.   By using a combination of regex and masking techniques, you can more effectively manipulate your data to suit your needs. For example:
    
    - To mask a `credit_card_number` column, a custom policy can be created to hash or obscure all but the last few characters, depending on the desired level of obfuscation.
3. **Using Existing Data Masking Policies:**
    
    DataOS offers predefined masking policies that can be applied by tagging relevant columns via Metis or Flare Workflow YAML during ingestion. For instance:
    
    - To mask an `age` column using predefined age bucketing, you can apply a tag such as `PII.Age`.
    - The associated masking policy will then automatically be enforced, ensuring sensitive data is protected. 

---

##  Semantic Models and Integration

 **Q: Can semantic models be used to map logical and physical layers, supporting aggregation outside Explore?**

 **A:** Yes, semantic models in **DataOS** allow **logical-to-physical mapping** for **aggregation outside Explore**.

 **Integration Options:**

- **Lens APIs** for data access
- **BI Tools:** Tableau, Power BI, Superset, Excel
- **Data Science Tools:** Jupyter Notebook
- **Custom Applications:** Programmatic data retrieval

 **Connectivity Options:**

- **REST APIs** – Query data via HTTP
- **GraphQL** – Flexible, precise data interactions
- **Postgres** – Query semantic models directly

##  Access and Security

 **Q: Are usernames/passwords visible in depot files?**

**A:** No, credentials can be securely stored and referenced in multiple ways:

1. **Instance Secrets:** Store credentials securely and reference them in the **Depot manifest file** without exposing them.
2. **JSON File Path:** Define a JSON file containing sensitive information and reference its path in the **Depot manifest** for security.
3. **Environment Variables:**
    - Define credentials as variables (e.g., `$SNOWFLAKE_USERNAME`, `$SNOWFLAKE_PASSWORD`).
    - Set them using the `export` command in the shell.
    - Reference these variables in the **Depot manifest file** to prevent direct exposure in code repositories.

 **Q: Can users see everyone’s depots or only their own?**

**A:** By default, users with **data-dev** and **operator** roles can **create and view all depots**. However, access can be restricted by:

- **Removing the "Manage All Depot" use-case.**
- **Creating policies** to prevent access to the depot listing endpoint.
- **Exploring depot details** via the **Operations App** or **Metis UI** in DataOS.

 **Q: How do you control who creates clusters or policies?**

**A:** Access control is managed through **Bifrost** in DataOS:

1. **Role-Based Access (RBAC):** Assign roles that bundle multiple permissions together.
2. **Use-Case-Based Access:** Grant specific permissions (e.g., Cluster creation) based on operational needs.
3. **Operator Assignments:** Users with `roles:id:operator` can assign use cases and manage permissions in **Bifrost**.

---

##  Operational and Performance

 **Q: If schema updates occur, will scans adapt automatically?**

**A:** Yes! Scans will automatically adapt to schema updates.

 **Q: When promoting or updating schemas, does the scanner need to run again?**

**A:** No! A continuously running **Indexer service** detects changes and triggers the **Scanner Workflow**, ensuring metadata updates in the **Metis database**.

 **Q: How are schema updates detected?**

**A:** A **continuous monitoring service** tracks schema changes and triggers a **Reconciliation Scanner YAML** workflow to update the target metastore automatically.

 **Q: What happens if multiple scans or bundles are run?**

**A:**

- If a **Bundle configuration changes**, the **deployed Bundle Resource updates** accordingly.
- If **no changes** are detected, **no updates** will be made.
- If a **Scanner runs multiple times** without detecting metadata changes, **Metis remains unchanged**.

 **Q: What happens if a step in the workflow fails?**

**A:**

- The **entire workflow fails** if any step encounters an issue.
- **Alerting Mechanisms** (e.g., Pager notifications) can notify stakeholders.
- **Retry Mechanisms** can be configured with policies like `Always` or `On_transient_failure`.

 **Q: If the first three steps in a Bundle succeed and the fourth fails, do the earlier steps revert?**

**A:** No, Bundles **do not support partial rollbacks**. If a failure occurs:

- The issue must be resolved.
- The entire **Bundle must be rerun**.

Bundles ensure **consistent deployment** by managing resources in a structured manner.

 **Q: How far can offsets go in queries?**

**A:** An offset can match the total number of rows in a table. However, **large offsets are discouraged** as they can significantly impact performance.

 **Q: Are logical layers cataloged alongside physical tables?**

**A:** Yes! Logical layers, referred to as **Lenses**, are cataloged in **Metis** alongside physical tables.

- Logical tables can be traced back to **source physical tables**.
- Business views built on logical tables are **fully documented** in Metis.

---

##  Naming and Conventions

 **Q: Can special characters or capitals be allowed in naming conventions?**

**A:** No, naming conventions enforce strict rules:

- **Allowed:** Lowercase letters (`a-z`), numbers (`0-9`), hyphens (if applicable).
- **Not Allowed:** Uppercase letters, special characters (except permitted hyphens).

 **Q: Can names overlap if they correspond to different tables?**

**A:** Yes! Column names can be **reused across different tables** (e.g., `customer_id` in both `orders` and `customers` tables).

 **Q: How do you ensure naming conventions are consistently applied across global and local policies?**

**A:**

- **Taxonomy Definitions** standardize naming across domains.
- **Metis Glossary & Tags** enforce consistent classification.
- **Example:**
    - **PII Data:** age, email, income.
    - **PHI Data:** disease name, patient details.
- **Automatic Global Policies:** Tags ensure proper governance and security enforcement.

---

##  API and Queries

 **Q: Can APIs allow querying schema or filtering data by company?**

**A:** Yes, APIs provide SQL-like capabilities for querying schemas and filtering data.

 **Q: What is the general practice for indexing and API consumption?**

**A:** **Best Practices for API Consumption in DataOS:**

1. **Use Group-Based Access Control:** Secure API endpoints with **role-based permissions**.
2. **Apply Dynamic Data Masking:** Ensure **sensitive data protection** in API responses.
3. **Filter and Query Smartly:** Optimize API requests to fetch **only necessary data**.
4. **Monitor API Usage:** Track consumption metrics for **performance optimization**.
5. **Utilize Caching:** Improve **response times and reduce server load**.

 **Q: Can APIs support aggregations outside of Explore?**

**A:** Yes! APIs such as **SQL APIs and GraphQL** support aggregations like `SUM`, `COUNT`, and `AVG`.

 **Q: Are APIs available to dynamically tag records?**

**A:** No, **dynamic record-level tagging is not supported**. However, classified metadata enables:

- **Dynamic Data Masking**
- **Column-level & Row-level security**

---

##  Infrastructure and Installation

 **Q: Are workspaces distinctly different if multiple depots are spun up?**

**A:** No. **Depots are global entities** in DataOS and are **not tied to specific workspaces**.

 **Q: Can clusters or workspaces have overlapping resource dependencies?**

**A:** Yes!

- Workspaces act as **logical namespaces** (similar to Kubernetes).
- Clusters can be deployed **within a workspace**, allowing shared resource dependencies.